{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "draft_multilabel_Arumentation_type_classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Task for the identification of argumentative sentence types with a multilabel approach"
      ],
      "metadata": {
        "id": "sL28VxkSiFAa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Files needed to run the notebook:\n",
        "\n",
        "- The pickle file *dataset.pkl*; "
      ],
      "metadata": {
        "id": "MfiwTd3Wn5Lg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import libraries:"
      ],
      "metadata": {
        "id": "ZGCVnxokoXQG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sentence_transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0OumxReDt4qJ",
        "outputId": "4f9b0f7c-3d98-47be-89ca-6cd4b059746a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.7/dist-packages (2.2.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (3.2.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (4.63.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.11.1+cu111)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.10.0+cu111)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.1.96)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.4.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.4.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (4.17.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.21.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence_transformers) (3.10.0.2)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.11.6)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.0.49)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2019.12.20)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (3.6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (4.11.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence_transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5.0.0,>=4.6.0->sentence_transformers) (3.7.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence_transformers) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence_transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence_transformers) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence_transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence_transformers) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle as pkl\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Activation, Dropout, LSTM, Input, GRU, BatchNormalization\n",
        "from tensorflow.keras.regularizers import l2"
      ],
      "metadata": {
        "id": "HyoTnNb6Ce9v"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xeO3TDhtn3BT"
      },
      "outputs": [],
      "source": [
        "df = pd.read_pickle(\"./dataset.pkl\")  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "id": "E_mgQNyFpL8T",
        "outputId": "17d08537-096d-41fc-b9fc-48062a5c0bda"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  Document  Name  Id                                           Sentence Type  \\\n",
              "2     2004  prem  A3  respect borne mind pursuant Article 58 Statute...    L   \n",
              "3     2004  prem  A4  Advocate General states point 20 Opinion Commi...    F   \n",
              "5     2004  prem  A6  Regarding Wam’s argument Commission’s appeal i...  L|F   \n",
              "6     2004  prem  A7  Appeals judgments Court First Instance governe...    L   \n",
              "7     2004  prem  A8  Next must noted obligation provide statement r...    L   \n",
              "\n",
              "  Supported_by Supported_from_failure Attacked_by Inhibited_by Rephrased_by  \\\n",
              "2          NaN                    NaN         NaN          NaN          NaN   \n",
              "3          NaN                    NaN         NaN          NaN          NaN   \n",
              "5          NaN                    NaN         NaN          NaN          NaN   \n",
              "6          NaN                    NaN         NaN          NaN          NaN   \n",
              "7          NaN                    NaN         NaN          NaN          NaN   \n",
              "\n",
              "  Argumentation_scheme  \n",
              "2                 Rule  \n",
              "3                  Aut  \n",
              "5                 Rule  \n",
              "6                 Rule  \n",
              "7                 Prec  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ee988c4e-ecc5-4c17-94ed-e78b5f9f8692\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Document</th>\n",
              "      <th>Name</th>\n",
              "      <th>Id</th>\n",
              "      <th>Sentence</th>\n",
              "      <th>Type</th>\n",
              "      <th>Supported_by</th>\n",
              "      <th>Supported_from_failure</th>\n",
              "      <th>Attacked_by</th>\n",
              "      <th>Inhibited_by</th>\n",
              "      <th>Rephrased_by</th>\n",
              "      <th>Argumentation_scheme</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2004</td>\n",
              "      <td>prem</td>\n",
              "      <td>A3</td>\n",
              "      <td>respect borne mind pursuant Article 58 Statute...</td>\n",
              "      <td>L</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Rule</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2004</td>\n",
              "      <td>prem</td>\n",
              "      <td>A4</td>\n",
              "      <td>Advocate General states point 20 Opinion Commi...</td>\n",
              "      <td>F</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Aut</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2004</td>\n",
              "      <td>prem</td>\n",
              "      <td>A6</td>\n",
              "      <td>Regarding Wam’s argument Commission’s appeal i...</td>\n",
              "      <td>L|F</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Rule</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2004</td>\n",
              "      <td>prem</td>\n",
              "      <td>A7</td>\n",
              "      <td>Appeals judgments Court First Instance governe...</td>\n",
              "      <td>L</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Rule</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2004</td>\n",
              "      <td>prem</td>\n",
              "      <td>A8</td>\n",
              "      <td>Next must noted obligation provide statement r...</td>\n",
              "      <td>L</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Prec</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ee988c4e-ecc5-4c17-94ed-e78b5f9f8692')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ee988c4e-ecc5-4c17-94ed-e78b5f9f8692 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ee988c4e-ecc5-4c17-94ed-e78b5f9f8692');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Argumentation_scheme\"].values[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "zbJEJDGcih55",
        "outputId": "8d09af13-2210-4e64-85f4-741a2b1d3d34"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Rule'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Argumentation_scheme\"].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4v0aC-cC53Z",
        "outputId": "c72aea54-004e-4a79-a873-3409e99aaf50"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Prec                  279\n",
              "Itpr                   95\n",
              "Princ                  64\n",
              "Rule                   61\n",
              "Prec|Rule              35\n",
              "Itpr|Prec              22\n",
              "Aut                    21\n",
              "Class|Prec|Rule         9\n",
              "Tele                    8\n",
              "Class                   7\n",
              "Class|Prec              5\n",
              "Aut|Itpr                5\n",
              "Prec|Tele               5\n",
              "Itpr|Tele               4\n",
              "Rule|Syst               3\n",
              "Itpr|Rule               3\n",
              "Prec|Syst               3\n",
              "Syst                    3\n",
              "Princ|Rule              2\n",
              "Psy|Tele                2\n",
              "Lit|Rule                2\n",
              "Lit|Prec                2\n",
              "Prec|Psy|Rule|Tele      1\n",
              "Princ|Tele              1\n",
              "Itpr|Syst               1\n",
              "Itpr|Psy                1\n",
              "Itpr|Lit                1\n",
              "Aut|Prec|Princ          1\n",
              "Princ|Psy|Syst          1\n",
              "Itpr|Princ|Syst         1\n",
              "Lit|Rule|Syst           1\n",
              "Syst|Tele               1\n",
              "Aut|Class               1\n",
              "Lit|Rule|Tele           1\n",
              "Aut|Itpr|Prec           1\n",
              "Prec|Psy                1\n",
              "Aut|Rule                1\n",
              "Aut|Syst                1\n",
              "Lit                     1\n",
              "Prec|Princ              1\n",
              "Psy|Rule                1\n",
              "Aut|Prec                1\n",
              "Class|Rule              1\n",
              "Rule|Tele               1\n",
              "Itpr|Prec|Rule          1\n",
              "Name: Argumentation_scheme, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "value_counts = df['Argumentation_scheme'].value_counts() # Specific column \n",
        "df_truncated60 = df[df['Argumentation_scheme'].isin(value_counts[value_counts > 60].index)]\n",
        "df_truncated20 = df[df['Argumentation_scheme'].isin(value_counts[value_counts > 20].index)]"
      ],
      "metadata": {
        "id": "AkPsGUh9NzVU"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_truncated60['Argumentation_scheme'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52TOc7cLN0ND",
        "outputId": "881baa4c-46a7-4979-8da1-9301777821d2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Prec     279\n",
              "Itpr      95\n",
              "Princ     64\n",
              "Rule      61\n",
              "Name: Argumentation_scheme, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_truncated20['Argumentation_scheme'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYQ87vX5jH-a",
        "outputId": "efa77857-4ddb-42fa-fdb9-3ecca7189e51"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Prec         279\n",
              "Itpr          95\n",
              "Princ         64\n",
              "Rule          61\n",
              "Prec|Rule     35\n",
              "Itpr|Prec     22\n",
              "Aut           21\n",
              "Name: Argumentation_scheme, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_balanced = pd.concat([df[df[\"Argumentation_scheme\"] == \"Prec\"].sample(61),\n",
        "                    df[df[\"Argumentation_scheme\"] == \"Itpr\"].sample(61),\n",
        "                    df[df[\"Argumentation_scheme\"] == \"Princ\"].sample(61),\n",
        "                    df[df[\"Argumentation_scheme\"] == \"Rule\"]\n",
        "                    ])"
      ],
      "metadata": {
        "id": "cg5TNB0xDB3I"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_balanced[\"Argumentation_scheme\"].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ND7BsHfKDdXD",
        "outputId": "58e80075-6140-4274-ad65-da641a5c58f1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Prec     61\n",
              "Itpr     61\n",
              "Princ    61\n",
              "Rule     61\n",
              "Name: Argumentation_scheme, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choice of parameters:"
      ],
      "metadata": {
        "id": "JIZ4EXFsgyL9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# EMBEDDINGS\n",
        "EMBEDDING = \"legal_bert_sentence\"\n",
        "\n",
        "# MODELS\n",
        "MODEL = \"RNN\"\n",
        "\n",
        "# DATASET (uncomment the chosen one)\n",
        "# DATASET = df\n",
        "# DATASET = df_truncated60\n",
        "DATASET = df_truncated20\n",
        "# DATASET = df_balanced\n",
        "\n",
        "TEST_SIZE = 0.3\n",
        "VAL_SIZE = 0.2  # percentage taken from the train subset\n",
        "\n",
        "# MODEL PARAMETERS\n",
        "\n",
        "EPOCHS = 200\n",
        "BATCH_SIZE = 64\n",
        "LR = 10e-7\n",
        "DROP = 0.2\n",
        "\n",
        "L2_FACTOR = 0.001\n",
        "LAYERS = [256, 128, 64] # 3 layers"
      ],
      "metadata": {
        "id": "RrzDOKDtjW5h"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aut_col = df['Argumentation_scheme'].apply(lambda x: 1 if 'Aut' in str(x) else 0)\n",
        "class_col = df['Argumentation_scheme'].apply(lambda x: 1 if 'Class' in str(x) else 0)\n",
        "itpr_col = df['Argumentation_scheme'].apply(lambda x: 1 if 'Itpr' in str(x) else 0)\n",
        "lit_col = df['Argumentation_scheme'].apply(lambda x: 1 if 'Lit' in str(x) else 0)\n",
        "prec_col = df['Argumentation_scheme'].apply(lambda x: 1 if 'Prec' in str(x) else 0)\n",
        "princ_col = df['Argumentation_scheme'].apply(lambda x: 1 if 'Princ' in str(x) else 0)\n",
        "psy_col = df['Argumentation_scheme'].apply(lambda x: 1 if 'Psy' in str(x) else 0)\n",
        "rule_col = df['Argumentation_scheme'].apply(lambda x: 1 if 'Rule' in str(x) else 0)\n",
        "syst_col = df['Argumentation_scheme'].apply(lambda x: 1 if 'Syst' in str(x) else 0)\n",
        "tele_col = df['Argumentation_scheme'].apply(lambda x: 1 if 'Tele' in str(x) else 0)\n",
        "\n",
        "df_new = pd.DataFrame(DATASET['Sentence'])\n",
        "df_new['Aut'] = aut_col\n",
        "df_new['Class'] = class_col\n",
        "df_new['Itpr'] = itpr_col\n",
        "df_new['Lit'] = lit_col\n",
        "df_new['Prec'] = prec_col\n",
        "df_new['Princ'] = princ_col\n",
        "df_new['Psy'] = psy_col\n",
        "df_new['Rule'] = rule_col\n",
        "df_new['Syst'] = syst_col\n",
        "df_new['Tele'] = tele_col"
      ],
      "metadata": {
        "id": "jwDWRMdPD0-Q"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_new.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "qPHEb3rIFYBY",
        "outputId": "f1231a98-6854-45b3-bf74-2262201056df"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            Sentence  Aut  Class  Itpr  Lit  \\\n",
              "2  respect borne mind pursuant Article 58 Statute...    0      0     0    0   \n",
              "3  Advocate General states point 20 Opinion Commi...    1      0     0    0   \n",
              "5  Regarding Wam’s argument Commission’s appeal i...    0      0     0    0   \n",
              "6  Appeals judgments Court First Instance governe...    0      0     0    0   \n",
              "7  Next must noted obligation provide statement r...    0      0     0    0   \n",
              "\n",
              "   Prec  Princ  Psy  Rule  Syst  Tele  \n",
              "2     0      0    0     1     0     0  \n",
              "3     0      0    0     0     0     0  \n",
              "5     0      0    0     1     0     0  \n",
              "6     0      0    0     1     0     0  \n",
              "7     1      0    0     0     0     0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-67fc0cc7-4b06-4592-bb92-d6b7092c3638\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence</th>\n",
              "      <th>Aut</th>\n",
              "      <th>Class</th>\n",
              "      <th>Itpr</th>\n",
              "      <th>Lit</th>\n",
              "      <th>Prec</th>\n",
              "      <th>Princ</th>\n",
              "      <th>Psy</th>\n",
              "      <th>Rule</th>\n",
              "      <th>Syst</th>\n",
              "      <th>Tele</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>respect borne mind pursuant Article 58 Statute...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Advocate General states point 20 Opinion Commi...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Regarding Wam’s argument Commission’s appeal i...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Appeals judgments Court First Instance governe...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Next must noted obligation provide statement r...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-67fc0cc7-4b06-4592-bb92-d6b7092c3638')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-67fc0cc7-4b06-4592-bb92-d6b7092c3638 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-67fc0cc7-4b06-4592-bb92-d6b7092c3638');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding"
      ],
      "metadata": {
        "id": "OV-HtpmRjx7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if EMBEDDING == \"legal_bert_sentence\":\n",
        "  sbert_model = SentenceTransformer(\"nlpaueb/legal-bert-small-uncased\")\n",
        "  sentence_embeddings = sbert_model.encode(df_new[\"Sentence\"].values)\n",
        "\n",
        "  X = sentence_embeddings\n",
        "  # X = X.reshape(len(X), 1, X.shape[1])\n",
        "  y = df_new.drop(\"Sentence\", axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KsUsgc7oj3aV",
        "outputId": "4e45da77-d75a-43a8-b1aa-c2507646e6e4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/nlpaueb_legal-bert-small-uncased. Creating a new one with MEAN pooling.\n",
            "Some weights of the model checkpoint at /root/.cache/torch/sentence_transformers/nlpaueb_legal-bert-small-uncased were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#y = y.apply(lambda row: row/list(row).count(1) if 1 in list(row) else row, axis=1)"
      ],
      "metadata": {
        "id": "Oe1sutl-3FHt"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Uom-Lig280Xb",
        "outputId": "6e8bc626-98c0-499b-86f9-b82eeee405e7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Aut  Class  Itpr  Lit  Prec  Princ  Psy  Rule  Syst  Tele\n",
              "2    0      0     0    0     0      0    0     1     0     0\n",
              "3    1      0     0    0     0      0    0     0     0     0\n",
              "5    0      0     0    0     0      0    0     1     0     0\n",
              "6    0      0     0    0     0      0    0     1     0     0\n",
              "7    0      0     0    0     1      0    0     0     0     0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d5b17715-bfed-4a00-85cd-837de1e3fdd3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Aut</th>\n",
              "      <th>Class</th>\n",
              "      <th>Itpr</th>\n",
              "      <th>Lit</th>\n",
              "      <th>Prec</th>\n",
              "      <th>Princ</th>\n",
              "      <th>Psy</th>\n",
              "      <th>Rule</th>\n",
              "      <th>Syst</th>\n",
              "      <th>Tele</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d5b17715-bfed-4a00-85cd-837de1e3fdd3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d5b17715-bfed-4a00-85cd-837de1e3fdd3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d5b17715-bfed-4a00-85cd-837de1e3fdd3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"EMBEDDING: \", EMBEDDING)\n",
        "print(\"SHAPE X: \", X.shape)\n",
        "print(\"SHAPE y: \", y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x184U6bDj72i",
        "outputId": "b1ff7d26-a89a-499f-992b-9ce8c8d0dbf6"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EMBEDDING:  legal_bert_sentence\n",
            "SHAPE X:  (577, 512)\n",
            "SHAPE y:  (577, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "nVm1lyk4kC4j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if MODEL == \"RNNout\":\n",
        "  INPUTS = X.shape[-1]\n",
        "  OUTPUTS = y.shape[-1]\n",
        "\n",
        "  def create_deep_model(factor, rate):\n",
        "      model = Sequential()      \n",
        "      model.add(LSTM(units=LAYERS[0], return_sequences = True, input_dim=INPUTS,kernel_initializer='he_uniform', kernel_regularizer=l2(factor), \n",
        "        activation='relu')), Dropout(rate),\n",
        "      model.add(LSTM(units=LAYERS[1], return_sequences = True, kernel_regularizer=l2(factor),\n",
        "        activation='relu')), Dropout(rate),\n",
        "      model.add(LSTM(units=LAYERS[2], kernel_regularizer=l2(factor),\n",
        "        activation='relu')), Dropout(rate),\n",
        "      #Output layer\n",
        "      model.add(Dense(units=OUTPUTS, activation='softmax'))\n",
        "      return model\n",
        "  model= create_deep_model(factor=L2_FACTOR, rate=DROP)\n",
        "\n",
        "  opt=tf.keras.optimizers.Adam(learning_rate=LR)\n",
        "  model.compile(loss='kullback_leibler_divergence', optimizer=opt,   \n",
        "  metrics=['accuracy'])\n",
        "\n",
        "if MODEL == \"RNN\":\n",
        "  INPUTS = X.shape[-1]\n",
        "  OUTPUTS = y.shape[-1]\n",
        "\n",
        "  def create_deep_model(factor, rate):\n",
        "\n",
        "      input= Input(shape=(INPUTS, 1), name=\"input\")\n",
        "      print(input.shape)\n",
        "      x = LSTM(units=LAYERS[0], return_sequences = True,kernel_initializer='he_uniform', kernel_regularizer=l2(factor), activation='relu')(input)\n",
        "      x = Dropout(rate)(x)\n",
        "      x = LSTM(units=LAYERS[1], return_sequences = True,kernel_initializer='he_uniform', kernel_regularizer=l2(factor), activation='relu')(x)\n",
        "      x = Dropout(rate)(x)\n",
        "      x = LSTM(units=LAYERS[2], kernel_regularizer=l2(factor), activation='relu')(x)\n",
        "      x = Dropout(rate)(x)\n",
        "      #Output layer\n",
        "      aut_output  = Dense(units=1, activation='sigmoid', name=\"aut\")(x)\n",
        "      class_output  = Dense(units=1, activation='sigmoid', name=\"class\")(x)\n",
        "      itpr_output  = Dense(units=1, activation='sigmoid', name=\"itpr\")(x)\n",
        "      lit_output  = Dense(units=1, activation='sigmoid', name=\"lit\")(x)\n",
        "      prec_output  = Dense(units=1, activation='sigmoid', name=\"prec\")(x)\n",
        "      princ_output  = Dense(units=1, activation='sigmoid', name=\"princ\")(x)\n",
        "      psy_output  = Dense(units=1, activation='sigmoid', name=\"psy\")(x)\n",
        "      rule_output  = Dense(units=1, activation='sigmoid', name=\"rule\")(x)\n",
        "      syst_output  = Dense(units=1, activation='sigmoid', name=\"syst\")(x)\n",
        "      tele_output  = Dense(units=1, activation='sigmoid', name=\"tele\")(x)\n",
        "      model = Model(\n",
        "        inputs = input,\n",
        "        outputs = [\n",
        "          aut_output,\n",
        "          class_output,\n",
        "          itpr_output,\n",
        "          lit_output,\n",
        "          prec_output,\n",
        "          princ_output,\n",
        "          psy_output,\n",
        "          rule_output,\n",
        "          syst_output,\n",
        "          tele_output]\n",
        "    )\n",
        "      return model\n",
        "  model= create_deep_model(factor=L2_FACTOR, rate=DROP)\n",
        "\n",
        "  opt=tf.keras.optimizers.Adam(learning_rate=LR, clipnorm=1.0, clipvalue=0.5)\n",
        "  # opt = tf.keras.optimizers.SGD(learning_rate=LR, momentum=0.9)\n",
        "  loss = tf.keras.losses.BinaryCrossentropy(axis=1)\n",
        "  model.compile(loss=loss, optimizer=opt,   \n",
        "  metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "BE6OhtLQFmjA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93fe71c0-1cf4-4f65-a762-9556095f7945"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, 512, 1)\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, shuffle= True)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=VAL_SIZE, shuffle= True)\n",
        "\n",
        "#fit the model\n",
        "history=model.fit(x=X_train, y={\n",
        "                  \"aut\": y_train['Aut'],\n",
        "                  \"class\": y_train[\"Class\"], \n",
        "                  \"itpr\": y_train[\"Itpr\"],\n",
        "                  \"lit\": y_train[\"Lit\"],  \n",
        "                  \"prec\": y_train[\"Prec\"],  \n",
        "                  \"princ\": y_train[\"Princ\"],  \n",
        "                  \"psy\": y_train[\"Psy\"],  \n",
        "                  \"rule\": y_train[\"Rule\"],  \n",
        "                  \"syst\": y_train[\"Syst\"],   \n",
        "                  \"tele\": y_train['Tele'] \n",
        "                  },\n",
        "                  batch_size=BATCH_SIZE, epochs=EPOCHS,\n",
        "                  validation_data=(X_val, {\n",
        "                  \"aut\": y_val['Aut'],\n",
        "                  \"class\": y_val[\"Class\"], \n",
        "                  \"itpr\": y_val[\"Itpr\"],\n",
        "                  \"lit\": y_val[\"Lit\"],  \n",
        "                  \"prec\": y_val[\"Prec\"],  \n",
        "                  \"princ\": y_val[\"Princ\"],  \n",
        "                  \"psy\": y_val[\"Psy\"],  \n",
        "                  \"rule\": y_val[\"Rule\"],  \n",
        "                  \"syst\": y_val[\"Syst\"],   \n",
        "                  \"tele\": y_val['Tele'] }),\n",
        "                  verbose=1)"
      ],
      "metadata": {
        "id": "2hMuzyxUFtDW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6164ce1-f55c-4fdb-ad58-860e3dc864a2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "6/6 [==============================] - 49s 4s/step - loss: 305.9304 - aut_loss: 29.4295 - class_loss: 35.9206 - itpr_loss: 21.5484 - lit_loss: 0.6737 - prec_loss: 129.6910 - princ_loss: 2.9515 - psy_loss: 10.2064 - rule_loss: 65.3416 - syst_loss: 6.4523 - tele_loss: 0.5179 - aut_accuracy: 0.9596 - class_accuracy: 0.0280 - itpr_accuracy: 0.7019 - lit_accuracy: 0.5994 - prec_accuracy: 0.4814 - princ_accuracy: 0.8851 - psy_accuracy: 0.6149 - rule_accuracy: 0.2081 - syst_accuracy: 0.1708 - tele_accuracy: 0.8385 - val_loss: 75.7199 - val_aut_loss: 24.0475 - val_class_loss: 1.4918 - val_itpr_loss: 0.6118 - val_lit_loss: 0.6007 - val_prec_loss: 0.9617 - val_princ_loss: 0.6678 - val_psy_loss: 30.1985 - val_rule_loss: 10.8246 - val_syst_loss: 2.6546 - val_tele_loss: 0.4633 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0123 - val_itpr_accuracy: 0.8889 - val_lit_accuracy: 0.9753 - val_prec_accuracy: 0.3210 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.8642 - val_rule_accuracy: 0.2222 - val_syst_accuracy: 0.0000e+00 - val_tele_accuracy: 1.0000\n",
            "Epoch 2/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 2754.9531 - aut_loss: 336.6105 - class_loss: 515.6829 - itpr_loss: 283.2411 - lit_loss: 165.4499 - prec_loss: 727.5039 - princ_loss: 3.2264 - psy_loss: 220.4507 - rule_loss: 168.1797 - syst_loss: 330.7948 - tele_loss: 0.6159 - aut_accuracy: 0.9565 - class_accuracy: 0.0124 - itpr_accuracy: 0.7391 - lit_accuracy: 0.6584 - prec_accuracy: 0.4627 - princ_accuracy: 0.8882 - psy_accuracy: 0.5652 - rule_accuracy: 0.2143 - syst_accuracy: 0.1460 - tele_accuracy: 0.8727 - val_loss: 10.7619 - val_aut_loss: 0.4688 - val_class_loss: 1.4505 - val_itpr_loss: 0.6183 - val_lit_loss: 0.6123 - val_prec_loss: 0.9296 - val_princ_loss: 0.6074 - val_psy_loss: 0.6386 - val_rule_loss: 0.8869 - val_syst_loss: 0.8814 - val_tele_loss: 0.4706 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0000e+00 - val_itpr_accuracy: 0.8889 - val_lit_accuracy: 0.9753 - val_prec_accuracy: 0.3210 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.8765 - val_rule_accuracy: 0.2222 - val_syst_accuracy: 0.0000e+00 - val_tele_accuracy: 1.0000\n",
            "Epoch 3/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 8257.8008 - aut_loss: 5.3566 - class_loss: 2525.5969 - itpr_loss: 0.9016 - lit_loss: 0.6611 - prec_loss: 1.2224 - princ_loss: 3154.9426 - psy_loss: 3.1155 - rule_loss: 2104.0010 - syst_loss: 458.2798 - tele_loss: 0.5250 - aut_accuracy: 0.9689 - class_accuracy: 0.0248 - itpr_accuracy: 0.6770 - lit_accuracy: 0.5963 - prec_accuracy: 0.4689 - princ_accuracy: 0.8882 - psy_accuracy: 0.5776 - rule_accuracy: 0.2205 - syst_accuracy: 0.1646 - tele_accuracy: 0.8416 - val_loss: 10.6067 - val_aut_loss: 0.4707 - val_class_loss: 1.3951 - val_itpr_loss: 0.6189 - val_lit_loss: 0.6166 - val_prec_loss: 0.9035 - val_princ_loss: 0.5549 - val_psy_loss: 0.6441 - val_rule_loss: 0.8594 - val_syst_loss: 0.8719 - val_tele_loss: 0.4741 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0000e+00 - val_itpr_accuracy: 0.8889 - val_lit_accuracy: 0.9630 - val_prec_accuracy: 0.3210 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.8642 - val_rule_accuracy: 0.2222 - val_syst_accuracy: 0.0000e+00 - val_tele_accuracy: 1.0000\n",
            "Epoch 4/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 1118395301888.0000 - aut_loss: 696.9431 - class_loss: 10.3636 - itpr_loss: 2188.2207 - lit_loss: 274440781824.0000 - prec_loss: 6.8606 - princ_loss: 1180.4846 - psy_loss: 521778823168.0000 - rule_loss: 322175696896.0000 - syst_loss: 5.7291 - tele_loss: 0.5145 - aut_accuracy: 0.9534 - class_accuracy: 0.0559 - itpr_accuracy: 0.6957 - lit_accuracy: 0.6429 - prec_accuracy: 0.4845 - princ_accuracy: 0.8820 - psy_accuracy: 0.5590 - rule_accuracy: 0.2422 - syst_accuracy: 0.1584 - tele_accuracy: 0.8634 - val_loss: 10.4984 - val_aut_loss: 0.4722 - val_class_loss: 1.3554 - val_itpr_loss: 0.6196 - val_lit_loss: 0.6196 - val_prec_loss: 0.8857 - val_princ_loss: 0.5187 - val_psy_loss: 0.6486 - val_rule_loss: 0.8398 - val_syst_loss: 0.8645 - val_tele_loss: 0.4769 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0000e+00 - val_itpr_accuracy: 0.8889 - val_lit_accuracy: 0.9630 - val_prec_accuracy: 0.3210 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.8519 - val_rule_accuracy: 0.2222 - val_syst_accuracy: 0.0000e+00 - val_tele_accuracy: 1.0000\n",
            "Epoch 5/200\n",
            "6/6 [==============================] - 24s 4s/step - loss: 39.7523 - aut_loss: 1.6126 - class_loss: 3.8946 - itpr_loss: 5.6301 - lit_loss: 0.6749 - prec_loss: 1.9835 - princ_loss: 8.9855 - psy_loss: 1.4377 - rule_loss: 4.7660 - syst_loss: 7.0408 - tele_loss: 0.5289 - aut_accuracy: 0.9627 - class_accuracy: 0.0093 - itpr_accuracy: 0.6770 - lit_accuracy: 0.5870 - prec_accuracy: 0.4658 - princ_accuracy: 0.8851 - psy_accuracy: 0.5714 - rule_accuracy: 0.1863 - syst_accuracy: 0.1460 - tele_accuracy: 0.8354 - val_loss: 10.3886 - val_aut_loss: 0.4744 - val_class_loss: 1.3133 - val_itpr_loss: 0.6207 - val_lit_loss: 0.6232 - val_prec_loss: 0.8670 - val_princ_loss: 0.4826 - val_psy_loss: 0.6538 - val_rule_loss: 0.8197 - val_syst_loss: 0.8560 - val_tele_loss: 0.4804 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0000e+00 - val_itpr_accuracy: 0.8889 - val_lit_accuracy: 0.9630 - val_prec_accuracy: 0.3210 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.8272 - val_rule_accuracy: 0.2222 - val_syst_accuracy: 0.0000e+00 - val_tele_accuracy: 1.0000\n",
            "Epoch 6/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 1175.3340 - aut_loss: 521.4946 - class_loss: 1.3625 - itpr_loss: 0.6519 - lit_loss: 0.6777 - prec_loss: 0.7829 - princ_loss: 0.8323 - psy_loss: 643.9681 - rule_loss: 0.9886 - syst_loss: 0.8570 - tele_loss: 0.5208 - aut_accuracy: 0.9596 - class_accuracy: 0.0435 - itpr_accuracy: 0.7236 - lit_accuracy: 0.5652 - prec_accuracy: 0.4783 - princ_accuracy: 0.8882 - psy_accuracy: 0.5901 - rule_accuracy: 0.2236 - syst_accuracy: 0.1553 - tele_accuracy: 0.8540 - val_loss: 10.3028 - val_aut_loss: 0.4765 - val_class_loss: 1.2783 - val_itpr_loss: 0.6221 - val_lit_loss: 0.6265 - val_prec_loss: 0.8514 - val_princ_loss: 0.4552 - val_psy_loss: 0.6587 - val_rule_loss: 0.8044 - val_syst_loss: 0.8482 - val_tele_loss: 0.4839 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0000e+00 - val_itpr_accuracy: 0.8889 - val_lit_accuracy: 0.9630 - val_prec_accuracy: 0.3210 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.7778 - val_rule_accuracy: 0.2222 - val_syst_accuracy: 0.0000e+00 - val_tele_accuracy: 1.0000\n",
            "Epoch 7/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 135138.3125 - aut_loss: 0.4494 - class_loss: 1.2036 - itpr_loss: 0.6193 - lit_loss: 46875.1445 - prec_loss: 0.8063 - princ_loss: 0.5508 - psy_loss: 51037.1602 - rule_loss: 37217.8438 - syst_loss: 0.8273 - tele_loss: 0.5284 - aut_accuracy: 0.9720 - class_accuracy: 0.0311 - itpr_accuracy: 0.7422 - lit_accuracy: 0.5435 - prec_accuracy: 0.4627 - princ_accuracy: 0.8882 - psy_accuracy: 0.5062 - rule_accuracy: 0.2174 - syst_accuracy: 0.1863 - tele_accuracy: 0.8354 - val_loss: 10.2330 - val_aut_loss: 0.4788 - val_class_loss: 1.2476 - val_itpr_loss: 0.6236 - val_lit_loss: 0.6303 - val_prec_loss: 0.8367 - val_princ_loss: 0.4340 - val_psy_loss: 0.6639 - val_rule_loss: 0.7923 - val_syst_loss: 0.8408 - val_tele_loss: 0.4874 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0000e+00 - val_itpr_accuracy: 0.8889 - val_lit_accuracy: 0.9630 - val_prec_accuracy: 0.3210 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.7037 - val_rule_accuracy: 0.2222 - val_syst_accuracy: 0.0000e+00 - val_tele_accuracy: 1.0000\n",
            "Epoch 8/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 2918.4834 - aut_loss: 210.4683 - class_loss: 1.1246 - itpr_loss: 880.5920 - lit_loss: 35.5003 - prec_loss: 609.4899 - princ_loss: 256.6281 - psy_loss: 538.5576 - rule_loss: 0.8150 - syst_loss: 380.8257 - tele_loss: 1.2846 - aut_accuracy: 0.9720 - class_accuracy: 0.0373 - itpr_accuracy: 0.7391 - lit_accuracy: 0.5963 - prec_accuracy: 0.4627 - princ_accuracy: 0.8882 - psy_accuracy: 0.5031 - rule_accuracy: 0.2174 - syst_accuracy: 0.1398 - tele_accuracy: 0.8385 - val_loss: 10.1897 - val_aut_loss: 0.4806 - val_class_loss: 1.2269 - val_itpr_loss: 0.6248 - val_lit_loss: 0.6332 - val_prec_loss: 0.8264 - val_princ_loss: 0.4221 - val_psy_loss: 0.6676 - val_rule_loss: 0.7849 - val_syst_loss: 0.8356 - val_tele_loss: 0.4901 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0000e+00 - val_itpr_accuracy: 0.8889 - val_lit_accuracy: 0.9630 - val_prec_accuracy: 0.3210 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.6914 - val_rule_accuracy: 0.2222 - val_syst_accuracy: 0.0000e+00 - val_tele_accuracy: 1.0000\n",
            "Epoch 9/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 38.6463 - aut_loss: 0.5285 - class_loss: 1.5329 - itpr_loss: 12.6971 - lit_loss: 0.6688 - prec_loss: 0.7561 - princ_loss: 0.5003 - psy_loss: 8.5183 - rule_loss: 0.9510 - syst_loss: 8.7688 - tele_loss: 0.5271 - aut_accuracy: 0.9720 - class_accuracy: 0.0373 - itpr_accuracy: 0.7236 - lit_accuracy: 0.5714 - prec_accuracy: 0.4752 - princ_accuracy: 0.8882 - psy_accuracy: 0.4720 - rule_accuracy: 0.2174 - syst_accuracy: 0.1491 - tele_accuracy: 0.8509 - val_loss: 10.1559 - val_aut_loss: 0.4823 - val_class_loss: 1.2095 - val_itpr_loss: 0.6259 - val_lit_loss: 0.6359 - val_prec_loss: 0.8174 - val_princ_loss: 0.4137 - val_psy_loss: 0.6710 - val_rule_loss: 0.7790 - val_syst_loss: 0.8311 - val_tele_loss: 0.4925 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0000e+00 - val_itpr_accuracy: 0.8889 - val_lit_accuracy: 0.9630 - val_prec_accuracy: 0.3210 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.6667 - val_rule_accuracy: 0.2222 - val_syst_accuracy: 0.0000e+00 - val_tele_accuracy: 1.0000\n",
            "Epoch 10/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 101.9754 - aut_loss: 17.9992 - class_loss: 1.4514 - itpr_loss: 32.7692 - lit_loss: 0.6644 - prec_loss: 0.7636 - princ_loss: 10.0323 - psy_loss: 22.4345 - rule_loss: 1.0429 - syst_loss: 11.0920 - tele_loss: 0.5284 - aut_accuracy: 0.9627 - class_accuracy: 0.0217 - itpr_accuracy: 0.7236 - lit_accuracy: 0.5807 - prec_accuracy: 0.4503 - princ_accuracy: 0.8851 - psy_accuracy: 0.5155 - rule_accuracy: 0.2081 - syst_accuracy: 0.1460 - tele_accuracy: 0.8820 - val_loss: 10.1240 - val_aut_loss: 0.4844 - val_class_loss: 1.1914 - val_itpr_loss: 0.6271 - val_lit_loss: 0.6391 - val_prec_loss: 0.8080 - val_princ_loss: 0.4063 - val_psy_loss: 0.6746 - val_rule_loss: 0.7734 - val_syst_loss: 0.8265 - val_tele_loss: 0.4956 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0000e+00 - val_itpr_accuracy: 0.8889 - val_lit_accuracy: 0.9630 - val_prec_accuracy: 0.3210 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.6296 - val_rule_accuracy: 0.2222 - val_syst_accuracy: 0.0000e+00 - val_tele_accuracy: 1.0000\n",
            "Epoch 11/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 10.2027 - aut_loss: 0.4532 - class_loss: 1.1486 - itpr_loss: 0.6388 - lit_loss: 0.6811 - prec_loss: 0.7553 - princ_loss: 0.4554 - psy_loss: 0.7057 - rule_loss: 0.8149 - syst_loss: 0.8250 - tele_loss: 0.5272 - aut_accuracy: 0.9720 - class_accuracy: 0.0373 - itpr_accuracy: 0.7112 - lit_accuracy: 0.5652 - prec_accuracy: 0.4845 - princ_accuracy: 0.8882 - psy_accuracy: 0.4503 - rule_accuracy: 0.2547 - syst_accuracy: 0.1615 - tele_accuracy: 0.8696 - val_loss: 10.0980 - val_aut_loss: 0.4862 - val_class_loss: 1.1756 - val_itpr_loss: 0.6282 - val_lit_loss: 0.6417 - val_prec_loss: 0.8001 - val_princ_loss: 0.4014 - val_psy_loss: 0.6782 - val_rule_loss: 0.7687 - val_syst_loss: 0.8223 - val_tele_loss: 0.4981 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0000e+00 - val_itpr_accuracy: 0.8889 - val_lit_accuracy: 0.9630 - val_prec_accuracy: 0.3210 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.6173 - val_rule_accuracy: 0.2222 - val_syst_accuracy: 0.0000e+00 - val_tele_accuracy: 1.0000\n",
            "Epoch 12/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 15.1134 - aut_loss: 1.5970 - class_loss: 1.0973 - itpr_loss: 0.6388 - lit_loss: 0.6835 - prec_loss: 2.6757 - princ_loss: 0.4485 - psy_loss: 2.0468 - rule_loss: 0.8171 - syst_loss: 1.3785 - tele_loss: 0.5327 - aut_accuracy: 0.9720 - class_accuracy: 0.0124 - itpr_accuracy: 0.7516 - lit_accuracy: 0.5683 - prec_accuracy: 0.4224 - princ_accuracy: 0.8882 - psy_accuracy: 0.5248 - rule_accuracy: 0.2267 - syst_accuracy: 0.1832 - tele_accuracy: 0.8478 - val_loss: 10.0754 - val_aut_loss: 0.4878 - val_class_loss: 1.1612 - val_itpr_loss: 0.6292 - val_lit_loss: 0.6440 - val_prec_loss: 0.7926 - val_princ_loss: 0.3979 - val_psy_loss: 0.6819 - val_rule_loss: 0.7645 - val_syst_loss: 0.8185 - val_tele_loss: 0.5004 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0000e+00 - val_itpr_accuracy: 0.8889 - val_lit_accuracy: 0.9630 - val_prec_accuracy: 0.3210 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.5309 - val_rule_accuracy: 0.2222 - val_syst_accuracy: 0.0000e+00 - val_tele_accuracy: 1.0000\n",
            "Epoch 13/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 10.1244 - aut_loss: 0.4529 - class_loss: 1.1006 - itpr_loss: 0.6368 - lit_loss: 0.6817 - prec_loss: 0.7371 - princ_loss: 0.4370 - psy_loss: 0.6978 - rule_loss: 0.8121 - syst_loss: 0.8306 - tele_loss: 0.5404 - aut_accuracy: 0.9689 - class_accuracy: 0.0311 - itpr_accuracy: 0.7360 - lit_accuracy: 0.5994 - prec_accuracy: 0.4783 - princ_accuracy: 0.8882 - psy_accuracy: 0.4565 - rule_accuracy: 0.2174 - syst_accuracy: 0.1770 - tele_accuracy: 0.8882 - val_loss: 10.0547 - val_aut_loss: 0.4894 - val_class_loss: 1.1472 - val_itpr_loss: 0.6301 - val_lit_loss: 0.6462 - val_prec_loss: 0.7852 - val_princ_loss: 0.3953 - val_psy_loss: 0.6860 - val_rule_loss: 0.7606 - val_syst_loss: 0.8146 - val_tele_loss: 0.5025 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0000e+00 - val_itpr_accuracy: 0.8889 - val_lit_accuracy: 0.9630 - val_prec_accuracy: 0.3210 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.4938 - val_rule_accuracy: 0.2222 - val_syst_accuracy: 0.0000e+00 - val_tele_accuracy: 1.0000\n",
            "Epoch 14/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 10.0973 - aut_loss: 0.4559 - class_loss: 1.0683 - itpr_loss: 0.6530 - lit_loss: 0.6776 - prec_loss: 0.7496 - princ_loss: 0.4205 - psy_loss: 0.7031 - rule_loss: 0.7963 - syst_loss: 0.8286 - tele_loss: 0.5467 - aut_accuracy: 0.9720 - class_accuracy: 0.0435 - itpr_accuracy: 0.6677 - lit_accuracy: 0.5776 - prec_accuracy: 0.4534 - princ_accuracy: 0.8882 - psy_accuracy: 0.4876 - rule_accuracy: 0.2236 - syst_accuracy: 0.1460 - tele_accuracy: 0.8323 - val_loss: 10.0355 - val_aut_loss: 0.4909 - val_class_loss: 1.1341 - val_itpr_loss: 0.6310 - val_lit_loss: 0.6480 - val_prec_loss: 0.7781 - val_princ_loss: 0.3935 - val_psy_loss: 0.6902 - val_rule_loss: 0.7569 - val_syst_loss: 0.8109 - val_tele_loss: 0.5042 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0000e+00 - val_itpr_accuracy: 0.8889 - val_lit_accuracy: 0.9630 - val_prec_accuracy: 0.3210 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.4568 - val_rule_accuracy: 0.2222 - val_syst_accuracy: 0.0000e+00 - val_tele_accuracy: 1.0000\n",
            "Epoch 15/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 10.1448 - aut_loss: 0.4500 - class_loss: 1.0939 - itpr_loss: 0.6395 - lit_loss: 0.6908 - prec_loss: 0.7708 - princ_loss: 0.4263 - psy_loss: 0.7054 - rule_loss: 0.8028 - syst_loss: 0.8200 - tele_loss: 0.5478 - aut_accuracy: 0.9720 - class_accuracy: 0.0217 - itpr_accuracy: 0.7205 - lit_accuracy: 0.5342 - prec_accuracy: 0.4503 - princ_accuracy: 0.8882 - psy_accuracy: 0.4876 - rule_accuracy: 0.2267 - syst_accuracy: 0.1522 - tele_accuracy: 0.8665 - val_loss: 10.0189 - val_aut_loss: 0.4921 - val_class_loss: 1.1228 - val_itpr_loss: 0.6318 - val_lit_loss: 0.6494 - val_prec_loss: 0.7720 - val_princ_loss: 0.3924 - val_psy_loss: 0.6942 - val_rule_loss: 0.7537 - val_syst_loss: 0.8075 - val_tele_loss: 0.5055 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0000e+00 - val_itpr_accuracy: 0.8889 - val_lit_accuracy: 0.9630 - val_prec_accuracy: 0.3210 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.4198 - val_rule_accuracy: 0.2222 - val_syst_accuracy: 0.0000e+00 - val_tele_accuracy: 1.0000\n",
            "Epoch 16/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 10.2034 - aut_loss: 0.4521 - class_loss: 1.1206 - itpr_loss: 0.6419 - lit_loss: 0.6860 - prec_loss: 0.7453 - princ_loss: 0.4768 - psy_loss: 0.7193 - rule_loss: 0.8032 - syst_loss: 0.8351 - tele_loss: 0.5257 - aut_accuracy: 0.9720 - class_accuracy: 0.0373 - itpr_accuracy: 0.6988 - lit_accuracy: 0.5497 - prec_accuracy: 0.4876 - princ_accuracy: 0.8882 - psy_accuracy: 0.3913 - rule_accuracy: 0.2516 - syst_accuracy: 0.1708 - tele_accuracy: 0.8975 - val_loss: 10.0032 - val_aut_loss: 0.4935 - val_class_loss: 1.1114 - val_itpr_loss: 0.6325 - val_lit_loss: 0.6507 - val_prec_loss: 0.7658 - val_princ_loss: 0.3919 - val_psy_loss: 0.6982 - val_rule_loss: 0.7504 - val_syst_loss: 0.8041 - val_tele_loss: 0.5071 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0000e+00 - val_itpr_accuracy: 0.8889 - val_lit_accuracy: 0.9630 - val_prec_accuracy: 0.3210 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.4074 - val_rule_accuracy: 0.2222 - val_syst_accuracy: 0.0000e+00 - val_tele_accuracy: 1.0000\n",
            "Epoch 17/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 10.5742 - aut_loss: 0.4655 - class_loss: 1.1661 - itpr_loss: 0.6491 - lit_loss: 0.6829 - prec_loss: 0.7200 - princ_loss: 0.7033 - psy_loss: 0.7160 - rule_loss: 0.9007 - syst_loss: 0.8226 - tele_loss: 0.5504 - aut_accuracy: 0.9720 - class_accuracy: 0.0280 - itpr_accuracy: 0.7050 - lit_accuracy: 0.5776 - prec_accuracy: 0.5031 - princ_accuracy: 0.8882 - psy_accuracy: 0.4410 - rule_accuracy: 0.2484 - syst_accuracy: 0.1708 - tele_accuracy: 0.8478 - val_loss: 9.9885 - val_aut_loss: 0.4950 - val_class_loss: 1.1000 - val_itpr_loss: 0.6333 - val_lit_loss: 0.6521 - val_prec_loss: 0.7597 - val_princ_loss: 0.3917 - val_psy_loss: 0.7025 - val_rule_loss: 0.7471 - val_syst_loss: 0.8007 - val_tele_loss: 0.5089 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0000e+00 - val_itpr_accuracy: 0.8889 - val_lit_accuracy: 0.9630 - val_prec_accuracy: 0.3210 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.3951 - val_rule_accuracy: 0.2222 - val_syst_accuracy: 0.0000e+00 - val_tele_accuracy: 1.0000\n",
            "Epoch 18/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 10.1263 - aut_loss: 0.4638 - class_loss: 1.0731 - itpr_loss: 0.6349 - lit_loss: 0.6878 - prec_loss: 0.7616 - princ_loss: 0.4308 - psy_loss: 0.7280 - rule_loss: 0.8076 - syst_loss: 0.8061 - tele_loss: 0.5352 - aut_accuracy: 0.9720 - class_accuracy: 0.0155 - itpr_accuracy: 0.6957 - lit_accuracy: 0.5404 - prec_accuracy: 0.5062 - princ_accuracy: 0.8882 - psy_accuracy: 0.3851 - rule_accuracy: 0.2205 - syst_accuracy: 0.2019 - tele_accuracy: 0.8944 - val_loss: 9.9750 - val_aut_loss: 0.4963 - val_class_loss: 1.0897 - val_itpr_loss: 0.6341 - val_lit_loss: 0.6532 - val_prec_loss: 0.7542 - val_princ_loss: 0.3918 - val_psy_loss: 0.7063 - val_rule_loss: 0.7441 - val_syst_loss: 0.7975 - val_tele_loss: 0.5103 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0000e+00 - val_itpr_accuracy: 0.8889 - val_lit_accuracy: 0.9630 - val_prec_accuracy: 0.3210 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.3704 - val_rule_accuracy: 0.2222 - val_syst_accuracy: 0.0000e+00 - val_tele_accuracy: 1.0000\n",
            "Epoch 19/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 10.0844 - aut_loss: 0.4513 - class_loss: 1.0668 - itpr_loss: 0.6380 - lit_loss: 0.6922 - prec_loss: 0.7602 - princ_loss: 0.4222 - psy_loss: 0.7318 - rule_loss: 0.7949 - syst_loss: 0.7984 - tele_loss: 0.5312 - aut_accuracy: 0.9720 - class_accuracy: 0.0280 - itpr_accuracy: 0.7112 - lit_accuracy: 0.5528 - prec_accuracy: 0.4348 - princ_accuracy: 0.8882 - psy_accuracy: 0.4006 - rule_accuracy: 0.2453 - syst_accuracy: 0.1832 - tele_accuracy: 0.8789 - val_loss: 9.9618 - val_aut_loss: 0.4975 - val_class_loss: 1.0796 - val_itpr_loss: 0.6348 - val_lit_loss: 0.6541 - val_prec_loss: 0.7488 - val_princ_loss: 0.3921 - val_psy_loss: 0.7103 - val_rule_loss: 0.7411 - val_syst_loss: 0.7944 - val_tele_loss: 0.5116 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0000e+00 - val_itpr_accuracy: 0.8889 - val_lit_accuracy: 0.9630 - val_prec_accuracy: 0.3210 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.3457 - val_rule_accuracy: 0.2222 - val_syst_accuracy: 0.0000e+00 - val_tele_accuracy: 1.0000\n",
            "Epoch 20/200\n",
            "6/6 [==============================] - 23s 4s/step - loss: 10.0119 - aut_loss: 0.4638 - class_loss: 1.0247 - itpr_loss: 0.6416 - lit_loss: 0.6985 - prec_loss: 0.7251 - princ_loss: 0.4165 - psy_loss: 0.7413 - rule_loss: 0.7791 - syst_loss: 0.7874 - tele_loss: 0.5366 - aut_accuracy: 0.9720 - class_accuracy: 0.0435 - itpr_accuracy: 0.7298 - lit_accuracy: 0.5683 - prec_accuracy: 0.4689 - princ_accuracy: 0.8882 - psy_accuracy: 0.3509 - rule_accuracy: 0.2733 - syst_accuracy: 0.2298 - tele_accuracy: 0.8665 - val_loss: 9.9493 - val_aut_loss: 0.4984 - val_class_loss: 1.0704 - val_itpr_loss: 0.6354 - val_lit_loss: 0.6548 - val_prec_loss: 0.7439 - val_princ_loss: 0.3924 - val_psy_loss: 0.7143 - val_rule_loss: 0.7384 - val_syst_loss: 0.7915 - val_tele_loss: 0.5124 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0000e+00 - val_itpr_accuracy: 0.8889 - val_lit_accuracy: 0.9630 - val_prec_accuracy: 0.3210 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.3086 - val_rule_accuracy: 0.2222 - val_syst_accuracy: 0.0000e+00 - val_tele_accuracy: 1.0000\n",
            "Epoch 21/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 10.0586 - aut_loss: 0.4543 - class_loss: 1.0334 - itpr_loss: 0.6421 - lit_loss: 0.6841 - prec_loss: 0.7394 - princ_loss: 0.4297 - psy_loss: 0.7442 - rule_loss: 0.7897 - syst_loss: 0.8024 - tele_loss: 0.5418 - aut_accuracy: 0.9720 - class_accuracy: 0.0404 - itpr_accuracy: 0.6988 - lit_accuracy: 0.5652 - prec_accuracy: 0.5062 - princ_accuracy: 0.8882 - psy_accuracy: 0.3354 - rule_accuracy: 0.2360 - syst_accuracy: 0.1801 - tele_accuracy: 0.8882 - val_loss: 9.9372 - val_aut_loss: 0.4992 - val_class_loss: 1.0616 - val_itpr_loss: 0.6360 - val_lit_loss: 0.6552 - val_prec_loss: 0.7393 - val_princ_loss: 0.3927 - val_psy_loss: 0.7183 - val_rule_loss: 0.7358 - val_syst_loss: 0.7887 - val_tele_loss: 0.5129 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0000e+00 - val_itpr_accuracy: 0.8889 - val_lit_accuracy: 0.9630 - val_prec_accuracy: 0.3210 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.2593 - val_rule_accuracy: 0.2222 - val_syst_accuracy: 0.0000e+00 - val_tele_accuracy: 1.0000\n",
            "Epoch 22/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 10.0011 - aut_loss: 0.4612 - class_loss: 1.0306 - itpr_loss: 0.6391 - lit_loss: 0.6810 - prec_loss: 0.7232 - princ_loss: 0.4202 - psy_loss: 0.7426 - rule_loss: 0.7748 - syst_loss: 0.7962 - tele_loss: 0.5347 - aut_accuracy: 0.9720 - class_accuracy: 0.0248 - itpr_accuracy: 0.7267 - lit_accuracy: 0.5714 - prec_accuracy: 0.5124 - princ_accuracy: 0.8882 - psy_accuracy: 0.3602 - rule_accuracy: 0.2516 - syst_accuracy: 0.1677 - tele_accuracy: 0.9099 - val_loss: 9.9260 - val_aut_loss: 0.4997 - val_class_loss: 1.0538 - val_itpr_loss: 0.6366 - val_lit_loss: 0.6553 - val_prec_loss: 0.7355 - val_princ_loss: 0.3930 - val_psy_loss: 0.7220 - val_rule_loss: 0.7333 - val_syst_loss: 0.7862 - val_tele_loss: 0.5130 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0000e+00 - val_itpr_accuracy: 0.8889 - val_lit_accuracy: 0.9630 - val_prec_accuracy: 0.3210 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.2099 - val_rule_accuracy: 0.2222 - val_syst_accuracy: 0.0000e+00 - val_tele_accuracy: 1.0000\n",
            "Epoch 23/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 10.0158 - aut_loss: 0.4645 - class_loss: 1.0246 - itpr_loss: 0.6326 - lit_loss: 0.6812 - prec_loss: 0.7484 - princ_loss: 0.4247 - psy_loss: 0.7480 - rule_loss: 0.7756 - syst_loss: 0.7821 - tele_loss: 0.5365 - aut_accuracy: 0.9689 - class_accuracy: 0.0280 - itpr_accuracy: 0.7267 - lit_accuracy: 0.5932 - prec_accuracy: 0.4130 - princ_accuracy: 0.8882 - psy_accuracy: 0.3292 - rule_accuracy: 0.2702 - syst_accuracy: 0.2143 - tele_accuracy: 0.8944 - val_loss: 9.9155 - val_aut_loss: 0.5000 - val_class_loss: 1.0467 - val_itpr_loss: 0.6372 - val_lit_loss: 0.6552 - val_prec_loss: 0.7322 - val_princ_loss: 0.3933 - val_psy_loss: 0.7255 - val_rule_loss: 0.7311 - val_syst_loss: 0.7840 - val_tele_loss: 0.5129 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0000e+00 - val_itpr_accuracy: 0.8889 - val_lit_accuracy: 0.9630 - val_prec_accuracy: 0.3210 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.2099 - val_rule_accuracy: 0.2346 - val_syst_accuracy: 0.0000e+00 - val_tele_accuracy: 1.0000\n",
            "Epoch 24/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 10.0079 - aut_loss: 0.4538 - class_loss: 1.0312 - itpr_loss: 0.6334 - lit_loss: 0.6760 - prec_loss: 0.7509 - princ_loss: 0.4164 - psy_loss: 0.7437 - rule_loss: 0.7647 - syst_loss: 0.7957 - tele_loss: 0.5448 - aut_accuracy: 0.9689 - class_accuracy: 0.0497 - itpr_accuracy: 0.7112 - lit_accuracy: 0.6398 - prec_accuracy: 0.4783 - princ_accuracy: 0.8882 - psy_accuracy: 0.3447 - rule_accuracy: 0.2640 - syst_accuracy: 0.1770 - tele_accuracy: 0.8540 - val_loss: 9.9052 - val_aut_loss: 0.5003 - val_class_loss: 1.0395 - val_itpr_loss: 0.6377 - val_lit_loss: 0.6550 - val_prec_loss: 0.7288 - val_princ_loss: 0.3937 - val_psy_loss: 0.7291 - val_rule_loss: 0.7291 - val_syst_loss: 0.7817 - val_tele_loss: 0.5128 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0000e+00 - val_itpr_accuracy: 0.8889 - val_lit_accuracy: 0.9630 - val_prec_accuracy: 0.3210 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.1852 - val_rule_accuracy: 0.2346 - val_syst_accuracy: 0.0000e+00 - val_tele_accuracy: 1.0000\n",
            "Epoch 25/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 10.0070 - aut_loss: 0.4697 - class_loss: 1.0048 - itpr_loss: 0.6305 - lit_loss: 0.6919 - prec_loss: 0.7349 - princ_loss: 0.4278 - psy_loss: 0.7516 - rule_loss: 0.7639 - syst_loss: 0.7901 - tele_loss: 0.5444 - aut_accuracy: 0.9689 - class_accuracy: 0.0435 - itpr_accuracy: 0.7329 - lit_accuracy: 0.5373 - prec_accuracy: 0.4534 - princ_accuracy: 0.8882 - psy_accuracy: 0.2888 - rule_accuracy: 0.2888 - syst_accuracy: 0.2453 - tele_accuracy: 0.8696 - val_loss: 9.8956 - val_aut_loss: 0.5005 - val_class_loss: 1.0334 - val_itpr_loss: 0.6381 - val_lit_loss: 0.6547 - val_prec_loss: 0.7260 - val_princ_loss: 0.3939 - val_psy_loss: 0.7322 - val_rule_loss: 0.7273 - val_syst_loss: 0.7795 - val_tele_loss: 0.5126 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0000e+00 - val_itpr_accuracy: 0.8889 - val_lit_accuracy: 0.9753 - val_prec_accuracy: 0.3210 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.1481 - val_rule_accuracy: 0.2346 - val_syst_accuracy: 0.0000e+00 - val_tele_accuracy: 1.0000\n",
            "Epoch 26/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 9.9758 - aut_loss: 0.4668 - class_loss: 1.0006 - itpr_loss: 0.6423 - lit_loss: 0.6752 - prec_loss: 0.7201 - princ_loss: 0.4328 - psy_loss: 0.7539 - rule_loss: 0.7603 - syst_loss: 0.7861 - tele_loss: 0.5405 - aut_accuracy: 0.9720 - class_accuracy: 0.0528 - itpr_accuracy: 0.7422 - lit_accuracy: 0.6118 - prec_accuracy: 0.5248 - princ_accuracy: 0.8882 - psy_accuracy: 0.2919 - rule_accuracy: 0.2919 - syst_accuracy: 0.2143 - tele_accuracy: 0.8602 - val_loss: 9.8858 - val_aut_loss: 0.5006 - val_class_loss: 1.0269 - val_itpr_loss: 0.6384 - val_lit_loss: 0.6545 - val_prec_loss: 0.7230 - val_princ_loss: 0.3942 - val_psy_loss: 0.7356 - val_rule_loss: 0.7254 - val_syst_loss: 0.7773 - val_tele_loss: 0.5124 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0000e+00 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 0.9753 - val_prec_accuracy: 0.3333 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.1358 - val_rule_accuracy: 0.2346 - val_syst_accuracy: 0.0000e+00 - val_tele_accuracy: 1.0000\n",
            "Epoch 27/200\n",
            "6/6 [==============================] - 21s 4s/step - loss: 9.9811 - aut_loss: 0.4638 - class_loss: 1.0018 - itpr_loss: 0.6294 - lit_loss: 0.6834 - prec_loss: 0.7342 - princ_loss: 0.4266 - psy_loss: 0.7558 - rule_loss: 0.7661 - syst_loss: 0.7766 - tele_loss: 0.5458 - aut_accuracy: 0.9720 - class_accuracy: 0.0280 - itpr_accuracy: 0.7484 - lit_accuracy: 0.5932 - prec_accuracy: 0.4876 - princ_accuracy: 0.8882 - psy_accuracy: 0.3199 - rule_accuracy: 0.2547 - syst_accuracy: 0.2453 - tele_accuracy: 0.8820 - val_loss: 9.8762 - val_aut_loss: 0.5006 - val_class_loss: 1.0211 - val_itpr_loss: 0.6388 - val_lit_loss: 0.6540 - val_prec_loss: 0.7204 - val_princ_loss: 0.3944 - val_psy_loss: 0.7388 - val_rule_loss: 0.7236 - val_syst_loss: 0.7753 - val_tele_loss: 0.5119 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0000e+00 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 0.9753 - val_prec_accuracy: 0.3333 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.1235 - val_rule_accuracy: 0.2346 - val_syst_accuracy: 0.0000e+00 - val_tele_accuracy: 1.0000\n",
            "Epoch 28/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 9.9680 - aut_loss: 0.4614 - class_loss: 1.0003 - itpr_loss: 0.6279 - lit_loss: 0.6749 - prec_loss: 0.7216 - princ_loss: 0.4272 - psy_loss: 0.7653 - rule_loss: 0.7631 - syst_loss: 0.7837 - tele_loss: 0.5452 - aut_accuracy: 0.9720 - class_accuracy: 0.0342 - itpr_accuracy: 0.7360 - lit_accuracy: 0.6149 - prec_accuracy: 0.4783 - princ_accuracy: 0.8882 - psy_accuracy: 0.2981 - rule_accuracy: 0.3230 - syst_accuracy: 0.1988 - tele_accuracy: 0.8696 - val_loss: 9.8670 - val_aut_loss: 0.5005 - val_class_loss: 1.0155 - val_itpr_loss: 0.6392 - val_lit_loss: 0.6533 - val_prec_loss: 0.7182 - val_princ_loss: 0.3946 - val_psy_loss: 0.7418 - val_rule_loss: 0.7218 - val_syst_loss: 0.7734 - val_tele_loss: 0.5114 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0000e+00 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.3333 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.0988 - val_rule_accuracy: 0.2346 - val_syst_accuracy: 0.0000e+00 - val_tele_accuracy: 1.0000\n",
            "Epoch 29/200\n",
            "6/6 [==============================] - 21s 4s/step - loss: 9.9782 - aut_loss: 0.4681 - class_loss: 0.9996 - itpr_loss: 0.6416 - lit_loss: 0.6893 - prec_loss: 0.7125 - princ_loss: 0.4254 - psy_loss: 0.7523 - rule_loss: 0.7633 - syst_loss: 0.7752 - tele_loss: 0.5535 - aut_accuracy: 0.9720 - class_accuracy: 0.0342 - itpr_accuracy: 0.7267 - lit_accuracy: 0.5745 - prec_accuracy: 0.4969 - princ_accuracy: 0.8882 - psy_accuracy: 0.2981 - rule_accuracy: 0.2640 - syst_accuracy: 0.2329 - tele_accuracy: 0.8540 - val_loss: 9.8579 - val_aut_loss: 0.5004 - val_class_loss: 1.0101 - val_itpr_loss: 0.6397 - val_lit_loss: 0.6524 - val_prec_loss: 0.7162 - val_princ_loss: 0.3950 - val_psy_loss: 0.7444 - val_rule_loss: 0.7200 - val_syst_loss: 0.7715 - val_tele_loss: 0.5109 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0000e+00 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.3333 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.0988 - val_rule_accuracy: 0.2346 - val_syst_accuracy: 0.0000e+00 - val_tele_accuracy: 1.0000\n",
            "Epoch 30/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 9.9251 - aut_loss: 0.4583 - class_loss: 0.9912 - itpr_loss: 0.6328 - lit_loss: 0.6810 - prec_loss: 0.6976 - princ_loss: 0.4255 - psy_loss: 0.7723 - rule_loss: 0.7591 - syst_loss: 0.7738 - tele_loss: 0.5362 - aut_accuracy: 0.9720 - class_accuracy: 0.0311 - itpr_accuracy: 0.7547 - lit_accuracy: 0.6211 - prec_accuracy: 0.5124 - princ_accuracy: 0.8882 - psy_accuracy: 0.2391 - rule_accuracy: 0.2702 - syst_accuracy: 0.2174 - tele_accuracy: 0.8913 - val_loss: 9.8485 - val_aut_loss: 0.5001 - val_class_loss: 1.0047 - val_itpr_loss: 0.6403 - val_lit_loss: 0.6514 - val_prec_loss: 0.7141 - val_princ_loss: 0.3952 - val_psy_loss: 0.7472 - val_rule_loss: 0.7184 - val_syst_loss: 0.7698 - val_tele_loss: 0.5100 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0000e+00 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.3457 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.0864 - val_rule_accuracy: 0.2346 - val_syst_accuracy: 0.0000e+00 - val_tele_accuracy: 1.0000\n",
            "Epoch 31/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 9.9347 - aut_loss: 0.4691 - class_loss: 0.9678 - itpr_loss: 0.6405 - lit_loss: 0.6816 - prec_loss: 0.7215 - princ_loss: 0.4236 - psy_loss: 0.7689 - rule_loss: 0.7478 - syst_loss: 0.7707 - tele_loss: 0.5459 - aut_accuracy: 0.9720 - class_accuracy: 0.0373 - itpr_accuracy: 0.7391 - lit_accuracy: 0.5963 - prec_accuracy: 0.4689 - princ_accuracy: 0.8882 - psy_accuracy: 0.2578 - rule_accuracy: 0.3478 - syst_accuracy: 0.2453 - tele_accuracy: 0.8913 - val_loss: 190.0631 - val_aut_loss: 36.5435 - val_class_loss: 0.9841 - val_itpr_loss: 62.5366 - val_lit_loss: 0.6424 - val_prec_loss: 17.8221 - val_princ_loss: 0.3936 - val_psy_loss: 63.2089 - val_rule_loss: 3.4723 - val_syst_loss: 0.7581 - val_tele_loss: 0.5042 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0123 - val_itpr_accuracy: 0.8519 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.3333 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.0494 - val_rule_accuracy: 0.2346 - val_syst_accuracy: 0.0123 - val_tele_accuracy: 1.0000\n",
            "Epoch 32/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 10.2861 - aut_loss: 0.4730 - class_loss: 1.0601 - itpr_loss: 0.6384 - lit_loss: 0.6714 - prec_loss: 0.7147 - princ_loss: 0.5681 - psy_loss: 0.7823 - rule_loss: 0.8460 - syst_loss: 0.7952 - tele_loss: 0.5396 - aut_accuracy: 0.9720 - class_accuracy: 0.0497 - itpr_accuracy: 0.7019 - lit_accuracy: 0.6304 - prec_accuracy: 0.4627 - princ_accuracy: 0.8882 - psy_accuracy: 0.2267 - rule_accuracy: 0.2888 - syst_accuracy: 0.2795 - tele_accuracy: 0.8820 - val_loss: 44.8567 - val_aut_loss: 0.4861 - val_class_loss: 0.9788 - val_itpr_loss: 0.6342 - val_lit_loss: 3.8150 - val_prec_loss: 0.8347 - val_princ_loss: 3.3869 - val_psy_loss: 0.7433 - val_rule_loss: 6.1825 - val_syst_loss: 24.0943 - val_tele_loss: 0.5035 - val_aut_accuracy: 0.9630 - val_class_accuracy: 0.0123 - val_itpr_accuracy: 0.8642 - val_lit_accuracy: 0.9753 - val_prec_accuracy: 0.3333 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0494 - val_rule_accuracy: 0.2346 - val_syst_accuracy: 0.0000e+00 - val_tele_accuracy: 1.0000\n",
            "Epoch 33/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 9.9091 - aut_loss: 0.4702 - class_loss: 0.9575 - itpr_loss: 0.6481 - lit_loss: 0.6789 - prec_loss: 0.7012 - princ_loss: 0.4262 - psy_loss: 0.7812 - rule_loss: 0.7450 - syst_loss: 0.7604 - tele_loss: 0.5430 - aut_accuracy: 0.9720 - class_accuracy: 0.0373 - itpr_accuracy: 0.6863 - lit_accuracy: 0.6211 - prec_accuracy: 0.5031 - princ_accuracy: 0.8882 - psy_accuracy: 0.2112 - rule_accuracy: 0.3354 - syst_accuracy: 0.2764 - tele_accuracy: 0.8727 - val_loss: 15.8821 - val_aut_loss: 0.4857 - val_class_loss: 0.9740 - val_itpr_loss: 0.6348 - val_lit_loss: 1.3531 - val_prec_loss: 2.5722 - val_princ_loss: 0.3939 - val_psy_loss: 1.1122 - val_rule_loss: 2.9115 - val_syst_loss: 1.7453 - val_tele_loss: 0.5023 - val_aut_accuracy: 0.9630 - val_class_accuracy: 0.0123 - val_itpr_accuracy: 0.8642 - val_lit_accuracy: 0.9753 - val_prec_accuracy: 0.3457 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.0247 - val_rule_accuracy: 0.2469 - val_syst_accuracy: 0.0000e+00 - val_tele_accuracy: 1.0000\n",
            "Epoch 34/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 9.9081 - aut_loss: 0.4669 - class_loss: 0.9790 - itpr_loss: 0.6324 - lit_loss: 0.6818 - prec_loss: 0.7120 - princ_loss: 0.4231 - psy_loss: 0.7746 - rule_loss: 0.7474 - syst_loss: 0.7584 - tele_loss: 0.5354 - aut_accuracy: 0.9720 - class_accuracy: 0.0280 - itpr_accuracy: 0.7453 - lit_accuracy: 0.5994 - prec_accuracy: 0.4783 - princ_accuracy: 0.8882 - psy_accuracy: 0.2516 - rule_accuracy: 0.3261 - syst_accuracy: 0.3106 - tele_accuracy: 0.9006 - val_loss: 21.1751 - val_aut_loss: 1.5798 - val_class_loss: 0.9694 - val_itpr_loss: 0.6353 - val_lit_loss: 1.1593 - val_prec_loss: 7.6650 - val_princ_loss: 0.3939 - val_psy_loss: 1.8032 - val_rule_loss: 0.7028 - val_syst_loss: 0.7531 - val_tele_loss: 2.3159 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0123 - val_itpr_accuracy: 0.8642 - val_lit_accuracy: 0.9753 - val_prec_accuracy: 0.3704 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.0247 - val_rule_accuracy: 0.2840 - val_syst_accuracy: 0.0123 - val_tele_accuracy: 0.9877\n",
            "Epoch 35/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 9.9211 - aut_loss: 0.4617 - class_loss: 0.9600 - itpr_loss: 0.6445 - lit_loss: 0.6800 - prec_loss: 0.6990 - princ_loss: 0.4345 - psy_loss: 0.7877 - rule_loss: 0.7564 - syst_loss: 0.7666 - tele_loss: 0.5335 - aut_accuracy: 0.9689 - class_accuracy: 0.0497 - itpr_accuracy: 0.7205 - lit_accuracy: 0.6025 - prec_accuracy: 0.5093 - princ_accuracy: 0.8882 - psy_accuracy: 0.2205 - rule_accuracy: 0.3012 - syst_accuracy: 0.2329 - tele_accuracy: 0.8851 - val_loss: 849847.5000 - val_aut_loss: 162934.3281 - val_class_loss: 316327.5312 - val_itpr_loss: 0.6357 - val_lit_loss: 0.6386 - val_prec_loss: 0.6984 - val_princ_loss: 0.3939 - val_psy_loss: 0.7520 - val_rule_loss: 309291.3438 - val_syst_loss: 61287.4883 - val_tele_loss: 0.4998 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0000e+00 - val_itpr_accuracy: 0.8642 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.3951 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.0370 - val_rule_accuracy: 0.2469 - val_syst_accuracy: 0.0000e+00 - val_tele_accuracy: 1.0000\n",
            "Epoch 36/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 9.9035 - aut_loss: 0.4621 - class_loss: 0.9525 - itpr_loss: 0.6457 - lit_loss: 0.6710 - prec_loss: 0.7341 - princ_loss: 0.4155 - psy_loss: 0.7892 - rule_loss: 0.7555 - syst_loss: 0.7560 - tele_loss: 0.5246 - aut_accuracy: 0.9720 - class_accuracy: 0.0714 - itpr_accuracy: 0.6863 - lit_accuracy: 0.6615 - prec_accuracy: 0.4472 - princ_accuracy: 0.8882 - psy_accuracy: 0.2205 - rule_accuracy: 0.2950 - syst_accuracy: 0.3043 - tele_accuracy: 0.8820 - val_loss: 16.6998 - val_aut_loss: 0.4842 - val_class_loss: 2.8714 - val_itpr_loss: 1.3769 - val_lit_loss: 0.6378 - val_prec_loss: 0.6964 - val_princ_loss: 0.3939 - val_psy_loss: 0.7550 - val_rule_loss: 3.3694 - val_syst_loss: 2.4186 - val_tele_loss: 0.4990 - val_aut_accuracy: 0.9630 - val_class_accuracy: 0.0000e+00 - val_itpr_accuracy: 0.8519 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.3951 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.0370 - val_rule_accuracy: 0.2593 - val_syst_accuracy: 0.0000e+00 - val_tele_accuracy: 1.0000\n",
            "Epoch 37/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 9.8716 - aut_loss: 0.4602 - class_loss: 0.9536 - itpr_loss: 0.6442 - lit_loss: 0.6741 - prec_loss: 0.6911 - princ_loss: 0.4304 - psy_loss: 0.7937 - rule_loss: 0.7412 - syst_loss: 0.7590 - tele_loss: 0.5269 - aut_accuracy: 0.9720 - class_accuracy: 0.0590 - itpr_accuracy: 0.7019 - lit_accuracy: 0.6118 - prec_accuracy: 0.5311 - princ_accuracy: 0.8882 - psy_accuracy: 0.1801 - rule_accuracy: 0.3509 - syst_accuracy: 0.2888 - tele_accuracy: 0.9037 - val_loss: 11.9820 - val_aut_loss: 1.0995 - val_class_loss: 1.8351 - val_itpr_loss: 0.6364 - val_lit_loss: 0.6369 - val_prec_loss: 0.6947 - val_princ_loss: 0.3940 - val_psy_loss: 0.7576 - val_rule_loss: 1.0331 - val_syst_loss: 0.7478 - val_tele_loss: 0.9497 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0000e+00 - val_itpr_accuracy: 0.8642 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.3951 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.0370 - val_rule_accuracy: 0.2716 - val_syst_accuracy: 0.0123 - val_tele_accuracy: 0.9877\n",
            "Epoch 38/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 9.8950 - aut_loss: 0.4644 - class_loss: 0.9536 - itpr_loss: 0.6389 - lit_loss: 0.6655 - prec_loss: 0.7246 - princ_loss: 0.4198 - psy_loss: 0.7906 - rule_loss: 0.7365 - syst_loss: 0.7706 - tele_loss: 0.5331 - aut_accuracy: 0.9720 - class_accuracy: 0.0497 - itpr_accuracy: 0.7267 - lit_accuracy: 0.6832 - prec_accuracy: 0.4720 - princ_accuracy: 0.8882 - psy_accuracy: 0.2081 - rule_accuracy: 0.3602 - syst_accuracy: 0.2236 - tele_accuracy: 0.9006 - val_loss: 115.8376 - val_aut_loss: 23.4152 - val_class_loss: 0.9520 - val_itpr_loss: 19.4644 - val_lit_loss: 4.7992 - val_prec_loss: 0.6933 - val_princ_loss: 14.9981 - val_psy_loss: 24.8019 - val_rule_loss: 13.7005 - val_syst_loss: 0.7462 - val_tele_loss: 9.0697 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0123 - val_itpr_accuracy: 0.8519 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.4074 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0123 - val_rule_accuracy: 0.2716 - val_syst_accuracy: 0.0123 - val_tele_accuracy: 0.9877\n",
            "Epoch 39/200\n",
            "6/6 [==============================] - 21s 4s/step - loss: 9.8497 - aut_loss: 0.4574 - class_loss: 0.9435 - itpr_loss: 0.6445 - lit_loss: 0.6692 - prec_loss: 0.6972 - princ_loss: 0.4299 - psy_loss: 0.8005 - rule_loss: 0.7347 - syst_loss: 0.7515 - tele_loss: 0.5242 - aut_accuracy: 0.9720 - class_accuracy: 0.0590 - itpr_accuracy: 0.7143 - lit_accuracy: 0.6398 - prec_accuracy: 0.4907 - princ_accuracy: 0.8882 - psy_accuracy: 0.1677 - rule_accuracy: 0.3447 - syst_accuracy: 0.3261 - tele_accuracy: 0.9255 - val_loss: 225169.5781 - val_aut_loss: 24412.4180 - val_class_loss: 114896.8516 - val_itpr_loss: 0.6372 - val_lit_loss: 0.6351 - val_prec_loss: 0.6917 - val_princ_loss: 0.3939 - val_psy_loss: 0.7628 - val_rule_loss: 85525.3672 - val_syst_loss: 328.1396 - val_tele_loss: 0.4956 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0000e+00 - val_itpr_accuracy: 0.8642 - val_lit_accuracy: 1.0000 - val_prec_accuracy: 0.4321 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.0123 - val_rule_accuracy: 0.2963 - val_syst_accuracy: 0.0000e+00 - val_tele_accuracy: 1.0000\n",
            "Epoch 40/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 9.8431 - aut_loss: 0.4622 - class_loss: 0.9356 - itpr_loss: 0.6399 - lit_loss: 0.6649 - prec_loss: 0.7163 - princ_loss: 0.4181 - psy_loss: 0.7945 - rule_loss: 0.7383 - syst_loss: 0.7494 - tele_loss: 0.5268 - aut_accuracy: 0.9720 - class_accuracy: 0.0559 - itpr_accuracy: 0.7298 - lit_accuracy: 0.6770 - prec_accuracy: 0.5093 - princ_accuracy: 0.8882 - psy_accuracy: 0.1770 - rule_accuracy: 0.3634 - syst_accuracy: 0.2764 - tele_accuracy: 0.9286 - val_loss: 11.0046 - val_aut_loss: 0.6685 - val_class_loss: 0.9459 - val_itpr_loss: 0.6375 - val_lit_loss: 0.6342 - val_prec_loss: 1.0285 - val_princ_loss: 0.3939 - val_psy_loss: 1.0828 - val_rule_loss: 0.7437 - val_syst_loss: 0.9532 - val_tele_loss: 0.7193 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0123 - val_itpr_accuracy: 0.8642 - val_lit_accuracy: 1.0000 - val_prec_accuracy: 0.4321 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.3210 - val_syst_accuracy: 0.0000e+00 - val_tele_accuracy: 0.9877\n",
            "Epoch 41/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 9.8465 - aut_loss: 0.4600 - class_loss: 0.9428 - itpr_loss: 0.6505 - lit_loss: 0.6663 - prec_loss: 0.6906 - princ_loss: 0.4213 - psy_loss: 0.7961 - rule_loss: 0.7423 - syst_loss: 0.7542 - tele_loss: 0.5252 - aut_accuracy: 0.9720 - class_accuracy: 0.0435 - itpr_accuracy: 0.6739 - lit_accuracy: 0.6491 - prec_accuracy: 0.5124 - princ_accuracy: 0.8882 - psy_accuracy: 0.1894 - rule_accuracy: 0.3540 - syst_accuracy: 0.3012 - tele_accuracy: 0.8913 - val_loss: 16.9977 - val_aut_loss: 1.1856 - val_class_loss: 0.9393 - val_itpr_loss: 1.0506 - val_lit_loss: 0.6332 - val_prec_loss: 0.6883 - val_princ_loss: 0.3985 - val_psy_loss: 3.2664 - val_rule_loss: 0.6934 - val_syst_loss: 2.2794 - val_tele_loss: 2.6659 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0123 - val_itpr_accuracy: 0.8519 - val_lit_accuracy: 1.0000 - val_prec_accuracy: 0.4568 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.3704 - val_syst_accuracy: 0.0000e+00 - val_tele_accuracy: 0.9877\n",
            "Epoch 42/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 9.8368 - aut_loss: 0.4567 - class_loss: 0.9301 - itpr_loss: 0.6410 - lit_loss: 0.6558 - prec_loss: 0.7122 - princ_loss: 0.4334 - psy_loss: 0.8051 - rule_loss: 0.7364 - syst_loss: 0.7459 - tele_loss: 0.5231 - aut_accuracy: 0.9720 - class_accuracy: 0.0621 - itpr_accuracy: 0.7267 - lit_accuracy: 0.6894 - prec_accuracy: 0.5093 - princ_accuracy: 0.8882 - psy_accuracy: 0.1366 - rule_accuracy: 0.3665 - syst_accuracy: 0.3075 - tele_accuracy: 0.9379 - val_loss: 84.1076 - val_aut_loss: 0.4806 - val_class_loss: 0.9350 - val_itpr_loss: 0.6376 - val_lit_loss: 0.6319 - val_prec_loss: 0.6865 - val_princ_loss: 5.6278 - val_psy_loss: 18.9481 - val_rule_loss: 0.6920 - val_syst_loss: 51.7792 - val_tele_loss: 0.4916 - val_aut_accuracy: 0.9630 - val_class_accuracy: 0.0123 - val_itpr_accuracy: 0.8642 - val_lit_accuracy: 1.0000 - val_prec_accuracy: 0.4691 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.4198 - val_syst_accuracy: 0.0123 - val_tele_accuracy: 1.0000\n",
            "Epoch 43/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 9.8396 - aut_loss: 0.4578 - class_loss: 0.9407 - itpr_loss: 0.6341 - lit_loss: 0.6661 - prec_loss: 0.7033 - princ_loss: 0.4195 - psy_loss: 0.7902 - rule_loss: 0.7429 - syst_loss: 0.7518 - tele_loss: 0.5358 - aut_accuracy: 0.9720 - class_accuracy: 0.0621 - itpr_accuracy: 0.7236 - lit_accuracy: 0.6801 - prec_accuracy: 0.5155 - princ_accuracy: 0.8882 - psy_accuracy: 0.2081 - rule_accuracy: 0.3478 - syst_accuracy: 0.3292 - tele_accuracy: 0.8789 - val_loss: 10.4064 - val_aut_loss: 0.7085 - val_class_loss: 1.0999 - val_itpr_loss: 0.6376 - val_lit_loss: 0.6308 - val_prec_loss: 0.6850 - val_princ_loss: 0.3936 - val_psy_loss: 0.8608 - val_rule_loss: 0.9653 - val_syst_loss: 0.7375 - val_tele_loss: 0.4903 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0000e+00 - val_itpr_accuracy: 0.8642 - val_lit_accuracy: 1.0000 - val_prec_accuracy: 0.5062 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.4198 - val_syst_accuracy: 0.0247 - val_tele_accuracy: 1.0000\n",
            "Epoch 44/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 9.7974 - aut_loss: 0.4519 - class_loss: 0.9278 - itpr_loss: 0.6369 - lit_loss: 0.6545 - prec_loss: 0.6978 - princ_loss: 0.4264 - psy_loss: 0.8145 - rule_loss: 0.7348 - syst_loss: 0.7340 - tele_loss: 0.5216 - aut_accuracy: 0.9720 - class_accuracy: 0.0559 - itpr_accuracy: 0.7081 - lit_accuracy: 0.6894 - prec_accuracy: 0.5342 - princ_accuracy: 0.8882 - psy_accuracy: 0.1429 - rule_accuracy: 0.3758 - syst_accuracy: 0.3696 - tele_accuracy: 0.9255 - val_loss: 2128.9250 - val_aut_loss: 571.1979 - val_class_loss: 0.9271 - val_itpr_loss: 372.6307 - val_lit_loss: 259.7356 - val_prec_loss: 0.6837 - val_princ_loss: 115.2822 - val_psy_loss: 401.8345 - val_rule_loss: 279.5617 - val_syst_loss: 0.7355 - val_tele_loss: 123.1384 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0123 - val_itpr_accuracy: 0.8519 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.5309 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.4074 - val_syst_accuracy: 0.0247 - val_tele_accuracy: 0.9877\n",
            "Epoch 45/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 9.8038 - aut_loss: 0.4490 - class_loss: 0.9322 - itpr_loss: 0.6429 - lit_loss: 0.6508 - prec_loss: 0.6976 - princ_loss: 0.4214 - psy_loss: 0.8084 - rule_loss: 0.7279 - syst_loss: 0.7561 - tele_loss: 0.5203 - aut_accuracy: 0.9720 - class_accuracy: 0.0776 - itpr_accuracy: 0.7081 - lit_accuracy: 0.7174 - prec_accuracy: 0.5280 - princ_accuracy: 0.8882 - psy_accuracy: 0.1646 - rule_accuracy: 0.3944 - syst_accuracy: 0.3137 - tele_accuracy: 0.9472 - val_loss: 2506.1748 - val_aut_loss: 668.0172 - val_class_loss: 0.9234 - val_itpr_loss: 438.5476 - val_lit_loss: 296.5821 - val_prec_loss: 0.6825 - val_princ_loss: 145.9287 - val_psy_loss: 469.8037 - val_rule_loss: 328.4308 - val_syst_loss: 0.7337 - val_tele_loss: 153.3277 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0123 - val_itpr_accuracy: 0.8519 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.5309 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.4074 - val_syst_accuracy: 0.0247 - val_tele_accuracy: 0.9877\n",
            "Epoch 46/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 9.8160 - aut_loss: 0.4627 - class_loss: 0.9297 - itpr_loss: 0.6345 - lit_loss: 0.6541 - prec_loss: 0.7049 - princ_loss: 0.4270 - psy_loss: 0.8044 - rule_loss: 0.7303 - syst_loss: 0.7440 - tele_loss: 0.5272 - aut_accuracy: 0.9720 - class_accuracy: 0.0839 - itpr_accuracy: 0.7360 - lit_accuracy: 0.7019 - prec_accuracy: 0.5248 - princ_accuracy: 0.8882 - psy_accuracy: 0.1522 - rule_accuracy: 0.3851 - syst_accuracy: 0.3478 - tele_accuracy: 0.8944 - val_loss: 11.2538 - val_aut_loss: 0.6323 - val_class_loss: 0.9196 - val_itpr_loss: 0.6374 - val_lit_loss: 0.6272 - val_prec_loss: 0.8641 - val_princ_loss: 0.3932 - val_psy_loss: 0.9234 - val_rule_loss: 1.1104 - val_syst_loss: 1.4628 - val_tele_loss: 0.4862 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0123 - val_itpr_accuracy: 0.8642 - val_lit_accuracy: 1.0000 - val_prec_accuracy: 0.5309 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.4568 - val_syst_accuracy: 0.0123 - val_tele_accuracy: 1.0000\n",
            "Epoch 47/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 9.7646 - aut_loss: 0.4565 - class_loss: 0.9170 - itpr_loss: 0.6276 - lit_loss: 0.6622 - prec_loss: 0.7037 - princ_loss: 0.4152 - psy_loss: 0.8078 - rule_loss: 0.7253 - syst_loss: 0.7282 - tele_loss: 0.5239 - aut_accuracy: 0.9720 - class_accuracy: 0.0590 - itpr_accuracy: 0.7453 - lit_accuracy: 0.7050 - prec_accuracy: 0.4876 - princ_accuracy: 0.8882 - psy_accuracy: 0.1957 - rule_accuracy: 0.3913 - syst_accuracy: 0.3882 - tele_accuracy: 0.8975 - val_loss: 10.4524 - val_aut_loss: 0.5084 - val_class_loss: 1.2706 - val_itpr_loss: 0.6986 - val_lit_loss: 0.6259 - val_prec_loss: 0.6801 - val_princ_loss: 0.3930 - val_psy_loss: 0.7971 - val_rule_loss: 1.0054 - val_syst_loss: 0.7916 - val_tele_loss: 0.4846 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0000e+00 - val_itpr_accuracy: 0.8519 - val_lit_accuracy: 1.0000 - val_prec_accuracy: 0.5556 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.0123 - val_rule_accuracy: 0.4815 - val_syst_accuracy: 0.0123 - val_tele_accuracy: 1.0000\n",
            "Epoch 48/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 9.7950 - aut_loss: 0.4562 - class_loss: 0.9143 - itpr_loss: 0.6364 - lit_loss: 0.6567 - prec_loss: 0.7063 - princ_loss: 0.4278 - psy_loss: 0.8141 - rule_loss: 0.7275 - syst_loss: 0.7371 - tele_loss: 0.5215 - aut_accuracy: 0.9720 - class_accuracy: 0.0839 - itpr_accuracy: 0.7298 - lit_accuracy: 0.6832 - prec_accuracy: 0.5031 - princ_accuracy: 0.8882 - psy_accuracy: 0.1522 - rule_accuracy: 0.4255 - syst_accuracy: 0.3478 - tele_accuracy: 0.9161 - val_loss: 3001.0781 - val_aut_loss: 795.1113 - val_class_loss: 0.9117 - val_itpr_loss: 524.3060 - val_lit_loss: 344.2527 - val_prec_loss: 0.6789 - val_princ_loss: 187.0734 - val_psy_loss: 559.1325 - val_rule_loss: 392.4605 - val_syst_loss: 0.7286 - val_tele_loss: 193.2255 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0123 - val_itpr_accuracy: 0.8519 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.5556 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.4691 - val_syst_accuracy: 0.0247 - val_tele_accuracy: 0.9877\n",
            "Epoch 49/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 9.7937 - aut_loss: 0.4537 - class_loss: 0.9060 - itpr_loss: 0.6386 - lit_loss: 0.6519 - prec_loss: 0.7249 - princ_loss: 0.4263 - psy_loss: 0.8266 - rule_loss: 0.7257 - syst_loss: 0.7317 - tele_loss: 0.5111 - aut_accuracy: 0.9720 - class_accuracy: 0.0683 - itpr_accuracy: 0.7174 - lit_accuracy: 0.7019 - prec_accuracy: 0.4938 - princ_accuracy: 0.8882 - psy_accuracy: 0.1242 - rule_accuracy: 0.3758 - syst_accuracy: 0.3571 - tele_accuracy: 0.9255 - val_loss: 59.0614 - val_aut_loss: 0.7211 - val_class_loss: 0.9079 - val_itpr_loss: 0.6379 - val_lit_loss: 1.4592 - val_prec_loss: 5.7870 - val_princ_loss: 10.3757 - val_psy_loss: 4.4208 - val_rule_loss: 4.2426 - val_syst_loss: 26.8308 - val_tele_loss: 0.4813 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0123 - val_itpr_accuracy: 0.8642 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.5556 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.4938 - val_syst_accuracy: 0.0123 - val_tele_accuracy: 1.0000\n",
            "Epoch 50/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 9.7568 - aut_loss: 0.4547 - class_loss: 0.8949 - itpr_loss: 0.6429 - lit_loss: 0.6582 - prec_loss: 0.7020 - princ_loss: 0.4245 - psy_loss: 0.8298 - rule_loss: 0.7152 - syst_loss: 0.7300 - tele_loss: 0.5077 - aut_accuracy: 0.9720 - class_accuracy: 0.0652 - itpr_accuracy: 0.7050 - lit_accuracy: 0.6894 - prec_accuracy: 0.5217 - princ_accuracy: 0.8882 - psy_accuracy: 0.1398 - rule_accuracy: 0.4193 - syst_accuracy: 0.3820 - tele_accuracy: 0.9286 - val_loss: 57.7777 - val_aut_loss: 0.4737 - val_class_loss: 0.9042 - val_itpr_loss: 0.6378 - val_lit_loss: 0.6219 - val_prec_loss: 6.2836 - val_princ_loss: 9.9536 - val_psy_loss: 3.9452 - val_rule_loss: 2.9015 - val_syst_loss: 28.3795 - val_tele_loss: 0.4796 - val_aut_accuracy: 0.9630 - val_class_accuracy: 0.0123 - val_itpr_accuracy: 0.8642 - val_lit_accuracy: 1.0000 - val_prec_accuracy: 0.5556 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.5679 - val_syst_accuracy: 0.0123 - val_tele_accuracy: 1.0000\n",
            "Epoch 51/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 9.7690 - aut_loss: 0.4567 - class_loss: 0.8986 - itpr_loss: 0.6471 - lit_loss: 0.6480 - prec_loss: 0.7133 - princ_loss: 0.4185 - psy_loss: 0.8350 - rule_loss: 0.7093 - syst_loss: 0.7313 - tele_loss: 0.5140 - aut_accuracy: 0.9720 - class_accuracy: 0.0839 - itpr_accuracy: 0.6863 - lit_accuracy: 0.7422 - prec_accuracy: 0.5186 - princ_accuracy: 0.8882 - psy_accuracy: 0.1304 - rule_accuracy: 0.4658 - syst_accuracy: 0.3820 - tele_accuracy: 0.9379 - val_loss: 2559.3201 - val_aut_loss: 690.4109 - val_class_loss: 0.9007 - val_itpr_loss: 453.6730 - val_lit_loss: 317.9147 - val_prec_loss: 0.6759 - val_princ_loss: 129.9140 - val_psy_loss: 477.4815 - val_rule_loss: 337.8385 - val_syst_loss: 0.7235 - val_tele_loss: 146.5902 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0123 - val_itpr_accuracy: 0.8519 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.5679 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.5679 - val_syst_accuracy: 0.0247 - val_tele_accuracy: 0.9877\n",
            "Epoch 52/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 9.7513 - aut_loss: 0.4522 - class_loss: 0.8987 - itpr_loss: 0.6403 - lit_loss: 0.6524 - prec_loss: 0.6858 - princ_loss: 0.4279 - psy_loss: 0.8474 - rule_loss: 0.7229 - syst_loss: 0.7211 - tele_loss: 0.5056 - aut_accuracy: 0.9720 - class_accuracy: 0.0714 - itpr_accuracy: 0.7329 - lit_accuracy: 0.7081 - prec_accuracy: 0.5435 - princ_accuracy: 0.8882 - psy_accuracy: 0.1211 - rule_accuracy: 0.3944 - syst_accuracy: 0.4193 - tele_accuracy: 0.9161 - val_loss: 217.9534 - val_aut_loss: 82.1902 - val_class_loss: 0.8975 - val_itpr_loss: 23.1573 - val_lit_loss: 19.8071 - val_prec_loss: 24.4946 - val_princ_loss: 0.3917 - val_psy_loss: 38.3325 - val_rule_loss: 22.2232 - val_syst_loss: 2.7864 - val_tele_loss: 0.4758 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0123 - val_itpr_accuracy: 0.8519 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.5679 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.6049 - val_syst_accuracy: 0.0247 - val_tele_accuracy: 1.0000\n",
            "Epoch 53/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 9.7246 - aut_loss: 0.4464 - class_loss: 0.9178 - itpr_loss: 0.6385 - lit_loss: 0.6472 - prec_loss: 0.6967 - princ_loss: 0.4101 - psy_loss: 0.8343 - rule_loss: 0.7104 - syst_loss: 0.7243 - tele_loss: 0.5020 - aut_accuracy: 0.9720 - class_accuracy: 0.0745 - itpr_accuracy: 0.7205 - lit_accuracy: 0.7329 - prec_accuracy: 0.5311 - princ_accuracy: 0.8882 - psy_accuracy: 0.1429 - rule_accuracy: 0.4224 - syst_accuracy: 0.3851 - tele_accuracy: 0.9441 - val_loss: 125.0174 - val_aut_loss: 15.6670 - val_class_loss: 0.8941 - val_itpr_loss: 0.6382 - val_lit_loss: 0.6175 - val_prec_loss: 56.9372 - val_princ_loss: 0.3915 - val_psy_loss: 22.3548 - val_rule_loss: 0.6789 - val_syst_loss: 0.7203 - val_tele_loss: 22.9207 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0123 - val_itpr_accuracy: 0.8642 - val_lit_accuracy: 1.0000 - val_prec_accuracy: 0.5802 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.6173 - val_syst_accuracy: 0.0370 - val_tele_accuracy: 0.9877\n",
            "Epoch 54/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 9.7613 - aut_loss: 0.4421 - class_loss: 0.9071 - itpr_loss: 0.6404 - lit_loss: 0.6492 - prec_loss: 0.7191 - princ_loss: 0.4254 - psy_loss: 0.8346 - rule_loss: 0.7155 - syst_loss: 0.7216 - tele_loss: 0.5095 - aut_accuracy: 0.9720 - class_accuracy: 0.0621 - itpr_accuracy: 0.7081 - lit_accuracy: 0.7453 - prec_accuracy: 0.5373 - princ_accuracy: 0.8882 - psy_accuracy: 0.1242 - rule_accuracy: 0.4720 - syst_accuracy: 0.3509 - tele_accuracy: 0.9565 - val_loss: 53.9798 - val_aut_loss: 0.4692 - val_class_loss: 0.8907 - val_itpr_loss: 0.6383 - val_lit_loss: 4.2839 - val_prec_loss: 9.0906 - val_princ_loss: 0.3913 - val_psy_loss: 10.7184 - val_rule_loss: 15.5617 - val_syst_loss: 8.2662 - val_tele_loss: 0.4726 - val_aut_accuracy: 0.9630 - val_class_accuracy: 0.0123 - val_itpr_accuracy: 0.8642 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.5926 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.6296 - val_syst_accuracy: 0.0247 - val_tele_accuracy: 1.0000\n",
            "Epoch 55/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 9.7094 - aut_loss: 0.4439 - class_loss: 0.8902 - itpr_loss: 0.6387 - lit_loss: 0.6453 - prec_loss: 0.7002 - princ_loss: 0.4289 - psy_loss: 0.8353 - rule_loss: 0.7066 - syst_loss: 0.7188 - tele_loss: 0.5045 - aut_accuracy: 0.9689 - class_accuracy: 0.0901 - itpr_accuracy: 0.7298 - lit_accuracy: 0.7453 - prec_accuracy: 0.5404 - princ_accuracy: 0.8851 - psy_accuracy: 0.1398 - rule_accuracy: 0.5155 - syst_accuracy: 0.4348 - tele_accuracy: 0.9441 - val_loss: 137.7123 - val_aut_loss: 39.9470 - val_class_loss: 0.9460 - val_itpr_loss: 20.5992 - val_lit_loss: 14.2478 - val_prec_loss: 15.4143 - val_princ_loss: 8.7842 - val_psy_loss: 12.6614 - val_rule_loss: 8.8115 - val_syst_loss: 12.6332 - val_tele_loss: 0.4709 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0000e+00 - val_itpr_accuracy: 0.8519 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.6049 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.6296 - val_syst_accuracy: 0.0247 - val_tele_accuracy: 1.0000\n",
            "Epoch 56/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 9.7124 - aut_loss: 0.4429 - class_loss: 0.8904 - itpr_loss: 0.6398 - lit_loss: 0.6434 - prec_loss: 0.7032 - princ_loss: 0.4200 - psy_loss: 0.8455 - rule_loss: 0.7048 - syst_loss: 0.7229 - tele_loss: 0.5025 - aut_accuracy: 0.9720 - class_accuracy: 0.0776 - itpr_accuracy: 0.7236 - lit_accuracy: 0.7360 - prec_accuracy: 0.5093 - princ_accuracy: 0.8882 - psy_accuracy: 0.1149 - rule_accuracy: 0.4565 - syst_accuracy: 0.4379 - tele_accuracy: 0.9441 - val_loss: 11.4227 - val_aut_loss: 0.4669 - val_class_loss: 0.8836 - val_itpr_loss: 0.6383 - val_lit_loss: 0.6132 - val_prec_loss: 0.6713 - val_princ_loss: 0.4187 - val_psy_loss: 1.1388 - val_rule_loss: 0.8424 - val_syst_loss: 2.0834 - val_tele_loss: 0.4691 - val_aut_accuracy: 0.9630 - val_class_accuracy: 0.0123 - val_itpr_accuracy: 0.8642 - val_lit_accuracy: 1.0000 - val_prec_accuracy: 0.6420 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.6296 - val_syst_accuracy: 0.0370 - val_tele_accuracy: 1.0000\n",
            "Epoch 57/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 9.7217 - aut_loss: 0.4416 - class_loss: 0.8966 - itpr_loss: 0.6399 - lit_loss: 0.6409 - prec_loss: 0.7028 - princ_loss: 0.4192 - psy_loss: 0.8398 - rule_loss: 0.7174 - syst_loss: 0.7239 - tele_loss: 0.5028 - aut_accuracy: 0.9720 - class_accuracy: 0.0745 - itpr_accuracy: 0.7112 - lit_accuracy: 0.7391 - prec_accuracy: 0.5155 - princ_accuracy: 0.8882 - psy_accuracy: 0.1398 - rule_accuracy: 0.4317 - syst_accuracy: 0.4130 - tele_accuracy: 0.9534 - val_loss: 2635.9819 - val_aut_loss: 673.6097 - val_class_loss: 0.8806 - val_itpr_loss: 457.7273 - val_lit_loss: 255.4637 - val_prec_loss: 0.6706 - val_princ_loss: 214.4080 - val_psy_loss: 489.5828 - val_rule_loss: 339.7180 - val_syst_loss: 0.7133 - val_tele_loss: 200.0111 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0123 - val_itpr_accuracy: 0.8519 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.6543 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.6667 - val_syst_accuracy: 0.0494 - val_tele_accuracy: 0.9877\n",
            "Epoch 58/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 9.6922 - aut_loss: 0.4384 - class_loss: 0.8867 - itpr_loss: 0.6400 - lit_loss: 0.6366 - prec_loss: 0.7051 - princ_loss: 0.4239 - psy_loss: 0.8479 - rule_loss: 0.7057 - syst_loss: 0.7160 - tele_loss: 0.4949 - aut_accuracy: 0.9720 - class_accuracy: 0.0870 - itpr_accuracy: 0.7298 - lit_accuracy: 0.7609 - prec_accuracy: 0.4969 - princ_accuracy: 0.8882 - psy_accuracy: 0.1118 - rule_accuracy: 0.4876 - syst_accuracy: 0.4565 - tele_accuracy: 0.9658 - val_loss: 77.0573 - val_aut_loss: 0.6221 - val_class_loss: 0.8775 - val_itpr_loss: 0.6383 - val_lit_loss: 1.6051 - val_prec_loss: 8.9201 - val_princ_loss: 14.2324 - val_psy_loss: 5.8888 - val_rule_loss: 5.7997 - val_syst_loss: 34.8113 - val_tele_loss: 0.4652 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0123 - val_itpr_accuracy: 0.8642 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.6543 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.6667 - val_syst_accuracy: 0.0370 - val_tele_accuracy: 1.0000\n",
            "Epoch 59/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 9.6971 - aut_loss: 0.4453 - class_loss: 0.8953 - itpr_loss: 0.6361 - lit_loss: 0.6434 - prec_loss: 0.7039 - princ_loss: 0.4173 - psy_loss: 0.8511 - rule_loss: 0.6991 - syst_loss: 0.7129 - tele_loss: 0.4957 - aut_accuracy: 0.9720 - class_accuracy: 0.0776 - itpr_accuracy: 0.7174 - lit_accuracy: 0.7516 - prec_accuracy: 0.5342 - princ_accuracy: 0.8882 - psy_accuracy: 0.1304 - rule_accuracy: 0.5031 - syst_accuracy: 0.4348 - tele_accuracy: 0.9565 - val_loss: 103.3223 - val_aut_loss: 51.2377 - val_class_loss: 7.9297 - val_itpr_loss: 0.6381 - val_lit_loss: 0.6084 - val_prec_loss: 0.6692 - val_princ_loss: 17.5477 - val_psy_loss: 7.8631 - val_rule_loss: 12.4586 - val_syst_loss: 0.7096 - val_tele_loss: 0.4633 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0000e+00 - val_itpr_accuracy: 0.8642 - val_lit_accuracy: 1.0000 - val_prec_accuracy: 0.6667 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.6790 - val_syst_accuracy: 0.0741 - val_tele_accuracy: 1.0000\n",
            "Epoch 60/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 9.6785 - aut_loss: 0.4413 - class_loss: 0.8761 - itpr_loss: 0.6456 - lit_loss: 0.6330 - prec_loss: 0.7028 - princ_loss: 0.4151 - psy_loss: 0.8537 - rule_loss: 0.7095 - syst_loss: 0.7060 - tele_loss: 0.4984 - aut_accuracy: 0.9720 - class_accuracy: 0.0932 - itpr_accuracy: 0.7081 - lit_accuracy: 0.7733 - prec_accuracy: 0.5062 - princ_accuracy: 0.8882 - psy_accuracy: 0.1522 - rule_accuracy: 0.5031 - syst_accuracy: 0.4752 - tele_accuracy: 0.9503 - val_loss: 101.5558 - val_aut_loss: 50.2944 - val_class_loss: 7.7520 - val_itpr_loss: 0.6378 - val_lit_loss: 0.6067 - val_prec_loss: 0.6686 - val_princ_loss: 16.4187 - val_psy_loss: 8.3801 - val_rule_loss: 12.4313 - val_syst_loss: 0.7077 - val_tele_loss: 0.4616 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0000e+00 - val_itpr_accuracy: 0.8642 - val_lit_accuracy: 1.0000 - val_prec_accuracy: 0.6667 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.6790 - val_syst_accuracy: 0.0864 - val_tele_accuracy: 1.0000\n",
            "Epoch 61/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 9.6194 - aut_loss: 0.4326 - class_loss: 0.8701 - itpr_loss: 0.6380 - lit_loss: 0.6253 - prec_loss: 0.6920 - princ_loss: 0.4026 - psy_loss: 0.8627 - rule_loss: 0.6943 - syst_loss: 0.7129 - tele_loss: 0.4920 - aut_accuracy: 0.9720 - class_accuracy: 0.1025 - itpr_accuracy: 0.7081 - lit_accuracy: 0.8106 - prec_accuracy: 0.5280 - princ_accuracy: 0.8882 - psy_accuracy: 0.0807 - rule_accuracy: 0.5435 - syst_accuracy: 0.4472 - tele_accuracy: 0.9379 - val_loss: 4434.1411 - val_aut_loss: 1054.3226 - val_class_loss: 0.8684 - val_itpr_loss: 784.4774 - val_lit_loss: 317.4400 - val_prec_loss: 0.6678 - val_princ_loss: 465.1640 - val_psy_loss: 892.0374 - val_rule_loss: 559.1476 - val_syst_loss: 0.7056 - val_tele_loss: 356.1137 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0123 - val_itpr_accuracy: 0.8519 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.6790 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.6790 - val_syst_accuracy: 0.0988 - val_tele_accuracy: 0.9877\n",
            "Epoch 62/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 9.6265 - aut_loss: 0.4330 - class_loss: 0.8680 - itpr_loss: 0.6414 - lit_loss: 0.6362 - prec_loss: 0.6849 - princ_loss: 0.4092 - psy_loss: 0.8509 - rule_loss: 0.6973 - syst_loss: 0.7110 - tele_loss: 0.4976 - aut_accuracy: 0.9720 - class_accuracy: 0.1118 - itpr_accuracy: 0.6832 - lit_accuracy: 0.7609 - prec_accuracy: 0.5559 - princ_accuracy: 0.8882 - psy_accuracy: 0.1366 - rule_accuracy: 0.5000 - syst_accuracy: 0.4627 - tele_accuracy: 0.9441 - val_loss: 7693.4146 - val_aut_loss: 2109.8066 - val_class_loss: 0.8654 - val_itpr_loss: 1375.3302 - val_lit_loss: 980.0407 - val_prec_loss: 0.6673 - val_princ_loss: 356.8361 - val_psy_loss: 1414.6840 - val_rule_loss: 1014.6935 - val_syst_loss: 0.7036 - val_tele_loss: 436.5896 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0123 - val_itpr_accuracy: 0.8519 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.6790 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7037 - val_syst_accuracy: 0.1358 - val_tele_accuracy: 0.9877\n",
            "Epoch 63/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 9.6341 - aut_loss: 0.4384 - class_loss: 0.8740 - itpr_loss: 0.6336 - lit_loss: 0.6250 - prec_loss: 0.6979 - princ_loss: 0.4226 - psy_loss: 0.8668 - rule_loss: 0.6938 - syst_loss: 0.7029 - tele_loss: 0.4822 - aut_accuracy: 0.9720 - class_accuracy: 0.1149 - itpr_accuracy: 0.7174 - lit_accuracy: 0.7888 - prec_accuracy: 0.5217 - princ_accuracy: 0.8882 - psy_accuracy: 0.1056 - rule_accuracy: 0.5280 - syst_accuracy: 0.5186 - tele_accuracy: 0.9596 - val_loss: 7413.3774 - val_aut_loss: 2057.7422 - val_class_loss: 0.8622 - val_itpr_loss: 1331.6658 - val_lit_loss: 1014.0494 - val_prec_loss: 0.6667 - val_princ_loss: 263.0041 - val_psy_loss: 1431.1917 - val_rule_loss: 982.5851 - val_syst_loss: 0.7016 - val_tele_loss: 327.7116 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0123 - val_itpr_accuracy: 0.8519 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.6914 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7037 - val_syst_accuracy: 0.1605 - val_tele_accuracy: 0.9877\n",
            "Epoch 64/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 9.6087 - aut_loss: 0.4314 - class_loss: 0.8632 - itpr_loss: 0.6369 - lit_loss: 0.6247 - prec_loss: 0.6752 - princ_loss: 0.4279 - psy_loss: 0.8729 - rule_loss: 0.6949 - syst_loss: 0.7026 - tele_loss: 0.4822 - aut_accuracy: 0.9720 - class_accuracy: 0.0807 - itpr_accuracy: 0.7391 - lit_accuracy: 0.8137 - prec_accuracy: 0.5621 - princ_accuracy: 0.8882 - psy_accuracy: 0.0901 - rule_accuracy: 0.5093 - syst_accuracy: 0.4627 - tele_accuracy: 0.9534 - val_loss: 4328.2515 - val_aut_loss: 1119.2802 - val_class_loss: 0.8590 - val_itpr_loss: 754.8014 - val_lit_loss: 441.0529 - val_prec_loss: 0.6660 - val_princ_loss: 328.6232 - val_psy_loss: 801.5242 - val_rule_loss: 560.6805 - val_syst_loss: 0.6996 - val_tele_loss: 316.8675 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0123 - val_itpr_accuracy: 0.8642 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.6914 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7037 - val_syst_accuracy: 0.1605 - val_tele_accuracy: 0.9877\n",
            "Epoch 65/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 9.6344 - aut_loss: 0.4343 - class_loss: 0.8685 - itpr_loss: 0.6383 - lit_loss: 0.6297 - prec_loss: 0.6927 - princ_loss: 0.4138 - psy_loss: 0.8727 - rule_loss: 0.6956 - syst_loss: 0.7095 - tele_loss: 0.4824 - aut_accuracy: 0.9720 - class_accuracy: 0.1118 - itpr_accuracy: 0.6957 - lit_accuracy: 0.7826 - prec_accuracy: 0.5497 - princ_accuracy: 0.8882 - psy_accuracy: 0.0776 - rule_accuracy: 0.5435 - syst_accuracy: 0.4720 - tele_accuracy: 0.9441 - val_loss: 4776.8745 - val_aut_loss: 1161.8824 - val_class_loss: 0.8557 - val_itpr_loss: 841.6805 - val_lit_loss: 377.9184 - val_prec_loss: 0.6653 - val_princ_loss: 467.9013 - val_psy_loss: 934.6887 - val_rule_loss: 606.6210 - val_syst_loss: 0.6977 - val_tele_loss: 380.7668 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0123 - val_itpr_accuracy: 0.8642 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.6790 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7407 - val_syst_accuracy: 0.1975 - val_tele_accuracy: 0.9877\n",
            "Epoch 66/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 9.6316 - aut_loss: 0.4321 - class_loss: 0.8590 - itpr_loss: 0.6363 - lit_loss: 0.6174 - prec_loss: 0.6926 - princ_loss: 0.4262 - psy_loss: 0.8844 - rule_loss: 0.6990 - syst_loss: 0.7094 - tele_loss: 0.4784 - aut_accuracy: 0.9720 - class_accuracy: 0.1242 - itpr_accuracy: 0.7081 - lit_accuracy: 0.8106 - prec_accuracy: 0.5497 - princ_accuracy: 0.8882 - psy_accuracy: 0.1149 - rule_accuracy: 0.5652 - syst_accuracy: 0.4720 - tele_accuracy: 0.9689 - val_loss: 3571.0347 - val_aut_loss: 927.8250 - val_class_loss: 0.8523 - val_itpr_loss: 618.9683 - val_lit_loss: 361.6281 - val_prec_loss: 0.6644 - val_princ_loss: 275.3073 - val_psy_loss: 646.5389 - val_rule_loss: 462.6018 - val_syst_loss: 0.6954 - val_tele_loss: 272.7563 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0123 - val_itpr_accuracy: 0.8642 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.6790 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7407 - val_syst_accuracy: 0.2222 - val_tele_accuracy: 0.9877\n",
            "Epoch 67/200\n",
            "6/6 [==============================] - 21s 4s/step - loss: 9.5635 - aut_loss: 0.4222 - class_loss: 0.8497 - itpr_loss: 0.6349 - lit_loss: 0.6182 - prec_loss: 0.6885 - princ_loss: 0.4139 - psy_loss: 0.8800 - rule_loss: 0.6872 - syst_loss: 0.7019 - tele_loss: 0.4704 - aut_accuracy: 0.9720 - class_accuracy: 0.1118 - itpr_accuracy: 0.7236 - lit_accuracy: 0.8043 - prec_accuracy: 0.5590 - princ_accuracy: 0.8882 - psy_accuracy: 0.0963 - rule_accuracy: 0.5373 - syst_accuracy: 0.4783 - tele_accuracy: 0.9565 - val_loss: 1231.9580 - val_aut_loss: 77.2944 - val_class_loss: 579.6677 - val_itpr_loss: 0.6354 - val_lit_loss: 0.5941 - val_prec_loss: 0.6634 - val_princ_loss: 0.3866 - val_psy_loss: 0.8562 - val_rule_loss: 523.8514 - val_syst_loss: 44.3645 - val_tele_loss: 0.4473 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0000e+00 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 1.0000 - val_prec_accuracy: 0.6790 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.0123 - val_rule_accuracy: 0.7407 - val_syst_accuracy: 0.2222 - val_tele_accuracy: 1.0000\n",
            "Epoch 68/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 9.5912 - aut_loss: 0.4333 - class_loss: 0.8540 - itpr_loss: 0.6326 - lit_loss: 0.6118 - prec_loss: 0.6983 - princ_loss: 0.4099 - psy_loss: 0.9065 - rule_loss: 0.6831 - syst_loss: 0.6937 - tele_loss: 0.4711 - aut_accuracy: 0.9720 - class_accuracy: 0.1335 - itpr_accuracy: 0.7174 - lit_accuracy: 0.8137 - prec_accuracy: 0.5621 - princ_accuracy: 0.8882 - psy_accuracy: 0.0559 - rule_accuracy: 0.5870 - syst_accuracy: 0.5000 - tele_accuracy: 0.9658 - val_loss: 4091.1108 - val_aut_loss: 1014.0017 - val_class_loss: 0.8458 - val_itpr_loss: 717.9674 - val_lit_loss: 351.2133 - val_prec_loss: 0.6625 - val_princ_loss: 374.9297 - val_psy_loss: 784.2155 - val_rule_loss: 522.9172 - val_syst_loss: 0.6910 - val_tele_loss: 320.4701 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0123 - val_itpr_accuracy: 0.8642 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.6914 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7531 - val_syst_accuracy: 0.2593 - val_tele_accuracy: 0.9877\n",
            "Epoch 69/200\n",
            "6/6 [==============================] - 21s 4s/step - loss: 9.5669 - aut_loss: 0.4240 - class_loss: 0.8511 - itpr_loss: 0.6427 - lit_loss: 0.6051 - prec_loss: 0.6825 - princ_loss: 0.4153 - psy_loss: 0.8868 - rule_loss: 0.6843 - syst_loss: 0.7017 - tele_loss: 0.4767 - aut_accuracy: 0.9720 - class_accuracy: 0.1335 - itpr_accuracy: 0.7236 - lit_accuracy: 0.8571 - prec_accuracy: 0.5466 - princ_accuracy: 0.8882 - psy_accuracy: 0.0870 - rule_accuracy: 0.5807 - syst_accuracy: 0.4845 - tele_accuracy: 0.9534 - val_loss: 8273.9336 - val_aut_loss: 2304.9753 - val_class_loss: 0.8426 - val_itpr_loss: 1492.2507 - val_lit_loss: 1153.8003 - val_prec_loss: 0.6618 - val_princ_loss: 265.0288 - val_psy_loss: 1623.9442 - val_rule_loss: 1097.6116 - val_syst_loss: 0.6889 - val_tele_loss: 330.9326 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0123 - val_itpr_accuracy: 0.8642 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.7037 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 0.3333 - val_tele_accuracy: 0.9877\n",
            "Epoch 70/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 9.5941 - aut_loss: 0.4276 - class_loss: 0.8523 - itpr_loss: 0.6447 - lit_loss: 0.6082 - prec_loss: 0.6972 - princ_loss: 0.4158 - psy_loss: 0.8925 - rule_loss: 0.6956 - syst_loss: 0.6958 - tele_loss: 0.4675 - aut_accuracy: 0.9720 - class_accuracy: 0.1149 - itpr_accuracy: 0.6988 - lit_accuracy: 0.8168 - prec_accuracy: 0.5280 - princ_accuracy: 0.8882 - psy_accuracy: 0.0621 - rule_accuracy: 0.5404 - syst_accuracy: 0.5062 - tele_accuracy: 0.9689 - val_loss: 4504.4014 - val_aut_loss: 1113.1863 - val_class_loss: 0.8394 - val_itpr_loss: 793.3892 - val_lit_loss: 388.0645 - val_prec_loss: 0.6611 - val_princ_loss: 410.4504 - val_psy_loss: 876.6265 - val_rule_loss: 575.5848 - val_syst_loss: 0.6867 - val_tele_loss: 341.7161 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0123 - val_itpr_accuracy: 0.8642 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.7160 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7778 - val_syst_accuracy: 0.3951 - val_tele_accuracy: 0.9877\n",
            "Epoch 71/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 9.5523 - aut_loss: 0.4244 - class_loss: 0.8486 - itpr_loss: 0.6379 - lit_loss: 0.6030 - prec_loss: 0.6918 - princ_loss: 0.4070 - psy_loss: 0.9144 - rule_loss: 0.6746 - syst_loss: 0.6917 - tele_loss: 0.4622 - aut_accuracy: 0.9720 - class_accuracy: 0.1056 - itpr_accuracy: 0.7236 - lit_accuracy: 0.8571 - prec_accuracy: 0.5155 - princ_accuracy: 0.8882 - psy_accuracy: 0.0621 - rule_accuracy: 0.6087 - syst_accuracy: 0.5373 - tele_accuracy: 0.9627 - val_loss: 13.5220 - val_aut_loss: 0.4455 - val_class_loss: 0.8363 - val_itpr_loss: 0.6329 - val_lit_loss: 0.9385 - val_prec_loss: 0.6604 - val_princ_loss: 0.3956 - val_psy_loss: 0.8723 - val_rule_loss: 1.4362 - val_syst_loss: 3.6675 - val_tele_loss: 0.4399 - val_aut_accuracy: 0.9630 - val_class_accuracy: 0.0123 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.7284 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0123 - val_rule_accuracy: 0.7778 - val_syst_accuracy: 0.4815 - val_tele_accuracy: 1.0000\n",
            "Epoch 72/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 9.5371 - aut_loss: 0.4235 - class_loss: 0.8385 - itpr_loss: 0.6369 - lit_loss: 0.5992 - prec_loss: 0.6815 - princ_loss: 0.4304 - psy_loss: 0.8960 - rule_loss: 0.6790 - syst_loss: 0.6890 - tele_loss: 0.4662 - aut_accuracy: 0.9720 - class_accuracy: 0.1553 - itpr_accuracy: 0.7236 - lit_accuracy: 0.8602 - prec_accuracy: 0.6056 - princ_accuracy: 0.8882 - psy_accuracy: 0.1087 - rule_accuracy: 0.5776 - syst_accuracy: 0.5031 - tele_accuracy: 0.9534 - val_loss: 25.4767 - val_aut_loss: 3.0704 - val_class_loss: 1.6728 - val_itpr_loss: 0.6321 - val_lit_loss: 0.5836 - val_prec_loss: 5.0668 - val_princ_loss: 0.3849 - val_psy_loss: 4.1131 - val_rule_loss: 5.6357 - val_syst_loss: 0.6822 - val_tele_loss: 0.4381 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0000e+00 - val_itpr_accuracy: 0.8889 - val_lit_accuracy: 1.0000 - val_prec_accuracy: 0.7160 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7778 - val_syst_accuracy: 0.5432 - val_tele_accuracy: 1.0000\n",
            "Epoch 73/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 9.5210 - aut_loss: 0.4235 - class_loss: 0.8331 - itpr_loss: 0.6236 - lit_loss: 0.6081 - prec_loss: 0.6902 - princ_loss: 0.4136 - psy_loss: 0.8964 - rule_loss: 0.6699 - syst_loss: 0.6900 - tele_loss: 0.4759 - aut_accuracy: 0.9720 - class_accuracy: 0.1584 - itpr_accuracy: 0.7360 - lit_accuracy: 0.8478 - prec_accuracy: 0.5497 - princ_accuracy: 0.8882 - psy_accuracy: 0.0776 - rule_accuracy: 0.6522 - syst_accuracy: 0.5497 - tele_accuracy: 0.9534 - val_loss: 11.4535 - val_aut_loss: 1.2514 - val_class_loss: 0.8404 - val_itpr_loss: 0.6312 - val_lit_loss: 0.5812 - val_prec_loss: 0.6589 - val_princ_loss: 0.3844 - val_psy_loss: 1.2951 - val_rule_loss: 1.2199 - val_syst_loss: 0.9580 - val_tele_loss: 0.4361 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0000e+00 - val_itpr_accuracy: 0.8889 - val_lit_accuracy: 1.0000 - val_prec_accuracy: 0.7407 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7901 - val_syst_accuracy: 0.5679 - val_tele_accuracy: 1.0000\n",
            "Epoch 74/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 9.5604 - aut_loss: 0.4202 - class_loss: 0.8252 - itpr_loss: 0.6363 - lit_loss: 0.6012 - prec_loss: 0.6978 - princ_loss: 0.4249 - psy_loss: 0.9278 - rule_loss: 0.6811 - syst_loss: 0.6763 - tele_loss: 0.4728 - aut_accuracy: 0.9720 - class_accuracy: 0.1304 - itpr_accuracy: 0.7143 - lit_accuracy: 0.8820 - prec_accuracy: 0.5559 - princ_accuracy: 0.8882 - psy_accuracy: 0.0714 - rule_accuracy: 0.5870 - syst_accuracy: 0.5714 - tele_accuracy: 0.9720 - val_loss: 4107.7139 - val_aut_loss: 1061.2180 - val_class_loss: 0.8270 - val_itpr_loss: 718.4829 - val_lit_loss: 419.6231 - val_prec_loss: 0.6582 - val_princ_loss: 309.7269 - val_psy_loss: 765.2932 - val_rule_loss: 532.4962 - val_syst_loss: 0.6777 - val_tele_loss: 295.5139 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0123 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.7654 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.8025 - val_syst_accuracy: 0.6543 - val_tele_accuracy: 0.9877\n",
            "Epoch 75/200\n",
            "6/6 [==============================] - 21s 4s/step - loss: 9.5527 - aut_loss: 0.4206 - class_loss: 0.8310 - itpr_loss: 0.6357 - lit_loss: 0.5997 - prec_loss: 0.7072 - princ_loss: 0.4206 - psy_loss: 0.9197 - rule_loss: 0.6722 - syst_loss: 0.6803 - tele_loss: 0.4689 - aut_accuracy: 0.9720 - class_accuracy: 0.1522 - itpr_accuracy: 0.7174 - lit_accuracy: 0.8758 - prec_accuracy: 0.5248 - princ_accuracy: 0.8882 - psy_accuracy: 0.0745 - rule_accuracy: 0.6242 - syst_accuracy: 0.5559 - tele_accuracy: 0.9410 - val_loss: 6201.2427 - val_aut_loss: 1559.4717 - val_class_loss: 0.8241 - val_itpr_loss: 1089.8734 - val_lit_loss: 571.6440 - val_prec_loss: 0.6577 - val_princ_loss: 528.9583 - val_psy_loss: 1188.7397 - val_rule_loss: 796.9749 - val_syst_loss: 0.6755 - val_tele_loss: 460.2267 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0123 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.7531 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.8025 - val_syst_accuracy: 0.7037 - val_tele_accuracy: 0.9877\n",
            "Epoch 76/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 9.5138 - aut_loss: 0.4189 - class_loss: 0.8187 - itpr_loss: 0.6313 - lit_loss: 0.6019 - prec_loss: 0.6796 - princ_loss: 0.4172 - psy_loss: 0.9192 - rule_loss: 0.6767 - syst_loss: 0.6777 - tele_loss: 0.4758 - aut_accuracy: 0.9720 - class_accuracy: 0.1553 - itpr_accuracy: 0.7267 - lit_accuracy: 0.8634 - prec_accuracy: 0.5621 - princ_accuracy: 0.8882 - psy_accuracy: 0.0776 - rule_accuracy: 0.6025 - syst_accuracy: 0.6025 - tele_accuracy: 0.9565 - val_loss: 4600.8813 - val_aut_loss: 1044.8837 - val_class_loss: 0.8212 - val_itpr_loss: 806.0327 - val_lit_loss: 224.0717 - val_prec_loss: 0.6573 - val_princ_loss: 596.1830 - val_psy_loss: 963.2761 - val_rule_loss: 571.4887 - val_syst_loss: 0.6732 - val_tele_loss: 389.5965 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0123 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.7531 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.8025 - val_syst_accuracy: 0.7778 - val_tele_accuracy: 0.9877\n",
            "Epoch 77/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 9.4846 - aut_loss: 0.4086 - class_loss: 0.8227 - itpr_loss: 0.6191 - lit_loss: 0.5923 - prec_loss: 0.7017 - princ_loss: 0.4124 - psy_loss: 0.9248 - rule_loss: 0.6692 - syst_loss: 0.6777 - tele_loss: 0.4593 - aut_accuracy: 0.9720 - class_accuracy: 0.1491 - itpr_accuracy: 0.7733 - lit_accuracy: 0.8944 - prec_accuracy: 0.5186 - princ_accuracy: 0.8882 - psy_accuracy: 0.0901 - rule_accuracy: 0.6304 - syst_accuracy: 0.5963 - tele_accuracy: 0.9627 - val_loss: 6842.9922 - val_aut_loss: 1698.1948 - val_class_loss: 0.8186 - val_itpr_loss: 1202.6005 - val_lit_loss: 586.5848 - val_prec_loss: 0.6570 - val_princ_loss: 627.5576 - val_psy_loss: 1313.4155 - val_rule_loss: 874.7808 - val_syst_loss: 0.6707 - val_tele_loss: 534.5148 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0123 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.7531 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7901 - val_syst_accuracy: 0.8148 - val_tele_accuracy: 0.9877\n",
            "Epoch 78/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 9.5197 - aut_loss: 0.4204 - class_loss: 0.8213 - itpr_loss: 0.6317 - lit_loss: 0.5942 - prec_loss: 0.6773 - princ_loss: 0.4310 - psy_loss: 0.9241 - rule_loss: 0.6728 - syst_loss: 0.6791 - tele_loss: 0.4711 - aut_accuracy: 0.9720 - class_accuracy: 0.1491 - itpr_accuracy: 0.7236 - lit_accuracy: 0.8882 - prec_accuracy: 0.5901 - princ_accuracy: 0.8882 - psy_accuracy: 0.0652 - rule_accuracy: 0.6118 - syst_accuracy: 0.5901 - tele_accuracy: 0.9658 - val_loss: 7427.1582 - val_aut_loss: 1898.8846 - val_class_loss: 0.8157 - val_itpr_loss: 1300.4277 - val_lit_loss: 722.3135 - val_prec_loss: 0.6565 - val_princ_loss: 597.5241 - val_psy_loss: 1391.7147 - val_rule_loss: 959.4741 - val_syst_loss: 0.6682 - val_tele_loss: 551.4824 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0123 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.7531 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7901 - val_syst_accuracy: 0.8642 - val_tele_accuracy: 0.9877\n",
            "Epoch 79/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 9.4837 - aut_loss: 0.4041 - class_loss: 0.8259 - itpr_loss: 0.6376 - lit_loss: 0.5866 - prec_loss: 0.6893 - princ_loss: 0.4137 - psy_loss: 0.9129 - rule_loss: 0.6648 - syst_loss: 0.6891 - tele_loss: 0.4630 - aut_accuracy: 0.9720 - class_accuracy: 0.1460 - itpr_accuracy: 0.7050 - lit_accuracy: 0.8975 - prec_accuracy: 0.5217 - princ_accuracy: 0.8882 - psy_accuracy: 0.0745 - rule_accuracy: 0.6211 - syst_accuracy: 0.5186 - tele_accuracy: 0.9627 - val_loss: 3354.6980 - val_aut_loss: 669.7763 - val_class_loss: 0.8129 - val_itpr_loss: 842.8532 - val_lit_loss: 0.5657 - val_prec_loss: 125.3942 - val_princ_loss: 304.1439 - val_psy_loss: 1087.0203 - val_rule_loss: 319.8437 - val_syst_loss: 0.6658 - val_tele_loss: 0.4254 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0123 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 1.0000 - val_prec_accuracy: 0.7654 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7778 - val_syst_accuracy: 0.8765 - val_tele_accuracy: 1.0000\n",
            "Epoch 80/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 9.4895 - aut_loss: 0.4082 - class_loss: 0.8243 - itpr_loss: 0.6322 - lit_loss: 0.5946 - prec_loss: 0.6881 - princ_loss: 0.4181 - psy_loss: 0.9365 - rule_loss: 0.6778 - syst_loss: 0.6625 - tele_loss: 0.4505 - aut_accuracy: 0.9720 - class_accuracy: 0.1615 - itpr_accuracy: 0.7236 - lit_accuracy: 0.8665 - prec_accuracy: 0.5745 - princ_accuracy: 0.8882 - psy_accuracy: 0.0373 - rule_accuracy: 0.5963 - syst_accuracy: 0.6118 - tele_accuracy: 0.9689 - val_loss: 4365.4492 - val_aut_loss: 871.1562 - val_class_loss: 0.8098 - val_itpr_loss: 1095.4169 - val_lit_loss: 0.5628 - val_prec_loss: 159.0627 - val_princ_loss: 402.8321 - val_psy_loss: 1413.5811 - val_rule_loss: 417.7443 - val_syst_loss: 0.6633 - val_tele_loss: 0.4234 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0123 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 1.0000 - val_prec_accuracy: 0.7654 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7778 - val_syst_accuracy: 0.8889 - val_tele_accuracy: 1.0000\n",
            "Epoch 81/200\n",
            "6/6 [==============================] - 21s 4s/step - loss: 9.4597 - aut_loss: 0.4128 - class_loss: 0.8124 - itpr_loss: 0.6247 - lit_loss: 0.5901 - prec_loss: 0.6724 - princ_loss: 0.4175 - psy_loss: 0.9406 - rule_loss: 0.6695 - syst_loss: 0.6654 - tele_loss: 0.4575 - aut_accuracy: 0.9720 - class_accuracy: 0.1646 - itpr_accuracy: 0.7391 - lit_accuracy: 0.8820 - prec_accuracy: 0.6025 - princ_accuracy: 0.8882 - psy_accuracy: 0.0745 - rule_accuracy: 0.6180 - syst_accuracy: 0.6087 - tele_accuracy: 0.9689 - val_loss: 3418.7773 - val_aut_loss: 682.4503 - val_class_loss: 0.8068 - val_itpr_loss: 874.1987 - val_lit_loss: 0.5597 - val_prec_loss: 134.3361 - val_princ_loss: 290.6016 - val_psy_loss: 1128.2539 - val_rule_loss: 303.2912 - val_syst_loss: 0.6608 - val_tele_loss: 0.4216 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0123 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 1.0000 - val_prec_accuracy: 0.7654 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7778 - val_syst_accuracy: 0.9136 - val_tele_accuracy: 1.0000\n",
            "Epoch 82/200\n",
            "6/6 [==============================] - 21s 4s/step - loss: 9.4534 - aut_loss: 0.4078 - class_loss: 0.8024 - itpr_loss: 0.6282 - lit_loss: 0.5829 - prec_loss: 0.6933 - princ_loss: 0.4201 - psy_loss: 0.9401 - rule_loss: 0.6618 - syst_loss: 0.6599 - tele_loss: 0.4603 - aut_accuracy: 0.9720 - class_accuracy: 0.2329 - itpr_accuracy: 0.7174 - lit_accuracy: 0.8975 - prec_accuracy: 0.5652 - princ_accuracy: 0.8882 - psy_accuracy: 0.0807 - rule_accuracy: 0.6180 - syst_accuracy: 0.6273 - tele_accuracy: 0.9503 - val_loss: 4502.1870 - val_aut_loss: 901.3009 - val_class_loss: 0.8036 - val_itpr_loss: 1177.9342 - val_lit_loss: 0.5566 - val_prec_loss: 191.8386 - val_princ_loss: 345.3276 - val_psy_loss: 1515.2507 - val_rule_loss: 364.9003 - val_syst_loss: 0.6584 - val_tele_loss: 0.4198 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0123 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 1.0000 - val_prec_accuracy: 0.7654 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7778 - val_syst_accuracy: 0.9383 - val_tele_accuracy: 1.0000\n",
            "Epoch 83/200\n",
            "6/6 [==============================] - 21s 4s/step - loss: 9.4481 - aut_loss: 0.4045 - class_loss: 0.8121 - itpr_loss: 0.6252 - lit_loss: 0.5818 - prec_loss: 0.6905 - princ_loss: 0.4175 - psy_loss: 0.9428 - rule_loss: 0.6658 - syst_loss: 0.6599 - tele_loss: 0.4514 - aut_accuracy: 0.9720 - class_accuracy: 0.1863 - itpr_accuracy: 0.7391 - lit_accuracy: 0.8975 - prec_accuracy: 0.5621 - princ_accuracy: 0.8882 - psy_accuracy: 0.0590 - rule_accuracy: 0.6304 - syst_accuracy: 0.6429 - tele_accuracy: 0.9689 - val_loss: 6499.2710 - val_aut_loss: 1525.2020 - val_class_loss: 0.8002 - val_itpr_loss: 1134.3588 - val_lit_loss: 381.3733 - val_prec_loss: 0.6548 - val_princ_loss: 780.5773 - val_psy_loss: 1302.2960 - val_rule_loss: 815.9327 - val_syst_loss: 0.6558 - val_tele_loss: 554.2234 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0247 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.7778 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7778 - val_syst_accuracy: 0.9506 - val_tele_accuracy: 0.9877\n",
            "Epoch 84/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 9.4425 - aut_loss: 0.4046 - class_loss: 0.8040 - itpr_loss: 0.6377 - lit_loss: 0.5738 - prec_loss: 0.6831 - princ_loss: 0.4168 - psy_loss: 0.9663 - rule_loss: 0.6678 - syst_loss: 0.6534 - tele_loss: 0.4384 - aut_accuracy: 0.9720 - class_accuracy: 0.1988 - itpr_accuracy: 0.7143 - lit_accuracy: 0.8882 - prec_accuracy: 0.5683 - princ_accuracy: 0.8882 - psy_accuracy: 0.0590 - rule_accuracy: 0.6460 - syst_accuracy: 0.6553 - tele_accuracy: 0.9752 - val_loss: 3712.7808 - val_aut_loss: 742.1106 - val_class_loss: 0.7970 - val_itpr_loss: 958.8172 - val_lit_loss: 0.5505 - val_prec_loss: 148.8827 - val_princ_loss: 306.0346 - val_psy_loss: 1234.1202 - val_rule_loss: 317.2018 - val_syst_loss: 0.6532 - val_tele_loss: 0.4162 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0247 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 1.0000 - val_prec_accuracy: 0.7654 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 0.9753 - val_tele_accuracy: 1.0000\n",
            "Epoch 85/200\n",
            "6/6 [==============================] - 21s 4s/step - loss: 9.4300 - aut_loss: 0.3952 - class_loss: 0.8177 - itpr_loss: 0.6257 - lit_loss: 0.5774 - prec_loss: 0.6928 - princ_loss: 0.4166 - psy_loss: 0.9645 - rule_loss: 0.6539 - syst_loss: 0.6543 - tele_loss: 0.4353 - aut_accuracy: 0.9720 - class_accuracy: 0.1491 - itpr_accuracy: 0.7360 - lit_accuracy: 0.8944 - prec_accuracy: 0.5435 - princ_accuracy: 0.8882 - psy_accuracy: 0.0714 - rule_accuracy: 0.6460 - syst_accuracy: 0.6491 - tele_accuracy: 0.9814 - val_loss: 4320.1846 - val_aut_loss: 1081.4639 - val_class_loss: 0.7937 - val_itpr_loss: 745.3345 - val_lit_loss: 356.2329 - val_prec_loss: 0.6545 - val_princ_loss: 422.0938 - val_psy_loss: 795.8800 - val_rule_loss: 554.1828 - val_syst_loss: 0.6507 - val_tele_loss: 359.7014 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0247 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.7778 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 0.9877 - val_tele_accuracy: 0.9877\n",
            "Epoch 86/200\n",
            "6/6 [==============================] - 21s 4s/step - loss: 9.4312 - aut_loss: 0.4006 - class_loss: 0.7976 - itpr_loss: 0.6252 - lit_loss: 0.5680 - prec_loss: 0.6868 - princ_loss: 0.4215 - psy_loss: 0.9759 - rule_loss: 0.6590 - syst_loss: 0.6556 - tele_loss: 0.4444 - aut_accuracy: 0.9720 - class_accuracy: 0.2205 - itpr_accuracy: 0.7360 - lit_accuracy: 0.9130 - prec_accuracy: 0.5528 - princ_accuracy: 0.8882 - psy_accuracy: 0.0590 - rule_accuracy: 0.6273 - syst_accuracy: 0.6398 - tele_accuracy: 0.9783 - val_loss: 3964.2041 - val_aut_loss: 1019.6517 - val_class_loss: 0.7907 - val_itpr_loss: 683.4060 - val_lit_loss: 373.7702 - val_prec_loss: 0.6544 - val_princ_loss: 339.7167 - val_psy_loss: 715.6044 - val_rule_loss: 513.4537 - val_syst_loss: 0.6480 - val_tele_loss: 313.3118 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0247 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.7778 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 0.9877 - val_tele_accuracy: 0.9877\n",
            "Epoch 87/200\n",
            "6/6 [==============================] - 21s 4s/step - loss: 9.3760 - aut_loss: 0.3973 - class_loss: 0.7735 - itpr_loss: 0.6314 - lit_loss: 0.5648 - prec_loss: 0.6874 - princ_loss: 0.4151 - psy_loss: 0.9680 - rule_loss: 0.6371 - syst_loss: 0.6568 - tele_loss: 0.4482 - aut_accuracy: 0.9720 - class_accuracy: 0.2919 - itpr_accuracy: 0.7050 - lit_accuracy: 0.9193 - prec_accuracy: 0.5559 - princ_accuracy: 0.8882 - psy_accuracy: 0.0497 - rule_accuracy: 0.6832 - syst_accuracy: 0.6491 - tele_accuracy: 0.9720 - val_loss: 4290.6919 - val_aut_loss: 1089.9900 - val_class_loss: 0.7874 - val_itpr_loss: 740.0512 - val_lit_loss: 380.8168 - val_prec_loss: 0.6542 - val_princ_loss: 391.8563 - val_psy_loss: 780.9877 - val_rule_loss: 553.3810 - val_syst_loss: 0.6453 - val_tele_loss: 348.3260 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0247 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.7778 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 1.0000 - val_tele_accuracy: 0.9877\n",
            "Epoch 88/200\n",
            "6/6 [==============================] - 21s 4s/step - loss: 9.3749 - aut_loss: 0.3934 - class_loss: 0.7902 - itpr_loss: 0.6256 - lit_loss: 0.5607 - prec_loss: 0.6848 - princ_loss: 0.4109 - psy_loss: 0.9768 - rule_loss: 0.6503 - syst_loss: 0.6470 - tele_loss: 0.4387 - aut_accuracy: 0.9720 - class_accuracy: 0.2112 - itpr_accuracy: 0.7422 - lit_accuracy: 0.9161 - prec_accuracy: 0.5342 - princ_accuracy: 0.8882 - psy_accuracy: 0.0683 - rule_accuracy: 0.6925 - syst_accuracy: 0.6584 - tele_accuracy: 0.9783 - val_loss: 5240.0029 - val_aut_loss: 1190.9438 - val_class_loss: 0.7840 - val_itpr_loss: 919.8261 - val_lit_loss: 254.5268 - val_prec_loss: 0.6541 - val_princ_loss: 678.4833 - val_psy_loss: 1098.8289 - val_rule_loss: 650.9155 - val_syst_loss: 0.6424 - val_tele_loss: 441.2008 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0247 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.7901 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 1.0000 - val_tele_accuracy: 0.9877\n",
            "Epoch 89/200\n",
            "6/6 [==============================] - 21s 4s/step - loss: 9.4124 - aut_loss: 0.3957 - class_loss: 0.7995 - itpr_loss: 0.6218 - lit_loss: 0.5722 - prec_loss: 0.7061 - princ_loss: 0.4191 - psy_loss: 0.9567 - rule_loss: 0.6539 - syst_loss: 0.6467 - tele_loss: 0.4441 - aut_accuracy: 0.9720 - class_accuracy: 0.2267 - itpr_accuracy: 0.7391 - lit_accuracy: 0.8789 - prec_accuracy: 0.5248 - princ_accuracy: 0.8882 - psy_accuracy: 0.0714 - rule_accuracy: 0.6615 - syst_accuracy: 0.6770 - tele_accuracy: 0.9783 - val_loss: 5302.4658 - val_aut_loss: 1161.5778 - val_class_loss: 0.7808 - val_itpr_loss: 927.7722 - val_lit_loss: 195.3380 - val_prec_loss: 0.6539 - val_princ_loss: 748.4091 - val_psy_loss: 1171.4800 - val_rule_loss: 650.5129 - val_syst_loss: 0.6394 - val_tele_loss: 442.1052 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0247 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.7901 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 1.0000 - val_tele_accuracy: 0.9877\n",
            "Epoch 90/200\n",
            "6/6 [==============================] - 21s 4s/step - loss: 9.3717 - aut_loss: 0.3916 - class_loss: 0.7868 - itpr_loss: 0.6362 - lit_loss: 0.5626 - prec_loss: 0.6841 - princ_loss: 0.4105 - psy_loss: 0.9786 - rule_loss: 0.6474 - syst_loss: 0.6450 - tele_loss: 0.4323 - aut_accuracy: 0.9720 - class_accuracy: 0.2236 - itpr_accuracy: 0.6957 - lit_accuracy: 0.9255 - prec_accuracy: 0.5776 - princ_accuracy: 0.8882 - psy_accuracy: 0.0683 - rule_accuracy: 0.6708 - syst_accuracy: 0.6522 - tele_accuracy: 0.9814 - val_loss: 6142.9009 - val_aut_loss: 1345.6318 - val_class_loss: 0.7778 - val_itpr_loss: 1074.6094 - val_lit_loss: 226.8248 - val_prec_loss: 0.6540 - val_princ_loss: 868.3096 - val_psy_loss: 1357.2975 - val_rule_loss: 753.8819 - val_syst_loss: 0.6364 - val_tele_loss: 511.0810 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0247 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.7901 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 1.0000 - val_tele_accuracy: 0.9877\n",
            "Epoch 91/200\n",
            "6/6 [==============================] - 21s 4s/step - loss: 9.3285 - aut_loss: 0.3920 - class_loss: 0.7719 - itpr_loss: 0.6230 - lit_loss: 0.5514 - prec_loss: 0.6787 - princ_loss: 0.4104 - psy_loss: 0.9943 - rule_loss: 0.6400 - syst_loss: 0.6337 - tele_loss: 0.4366 - aut_accuracy: 0.9720 - class_accuracy: 0.2702 - itpr_accuracy: 0.7267 - lit_accuracy: 0.9317 - prec_accuracy: 0.6118 - princ_accuracy: 0.8882 - psy_accuracy: 0.0621 - rule_accuracy: 0.6925 - syst_accuracy: 0.6739 - tele_accuracy: 0.9752 - val_loss: 2951.1128 - val_aut_loss: 804.1058 - val_class_loss: 0.7748 - val_itpr_loss: 518.4310 - val_lit_loss: 373.5759 - val_prec_loss: 0.6541 - val_princ_loss: 144.7129 - val_psy_loss: 562.2216 - val_rule_loss: 389.6238 - val_syst_loss: 0.6332 - val_tele_loss: 153.1832 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0247 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.7901 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 1.0000 - val_tele_accuracy: 0.9877\n",
            "Epoch 92/200\n",
            "6/6 [==============================] - 21s 4s/step - loss: 9.3228 - aut_loss: 0.3776 - class_loss: 0.7739 - itpr_loss: 0.6295 - lit_loss: 0.5412 - prec_loss: 0.6938 - princ_loss: 0.4183 - psy_loss: 1.0042 - rule_loss: 0.6335 - syst_loss: 0.6322 - tele_loss: 0.4223 - aut_accuracy: 0.9720 - class_accuracy: 0.2547 - itpr_accuracy: 0.7174 - lit_accuracy: 0.9503 - prec_accuracy: 0.5155 - princ_accuracy: 0.8882 - psy_accuracy: 0.0497 - rule_accuracy: 0.7112 - syst_accuracy: 0.6832 - tele_accuracy: 0.9814 - val_loss: 4934.5972 - val_aut_loss: 1188.8862 - val_class_loss: 0.7716 - val_itpr_loss: 857.0320 - val_lit_loss: 333.2206 - val_prec_loss: 0.6542 - val_princ_loss: 551.8202 - val_psy_loss: 954.1041 - val_rule_loss: 624.5908 - val_syst_loss: 0.6300 - val_tele_loss: 419.6907 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0247 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.7778 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 1.0000 - val_tele_accuracy: 0.9877\n",
            "Epoch 93/200\n",
            "6/6 [==============================] - 21s 4s/step - loss: 9.3326 - aut_loss: 0.3908 - class_loss: 0.7681 - itpr_loss: 0.6176 - lit_loss: 0.5418 - prec_loss: 0.7114 - princ_loss: 0.4104 - psy_loss: 1.0050 - rule_loss: 0.6328 - syst_loss: 0.6265 - tele_loss: 0.4318 - aut_accuracy: 0.9720 - class_accuracy: 0.2391 - itpr_accuracy: 0.7640 - lit_accuracy: 0.9317 - prec_accuracy: 0.5311 - princ_accuracy: 0.8882 - psy_accuracy: 0.0311 - rule_accuracy: 0.7236 - syst_accuracy: 0.7019 - tele_accuracy: 0.9783 - val_loss: 4355.6631 - val_aut_loss: 1022.5226 - val_class_loss: 0.7685 - val_itpr_loss: 760.1967 - val_lit_loss: 254.4832 - val_prec_loss: 0.6543 - val_princ_loss: 523.6631 - val_psy_loss: 872.8802 - val_rule_loss: 546.4358 - val_syst_loss: 0.6268 - val_tele_loss: 370.2360 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0247 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.7654 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 1.0000 - val_tele_accuracy: 0.9877\n",
            "Epoch 94/200\n",
            "6/6 [==============================] - 21s 4s/step - loss: 9.3451 - aut_loss: 0.3782 - class_loss: 0.7784 - itpr_loss: 0.6306 - lit_loss: 0.5383 - prec_loss: 0.7051 - princ_loss: 0.4098 - psy_loss: 0.9959 - rule_loss: 0.6396 - syst_loss: 0.6479 - tele_loss: 0.4249 - aut_accuracy: 0.9720 - class_accuracy: 0.2329 - itpr_accuracy: 0.7112 - lit_accuracy: 0.9565 - prec_accuracy: 0.5248 - princ_accuracy: 0.8882 - psy_accuracy: 0.0559 - rule_accuracy: 0.6708 - syst_accuracy: 0.6273 - tele_accuracy: 0.9845 - val_loss: 1352314.6250 - val_aut_loss: 352221.0000 - val_class_loss: 0.7654 - val_itpr_loss: 406487.0625 - val_lit_loss: 55548.0312 - val_prec_loss: 0.6547 - val_princ_loss: 0.3787 - val_psy_loss: 172148.5000 - val_rule_loss: 215458.3906 - val_syst_loss: 0.6235 - val_tele_loss: 150446.0938 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0247 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.7531 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 1.0000 - val_tele_accuracy: 0.9877\n",
            "Epoch 95/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 9.2885 - aut_loss: 0.3768 - class_loss: 0.7693 - itpr_loss: 0.6211 - lit_loss: 0.5365 - prec_loss: 0.6966 - princ_loss: 0.4137 - psy_loss: 0.9808 - rule_loss: 0.6359 - syst_loss: 0.6292 - tele_loss: 0.4321 - aut_accuracy: 0.9720 - class_accuracy: 0.2981 - itpr_accuracy: 0.7236 - lit_accuracy: 0.9503 - prec_accuracy: 0.5186 - princ_accuracy: 0.8882 - psy_accuracy: 0.0652 - rule_accuracy: 0.7174 - syst_accuracy: 0.6988 - tele_accuracy: 0.9845 - val_loss: 15930.2549 - val_aut_loss: 4279.5469 - val_class_loss: 0.7623 - val_itpr_loss: 2761.5410 - val_lit_loss: 1984.5280 - val_prec_loss: 0.6550 - val_princ_loss: 869.9706 - val_psy_loss: 3004.4890 - val_rule_loss: 2064.5608 - val_syst_loss: 0.6202 - val_tele_loss: 960.3851 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0370 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.7407 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 1.0000 - val_tele_accuracy: 0.9877\n",
            "Epoch 96/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 9.3064 - aut_loss: 0.3816 - class_loss: 0.7646 - itpr_loss: 0.6189 - lit_loss: 0.5350 - prec_loss: 0.6938 - princ_loss: 0.4181 - psy_loss: 1.0107 - rule_loss: 0.6260 - syst_loss: 0.6298 - tele_loss: 0.4315 - aut_accuracy: 0.9720 - class_accuracy: 0.2578 - itpr_accuracy: 0.7422 - lit_accuracy: 0.9410 - prec_accuracy: 0.5311 - princ_accuracy: 0.8882 - psy_accuracy: 0.0466 - rule_accuracy: 0.7298 - syst_accuracy: 0.7081 - tele_accuracy: 0.9441 - val_loss: 5722.7681 - val_aut_loss: 1254.9385 - val_class_loss: 0.7592 - val_itpr_loss: 1001.0312 - val_lit_loss: 209.3493 - val_prec_loss: 0.6553 - val_princ_loss: 809.4193 - val_psy_loss: 1264.3214 - val_rule_loss: 701.8776 - val_syst_loss: 0.6167 - val_tele_loss: 476.6033 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0370 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.7407 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 1.0000 - val_tele_accuracy: 0.9877\n",
            "Epoch 97/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 9.2469 - aut_loss: 0.3781 - class_loss: 0.7488 - itpr_loss: 0.6140 - lit_loss: 0.5303 - prec_loss: 0.6873 - princ_loss: 0.4067 - psy_loss: 1.0212 - rule_loss: 0.6159 - syst_loss: 0.6311 - tele_loss: 0.4170 - aut_accuracy: 0.9720 - class_accuracy: 0.3230 - itpr_accuracy: 0.7547 - lit_accuracy: 0.9627 - prec_accuracy: 0.5528 - princ_accuracy: 0.8882 - psy_accuracy: 0.0559 - rule_accuracy: 0.7329 - syst_accuracy: 0.6988 - tele_accuracy: 0.9752 - val_loss: 1269007.7500 - val_aut_loss: 332140.2188 - val_class_loss: 0.7562 - val_itpr_loss: 383157.1875 - val_lit_loss: 51721.9727 - val_prec_loss: 0.6556 - val_princ_loss: 0.3782 - val_psy_loss: 160164.1562 - val_rule_loss: 202660.0312 - val_syst_loss: 0.6131 - val_tele_loss: 139158.6094 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0617 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.7407 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 1.0000 - val_tele_accuracy: 0.9877\n",
            "Epoch 98/200\n",
            "6/6 [==============================] - 21s 4s/step - loss: 9.2518 - aut_loss: 0.3756 - class_loss: 0.7627 - itpr_loss: 0.6124 - lit_loss: 0.5229 - prec_loss: 0.7057 - princ_loss: 0.4092 - psy_loss: 1.0243 - rule_loss: 0.6216 - syst_loss: 0.6116 - tele_loss: 0.4095 - aut_accuracy: 0.9720 - class_accuracy: 0.3106 - itpr_accuracy: 0.7391 - lit_accuracy: 0.9503 - prec_accuracy: 0.4969 - princ_accuracy: 0.8882 - psy_accuracy: 0.0435 - rule_accuracy: 0.7236 - syst_accuracy: 0.7360 - tele_accuracy: 0.9783 - val_loss: 1377162.0000 - val_aut_loss: 360013.0000 - val_class_loss: 0.7533 - val_itpr_loss: 415434.3125 - val_lit_loss: 55425.5664 - val_prec_loss: 0.6560 - val_princ_loss: 0.3781 - val_psy_loss: 174421.1719 - val_rule_loss: 220046.2188 - val_syst_loss: 0.6093 - val_tele_loss: 151816.0938 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0617 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.7531 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 1.0000 - val_tele_accuracy: 0.9877\n",
            "Epoch 99/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 9.2431 - aut_loss: 0.3786 - class_loss: 0.7533 - itpr_loss: 0.6171 - lit_loss: 0.5259 - prec_loss: 0.6889 - princ_loss: 0.4044 - psy_loss: 1.0441 - rule_loss: 0.6133 - syst_loss: 0.6096 - tele_loss: 0.4116 - aut_accuracy: 0.9720 - class_accuracy: 0.3199 - itpr_accuracy: 0.7516 - lit_accuracy: 0.9534 - prec_accuracy: 0.5248 - princ_accuracy: 0.8882 - psy_accuracy: 0.0528 - rule_accuracy: 0.7795 - syst_accuracy: 0.7764 - tele_accuracy: 0.9783 - val_loss: 5224.9893 - val_aut_loss: 1115.1355 - val_class_loss: 0.7506 - val_itpr_loss: 888.7508 - val_lit_loss: 127.8620 - val_prec_loss: 0.6564 - val_princ_loss: 801.0900 - val_psy_loss: 1235.1233 - val_rule_loss: 615.1609 - val_syst_loss: 0.6053 - val_tele_loss: 436.6584 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0617 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.7531 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 1.0000 - val_tele_accuracy: 0.9877\n",
            "Epoch 100/200\n",
            "6/6 [==============================] - 21s 4s/step - loss: 9.2371 - aut_loss: 0.3655 - class_loss: 0.7512 - itpr_loss: 0.6158 - lit_loss: 0.5267 - prec_loss: 0.6928 - princ_loss: 0.4164 - psy_loss: 1.0248 - rule_loss: 0.6135 - syst_loss: 0.6098 - tele_loss: 0.4245 - aut_accuracy: 0.9720 - class_accuracy: 0.3106 - itpr_accuracy: 0.7050 - lit_accuracy: 0.9317 - prec_accuracy: 0.4969 - princ_accuracy: 0.8882 - psy_accuracy: 0.0528 - rule_accuracy: 0.7298 - syst_accuracy: 0.7484 - tele_accuracy: 0.9596 - val_loss: 5326.2861 - val_aut_loss: 1136.8716 - val_class_loss: 0.7475 - val_itpr_loss: 906.0909 - val_lit_loss: 130.2148 - val_prec_loss: 0.6570 - val_princ_loss: 816.7078 - val_psy_loss: 1259.1820 - val_rule_loss: 627.0774 - val_syst_loss: 0.6012 - val_tele_loss: 444.9402 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0741 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.7531 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 1.0000 - val_tele_accuracy: 0.9877\n",
            "Epoch 101/200\n",
            "6/6 [==============================] - 21s 4s/step - loss: 9.5660 - aut_loss: 0.3702 - class_loss: 0.7689 - itpr_loss: 0.6238 - lit_loss: 0.5799 - prec_loss: 0.7286 - princ_loss: 0.4176 - psy_loss: 1.0164 - rule_loss: 0.6750 - syst_loss: 0.7673 - tele_loss: 0.4220 - aut_accuracy: 0.9752 - class_accuracy: 0.2702 - itpr_accuracy: 0.7267 - lit_accuracy: 0.9441 - prec_accuracy: 0.5963 - princ_accuracy: 0.8882 - psy_accuracy: 0.0776 - rule_accuracy: 0.7236 - syst_accuracy: 0.7516 - tele_accuracy: 0.9658 - val_loss: 18908.9336 - val_aut_loss: 4598.0762 - val_class_loss: 0.7446 - val_itpr_loss: 3821.4500 - val_lit_loss: 867.7266 - val_prec_loss: 0.6575 - val_princ_loss: 2034.3654 - val_psy_loss: 3193.5042 - val_rule_loss: 2431.6750 - val_syst_loss: 0.5973 - val_tele_loss: 1956.9398 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0864 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.7407 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 1.0000 - val_tele_accuracy: 0.9877\n",
            "Epoch 102/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 9.1881 - aut_loss: 0.3688 - class_loss: 0.7506 - itpr_loss: 0.6084 - lit_loss: 0.5063 - prec_loss: 0.6880 - princ_loss: 0.4174 - psy_loss: 1.0478 - rule_loss: 0.6089 - syst_loss: 0.5939 - tele_loss: 0.4018 - aut_accuracy: 0.9720 - class_accuracy: 0.3261 - itpr_accuracy: 0.7826 - lit_accuracy: 0.9720 - prec_accuracy: 0.5373 - princ_accuracy: 0.8882 - psy_accuracy: 0.0311 - rule_accuracy: 0.7453 - syst_accuracy: 0.7702 - tele_accuracy: 0.9689 - val_loss: 6914.0078 - val_aut_loss: 1625.2471 - val_class_loss: 0.7418 - val_itpr_loss: 1208.3917 - val_lit_loss: 403.4352 - val_prec_loss: 0.6583 - val_princ_loss: 831.1368 - val_psy_loss: 1387.1045 - val_rule_loss: 867.7430 - val_syst_loss: 0.5932 - val_tele_loss: 585.7590 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0864 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.7531 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 1.0000 - val_tele_accuracy: 0.9877\n",
            "Epoch 103/200\n",
            "6/6 [==============================] - 21s 4s/step - loss: 9.2207 - aut_loss: 0.3691 - class_loss: 0.7479 - itpr_loss: 0.6183 - lit_loss: 0.5154 - prec_loss: 0.6816 - princ_loss: 0.4215 - psy_loss: 1.0472 - rule_loss: 0.6114 - syst_loss: 0.5982 - tele_loss: 0.4138 - aut_accuracy: 0.9720 - class_accuracy: 0.3602 - itpr_accuracy: 0.7298 - lit_accuracy: 0.9596 - prec_accuracy: 0.5714 - princ_accuracy: 0.8882 - psy_accuracy: 0.0590 - rule_accuracy: 0.7609 - syst_accuracy: 0.7764 - tele_accuracy: 0.9814 - val_loss: 1778482.0000 - val_aut_loss: 465444.0625 - val_class_loss: 0.7389 - val_itpr_loss: 536788.7500 - val_lit_loss: 71294.7188 - val_prec_loss: 0.6593 - val_princ_loss: 0.3778 - val_psy_loss: 225137.4375 - val_rule_loss: 284080.2812 - val_syst_loss: 0.5891 - val_tele_loss: 195731.0938 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.0864 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.7531 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 1.0000 - val_tele_accuracy: 0.9877\n",
            "Epoch 104/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 9.2071 - aut_loss: 0.3659 - class_loss: 0.7496 - itpr_loss: 0.6108 - lit_loss: 0.5061 - prec_loss: 0.6902 - princ_loss: 0.4194 - psy_loss: 1.0516 - rule_loss: 0.6133 - syst_loss: 0.5921 - tele_loss: 0.4119 - aut_accuracy: 0.9720 - class_accuracy: 0.2981 - itpr_accuracy: 0.7547 - lit_accuracy: 0.9503 - prec_accuracy: 0.5776 - princ_accuracy: 0.8882 - psy_accuracy: 0.0652 - rule_accuracy: 0.7267 - syst_accuracy: 0.7857 - tele_accuracy: 0.9689 - val_loss: 5700.6719 - val_aut_loss: 1376.5857 - val_class_loss: 0.7361 - val_itpr_loss: 990.6089 - val_lit_loss: 382.8192 - val_prec_loss: 0.6604 - val_princ_loss: 638.3138 - val_psy_loss: 1100.1581 - val_rule_loss: 721.4996 - val_syst_loss: 0.5850 - val_tele_loss: 485.5095 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.1111 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.7654 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 1.0000 - val_tele_accuracy: 0.9877\n",
            "Epoch 105/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 9.2136 - aut_loss: 0.3716 - class_loss: 0.7342 - itpr_loss: 0.6134 - lit_loss: 0.5114 - prec_loss: 0.6895 - princ_loss: 0.4196 - psy_loss: 1.0480 - rule_loss: 0.6150 - syst_loss: 0.5966 - tele_loss: 0.4181 - aut_accuracy: 0.9720 - class_accuracy: 0.3602 - itpr_accuracy: 0.7360 - lit_accuracy: 0.9472 - prec_accuracy: 0.5528 - princ_accuracy: 0.8882 - psy_accuracy: 0.0528 - rule_accuracy: 0.7298 - syst_accuracy: 0.7764 - tele_accuracy: 0.9814 - val_loss: 7251.2842 - val_aut_loss: 1597.5654 - val_class_loss: 0.7336 - val_itpr_loss: 1262.7966 - val_lit_loss: 264.4684 - val_prec_loss: 0.6613 - val_princ_loss: 1026.1998 - val_psy_loss: 1592.1599 - val_rule_loss: 889.5573 - val_syst_loss: 0.5810 - val_tele_loss: 613.3647 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.1111 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.7531 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 1.0000 - val_tele_accuracy: 0.9877\n",
            "Epoch 106/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 9.1865 - aut_loss: 0.3630 - class_loss: 0.7558 - itpr_loss: 0.6022 - lit_loss: 0.5093 - prec_loss: 0.6886 - princ_loss: 0.4111 - psy_loss: 1.0500 - rule_loss: 0.6130 - syst_loss: 0.5859 - tele_loss: 0.4113 - aut_accuracy: 0.9720 - class_accuracy: 0.3199 - itpr_accuracy: 0.7609 - lit_accuracy: 0.9658 - prec_accuracy: 0.5466 - princ_accuracy: 0.8882 - psy_accuracy: 0.0497 - rule_accuracy: 0.7112 - syst_accuracy: 0.7888 - tele_accuracy: 0.9752 - val_loss: 23194.8301 - val_aut_loss: 6187.6357 - val_class_loss: 0.7310 - val_itpr_loss: 4014.7654 - val_lit_loss: 2803.1487 - val_prec_loss: 0.6624 - val_princ_loss: 1368.8146 - val_psy_loss: 4334.4639 - val_rule_loss: 2993.7402 - val_syst_loss: 0.5769 - val_tele_loss: 1487.0942 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.1235 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.7160 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 1.0000 - val_tele_accuracy: 0.9877\n",
            "Epoch 107/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 9.1808 - aut_loss: 0.3643 - class_loss: 0.7364 - itpr_loss: 0.6095 - lit_loss: 0.5012 - prec_loss: 0.6904 - princ_loss: 0.4300 - psy_loss: 1.0522 - rule_loss: 0.6040 - syst_loss: 0.5843 - tele_loss: 0.4122 - aut_accuracy: 0.9720 - class_accuracy: 0.3602 - itpr_accuracy: 0.7826 - lit_accuracy: 0.9689 - prec_accuracy: 0.5342 - princ_accuracy: 0.8882 - psy_accuracy: 0.0652 - rule_accuracy: 0.7081 - syst_accuracy: 0.7857 - tele_accuracy: 0.9783 - val_loss: 29254.3047 - val_aut_loss: 7514.4346 - val_class_loss: 0.7282 - val_itpr_loss: 5782.4692 - val_lit_loss: 1807.5369 - val_prec_loss: 0.6638 - val_princ_loss: 2778.6494 - val_psy_loss: 4481.9058 - val_rule_loss: 3724.5005 - val_syst_loss: 0.5725 - val_tele_loss: 3159.6455 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.1358 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.6914 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 1.0000 - val_tele_accuracy: 0.9877\n",
            "Epoch 108/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 9.1817 - aut_loss: 0.3532 - class_loss: 0.7321 - itpr_loss: 0.6053 - lit_loss: 0.5014 - prec_loss: 0.7014 - princ_loss: 0.4190 - psy_loss: 1.0531 - rule_loss: 0.6257 - syst_loss: 0.5816 - tele_loss: 0.4129 - aut_accuracy: 0.9720 - class_accuracy: 0.3571 - itpr_accuracy: 0.7453 - lit_accuracy: 0.9472 - prec_accuracy: 0.5155 - princ_accuracy: 0.8882 - psy_accuracy: 0.0683 - rule_accuracy: 0.7298 - syst_accuracy: 0.7950 - tele_accuracy: 0.9720 - val_loss: 6341.5693 - val_aut_loss: 1356.7108 - val_class_loss: 0.7256 - val_itpr_loss: 1077.8054 - val_lit_loss: 152.4385 - val_prec_loss: 0.6651 - val_princ_loss: 973.3348 - val_psy_loss: 1498.4517 - val_rule_loss: 746.2090 - val_syst_loss: 0.5681 - val_tele_loss: 531.4639 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.1481 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.6790 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 1.0000 - val_tele_accuracy: 0.9877\n",
            "Epoch 109/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 9.1698 - aut_loss: 0.3555 - class_loss: 0.7361 - itpr_loss: 0.6165 - lit_loss: 0.4879 - prec_loss: 0.6962 - princ_loss: 0.4126 - psy_loss: 1.0661 - rule_loss: 0.6057 - syst_loss: 0.5864 - tele_loss: 0.4106 - aut_accuracy: 0.9720 - class_accuracy: 0.3727 - itpr_accuracy: 0.7391 - lit_accuracy: 0.9565 - prec_accuracy: 0.5404 - princ_accuracy: 0.8882 - psy_accuracy: 0.0745 - rule_accuracy: 0.7609 - syst_accuracy: 0.7764 - tele_accuracy: 0.9720 - val_loss: 4479.5176 - val_aut_loss: 989.6957 - val_class_loss: 0.7230 - val_itpr_loss: 779.8843 - val_lit_loss: 169.1404 - val_prec_loss: 0.6666 - val_princ_loss: 627.0009 - val_psy_loss: 985.9830 - val_rule_loss: 549.4103 - val_syst_loss: 0.5635 - val_tele_loss: 373.2542 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.1605 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.6543 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 1.0000 - val_tele_accuracy: 0.9877\n",
            "Epoch 110/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 9.1253 - aut_loss: 0.3551 - class_loss: 0.7298 - itpr_loss: 0.5975 - lit_loss: 0.4811 - prec_loss: 0.6989 - princ_loss: 0.4145 - psy_loss: 1.0889 - rule_loss: 0.5910 - syst_loss: 0.5713 - tele_loss: 0.4012 - aut_accuracy: 0.9720 - class_accuracy: 0.3727 - itpr_accuracy: 0.7702 - lit_accuracy: 0.9783 - prec_accuracy: 0.4907 - princ_accuracy: 0.8882 - psy_accuracy: 0.0373 - rule_accuracy: 0.7795 - syst_accuracy: 0.8261 - tele_accuracy: 0.9814 - val_loss: 8534.0107 - val_aut_loss: 1835.3633 - val_class_loss: 0.7202 - val_itpr_loss: 1434.6156 - val_lit_loss: 204.6612 - val_prec_loss: 0.6680 - val_princ_loss: 1316.6982 - val_psy_loss: 2002.8679 - val_rule_loss: 1002.9757 - val_syst_loss: 0.5588 - val_tele_loss: 731.6858 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.1605 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.6296 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 1.0000 - val_tele_accuracy: 0.9877\n",
            "Epoch 111/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 9.0953 - aut_loss: 0.3459 - class_loss: 0.7311 - itpr_loss: 0.5989 - lit_loss: 0.4751 - prec_loss: 0.6839 - princ_loss: 0.4383 - psy_loss: 1.0713 - rule_loss: 0.5941 - syst_loss: 0.5622 - tele_loss: 0.3984 - aut_accuracy: 0.9720 - class_accuracy: 0.3789 - itpr_accuracy: 0.7764 - lit_accuracy: 0.9845 - prec_accuracy: 0.5621 - princ_accuracy: 0.8882 - psy_accuracy: 0.0466 - rule_accuracy: 0.7609 - syst_accuracy: 0.8354 - tele_accuracy: 0.9814 - val_loss: 5090.8779 - val_aut_loss: 1091.6794 - val_class_loss: 0.7175 - val_itpr_loss: 859.5809 - val_lit_loss: 122.9723 - val_prec_loss: 0.6691 - val_princ_loss: 782.8134 - val_psy_loss: 1199.0901 - val_rule_loss: 598.6971 - val_syst_loss: 0.5545 - val_tele_loss: 430.9073 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.1605 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.6420 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 1.0000 - val_tele_accuracy: 0.9877\n",
            "Epoch 112/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 9.1038 - aut_loss: 0.3511 - class_loss: 0.7101 - itpr_loss: 0.6063 - lit_loss: 0.4723 - prec_loss: 0.7138 - princ_loss: 0.4158 - psy_loss: 1.0943 - rule_loss: 0.5761 - syst_loss: 0.5622 - tele_loss: 0.4057 - aut_accuracy: 0.9720 - class_accuracy: 0.4472 - itpr_accuracy: 0.7578 - lit_accuracy: 0.9814 - prec_accuracy: 0.4938 - princ_accuracy: 0.8882 - psy_accuracy: 0.0714 - rule_accuracy: 0.7733 - syst_accuracy: 0.8137 - tele_accuracy: 0.9658 - val_loss: 8483.5225 - val_aut_loss: 1938.6094 - val_class_loss: 0.7149 - val_itpr_loss: 1484.1366 - val_lit_loss: 405.4993 - val_prec_loss: 0.6704 - val_princ_loss: 1106.0687 - val_psy_loss: 1772.6422 - val_rule_loss: 1053.1703 - val_syst_loss: 0.5503 - val_tele_loss: 718.2637 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.1975 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.6420 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 1.0000 - val_tele_accuracy: 0.9877\n",
            "Epoch 113/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 9.1058 - aut_loss: 0.3461 - class_loss: 0.7212 - itpr_loss: 0.6033 - lit_loss: 0.4719 - prec_loss: 0.6948 - princ_loss: 0.4238 - psy_loss: 1.0923 - rule_loss: 0.5998 - syst_loss: 0.5625 - tele_loss: 0.3939 - aut_accuracy: 0.9720 - class_accuracy: 0.4255 - itpr_accuracy: 0.7547 - lit_accuracy: 0.9876 - prec_accuracy: 0.5342 - princ_accuracy: 0.8882 - psy_accuracy: 0.0311 - rule_accuracy: 0.7578 - syst_accuracy: 0.8075 - tele_accuracy: 0.9720 - val_loss: 59092.2656 - val_aut_loss: 16308.7734 - val_class_loss: 0.7124 - val_itpr_loss: 10660.1602 - val_lit_loss: 7056.0522 - val_prec_loss: 0.6719 - val_princ_loss: 3161.1641 - val_psy_loss: 9814.0771 - val_rule_loss: 7871.3535 - val_syst_loss: 0.5459 - val_tele_loss: 4215.5508 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.2222 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.6296 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 1.0000 - val_tele_accuracy: 0.9877\n",
            "Epoch 114/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 9.1009 - aut_loss: 0.3462 - class_loss: 0.7093 - itpr_loss: 0.6037 - lit_loss: 0.4739 - prec_loss: 0.7100 - princ_loss: 0.4241 - psy_loss: 1.0796 - rule_loss: 0.6032 - syst_loss: 0.5498 - tele_loss: 0.4050 - aut_accuracy: 0.9720 - class_accuracy: 0.4441 - itpr_accuracy: 0.7671 - lit_accuracy: 0.9627 - prec_accuracy: 0.5186 - princ_accuracy: 0.8882 - psy_accuracy: 0.0621 - rule_accuracy: 0.7826 - syst_accuracy: 0.8292 - tele_accuracy: 0.9534 - val_loss: 14424.2070 - val_aut_loss: 3886.3184 - val_class_loss: 0.7097 - val_itpr_loss: 2499.7842 - val_lit_loss: 1788.6776 - val_prec_loss: 0.6735 - val_princ_loss: 793.5152 - val_psy_loss: 2703.4683 - val_rule_loss: 1871.6239 - val_syst_loss: 0.5414 - val_tele_loss: 875.6998 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.2346 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.6296 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 1.0000 - val_tele_accuracy: 0.9877\n",
            "Epoch 115/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 25652.1094 - aut_loss: 0.3422 - class_loss: 0.6973 - itpr_loss: 8789.9766 - lit_loss: 180.6469 - prec_loss: 1957.8312 - princ_loss: 0.4008 - psy_loss: 11623.7900 - rule_loss: 3094.2686 - syst_loss: 0.5620 - tele_loss: 0.3963 - aut_accuracy: 0.9720 - class_accuracy: 0.4845 - itpr_accuracy: 0.7236 - lit_accuracy: 0.9752 - prec_accuracy: 0.5528 - princ_accuracy: 0.8913 - psy_accuracy: 0.0590 - rule_accuracy: 0.7733 - syst_accuracy: 0.8323 - tele_accuracy: 0.9783 - val_loss: 97081.8672 - val_aut_loss: 27231.6504 - val_class_loss: 0.7072 - val_itpr_loss: 17616.8867 - val_lit_loss: 12940.8418 - val_prec_loss: 0.6752 - val_princ_loss: 3626.9333 - val_psy_loss: 17859.2656 - val_rule_loss: 12807.3584 - val_syst_loss: 0.5371 - val_tele_loss: 4993.8071 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.2469 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.6296 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 1.0000 - val_tele_accuracy: 0.9877\n",
            "Epoch 116/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 9.0369 - aut_loss: 0.3408 - class_loss: 0.7206 - itpr_loss: 0.6058 - lit_loss: 0.4577 - prec_loss: 0.6823 - princ_loss: 0.4066 - psy_loss: 1.0980 - rule_loss: 0.5794 - syst_loss: 0.5612 - tele_loss: 0.3885 - aut_accuracy: 0.9720 - class_accuracy: 0.4224 - itpr_accuracy: 0.7484 - lit_accuracy: 0.9814 - prec_accuracy: 0.5497 - princ_accuracy: 0.8882 - psy_accuracy: 0.0528 - rule_accuracy: 0.7671 - syst_accuracy: 0.8137 - tele_accuracy: 0.9720 - val_loss: 3411.0767 - val_aut_loss: 665.0718 - val_class_loss: 0.7046 - val_itpr_loss: 908.0987 - val_lit_loss: 36.8762 - val_prec_loss: 144.8618 - val_princ_loss: 251.2377 - val_psy_loss: 1079.5942 - val_rule_loss: 320.5381 - val_syst_loss: 0.5328 - val_tele_loss: 0.3651 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.2593 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.6173 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 1.0000 - val_tele_accuracy: 1.0000\n",
            "Epoch 117/200\n",
            "6/6 [==============================] - 21s 4s/step - loss: 9.0062 - aut_loss: 0.3378 - class_loss: 0.7003 - itpr_loss: 0.5895 - lit_loss: 0.4520 - prec_loss: 0.6998 - princ_loss: 0.4078 - psy_loss: 1.1112 - rule_loss: 0.5813 - syst_loss: 0.5415 - tele_loss: 0.3889 - aut_accuracy: 0.9720 - class_accuracy: 0.4689 - itpr_accuracy: 0.7795 - lit_accuracy: 0.9752 - prec_accuracy: 0.5435 - princ_accuracy: 0.8882 - psy_accuracy: 0.0590 - rule_accuracy: 0.7764 - syst_accuracy: 0.8540 - tele_accuracy: 0.9720 - val_loss: 38.1915 - val_aut_loss: 14.8948 - val_class_loss: 2.2394 - val_itpr_loss: 0.5465 - val_lit_loss: 0.4202 - val_prec_loss: 0.6789 - val_princ_loss: 0.3772 - val_psy_loss: 2.5785 - val_rule_loss: 2.9036 - val_syst_loss: 6.4950 - val_tele_loss: 3.8614 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.2469 - val_itpr_accuracy: 0.8889 - val_lit_accuracy: 1.0000 - val_prec_accuracy: 0.6296 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 0.9877 - val_tele_accuracy: 0.9877\n",
            "Epoch 118/200\n",
            "6/6 [==============================] - 21s 4s/step - loss: 9.0547 - aut_loss: 0.3300 - class_loss: 0.7094 - itpr_loss: 0.5997 - lit_loss: 0.4539 - prec_loss: 0.7258 - princ_loss: 0.4266 - psy_loss: 1.1048 - rule_loss: 0.5772 - syst_loss: 0.5472 - tele_loss: 0.3841 - aut_accuracy: 0.9720 - class_accuracy: 0.4255 - itpr_accuracy: 0.7484 - lit_accuracy: 0.9752 - prec_accuracy: 0.5373 - princ_accuracy: 0.8882 - psy_accuracy: 0.0714 - rule_accuracy: 0.7764 - syst_accuracy: 0.8261 - tele_accuracy: 0.9689 - val_loss: 47176.0820 - val_aut_loss: 12986.3809 - val_class_loss: 0.6992 - val_itpr_loss: 8479.0771 - val_lit_loss: 5527.2969 - val_prec_loss: 0.6807 - val_princ_loss: 2656.0166 - val_psy_loss: 7712.8945 - val_rule_loss: 6269.1357 - val_syst_loss: 0.5236 - val_tele_loss: 3540.1841 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.2716 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.6049 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 1.0000 - val_tele_accuracy: 0.9877\n",
            "Epoch 119/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 771.0993 - aut_loss: 0.3262 - class_loss: 0.7129 - itpr_loss: 191.2994 - lit_loss: 141.9047 - prec_loss: 0.6994 - princ_loss: 0.4041 - psy_loss: 162.8415 - rule_loss: 140.6584 - syst_loss: 0.5351 - tele_loss: 128.5217 - aut_accuracy: 0.9720 - class_accuracy: 0.4658 - itpr_accuracy: 0.7671 - lit_accuracy: 0.9720 - prec_accuracy: 0.5683 - princ_accuracy: 0.8913 - psy_accuracy: 0.0714 - rule_accuracy: 0.7702 - syst_accuracy: 0.8696 - tele_accuracy: 0.9689 - val_loss: 12691.3857 - val_aut_loss: 3391.9143 - val_class_loss: 0.6967 - val_itpr_loss: 2195.7959 - val_lit_loss: 1536.2599 - val_prec_loss: 0.6823 - val_princ_loss: 744.6091 - val_psy_loss: 2377.7500 - val_rule_loss: 1637.9637 - val_syst_loss: 0.5197 - val_tele_loss: 801.9977 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.2963 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.6049 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 1.0000 - val_tele_accuracy: 0.9877\n",
            "Epoch 120/200\n",
            "6/6 [==============================] - 21s 4s/step - loss: 9.0477 - aut_loss: 0.3329 - class_loss: 0.7136 - itpr_loss: 0.5989 - lit_loss: 0.4508 - prec_loss: 0.7406 - princ_loss: 0.4102 - psy_loss: 1.0885 - rule_loss: 0.5865 - syst_loss: 0.5373 - tele_loss: 0.3926 - aut_accuracy: 0.9720 - class_accuracy: 0.4596 - itpr_accuracy: 0.7422 - lit_accuracy: 0.9876 - prec_accuracy: 0.5031 - princ_accuracy: 0.8882 - psy_accuracy: 0.0683 - rule_accuracy: 0.7516 - syst_accuracy: 0.8416 - tele_accuracy: 0.9689 - val_loss: 13504.6494 - val_aut_loss: 3582.1355 - val_class_loss: 0.6941 - val_itpr_loss: 2326.6025 - val_lit_loss: 1587.3318 - val_prec_loss: 0.6842 - val_princ_loss: 852.9689 - val_psy_loss: 2506.5859 - val_rule_loss: 1735.9392 - val_syst_loss: 0.5154 - val_tele_loss: 907.9969 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.3086 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.5802 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 1.0000 - val_tele_accuracy: 0.9877\n",
            "Epoch 121/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 1912.4988 - aut_loss: 27.0915 - class_loss: 450.8500 - itpr_loss: 34.1006 - lit_loss: 81.2049 - prec_loss: 0.7058 - princ_loss: 258.7719 - psy_loss: 300.3276 - rule_loss: 486.5177 - syst_loss: 109.2240 - tele_loss: 160.5088 - aut_accuracy: 0.9720 - class_accuracy: 0.4286 - itpr_accuracy: 0.7795 - lit_accuracy: 0.9689 - prec_accuracy: 0.5311 - princ_accuracy: 0.8851 - psy_accuracy: 0.0714 - rule_accuracy: 0.7609 - syst_accuracy: 0.8882 - tele_accuracy: 0.9752 - val_loss: 10808.9893 - val_aut_loss: 2971.5591 - val_class_loss: 0.6917 - val_itpr_loss: 1895.4614 - val_lit_loss: 1450.2433 - val_prec_loss: 0.6857 - val_princ_loss: 452.8414 - val_psy_loss: 2107.6953 - val_rule_loss: 1413.7388 - val_syst_loss: 0.5116 - val_tele_loss: 512.3643 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.3210 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.5802 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 1.0000 - val_tele_accuracy: 0.9877\n",
            "Epoch 122/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 9.0249 - aut_loss: 0.3285 - class_loss: 0.6976 - itpr_loss: 0.5905 - lit_loss: 0.4379 - prec_loss: 0.7428 - princ_loss: 0.4174 - psy_loss: 1.1166 - rule_loss: 0.5848 - syst_loss: 0.5285 - tele_loss: 0.3843 - aut_accuracy: 0.9720 - class_accuracy: 0.4969 - itpr_accuracy: 0.8012 - lit_accuracy: 0.9876 - prec_accuracy: 0.5062 - princ_accuracy: 0.8882 - psy_accuracy: 0.0466 - rule_accuracy: 0.7516 - syst_accuracy: 0.8416 - tele_accuracy: 0.9783 - val_loss: 24.1144 - val_aut_loss: 2.1651 - val_class_loss: 7.4614 - val_itpr_loss: 0.5359 - val_lit_loss: 0.4002 - val_prec_loss: 0.6872 - val_princ_loss: 0.3778 - val_psy_loss: 1.0784 - val_rule_loss: 6.7282 - val_syst_loss: 1.1241 - val_tele_loss: 0.3600 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.3210 - val_itpr_accuracy: 0.8889 - val_lit_accuracy: 1.0000 - val_prec_accuracy: 0.5802 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.0123 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 0.9877 - val_tele_accuracy: 1.0000\n",
            "Epoch 123/200\n",
            "6/6 [==============================] - 21s 4s/step - loss: 8.9783 - aut_loss: 0.3223 - class_loss: 0.6944 - itpr_loss: 0.6016 - lit_loss: 0.4363 - prec_loss: 0.7151 - princ_loss: 0.4125 - psy_loss: 1.1149 - rule_loss: 0.5721 - syst_loss: 0.5268 - tele_loss: 0.3863 - aut_accuracy: 0.9720 - class_accuracy: 0.4627 - itpr_accuracy: 0.7422 - lit_accuracy: 0.9876 - prec_accuracy: 0.4814 - princ_accuracy: 0.8882 - psy_accuracy: 0.0683 - rule_accuracy: 0.7547 - syst_accuracy: 0.8634 - tele_accuracy: 0.9845 - val_loss: 47333.0508 - val_aut_loss: 12769.1045 - val_class_loss: 0.6880 - val_itpr_loss: 8656.0244 - val_lit_loss: 4695.7158 - val_prec_loss: 0.6887 - val_princ_loss: 3416.9417 - val_psy_loss: 7080.8633 - val_rule_loss: 6250.1631 - val_syst_loss: 0.5038 - val_tele_loss: 4459.1606 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.3457 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.5802 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 1.0000 - val_tele_accuracy: 0.9877\n",
            "Epoch 124/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 752.2856 - aut_loss: 0.3236 - class_loss: 0.6947 - itpr_loss: 0.5861 - lit_loss: 167.4949 - prec_loss: 0.7054 - princ_loss: 82.1795 - psy_loss: 202.1582 - rule_loss: 149.5831 - syst_loss: 0.5194 - tele_loss: 144.8447 - aut_accuracy: 0.9720 - class_accuracy: 0.4876 - itpr_accuracy: 0.7671 - lit_accuracy: 0.9845 - prec_accuracy: 0.5311 - princ_accuracy: 0.8851 - psy_accuracy: 0.0621 - rule_accuracy: 0.7702 - syst_accuracy: 0.8571 - tele_accuracy: 0.9876 - val_loss: 10188.0059 - val_aut_loss: 2707.6953 - val_class_loss: 0.6854 - val_itpr_loss: 1752.6633 - val_lit_loss: 1194.3363 - val_prec_loss: 0.6912 - val_princ_loss: 645.9576 - val_psy_loss: 1877.5448 - val_rule_loss: 1311.7379 - val_syst_loss: 0.4995 - val_tele_loss: 692.9991 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.3704 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.5679 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 1.0000 - val_tele_accuracy: 0.9877\n",
            "Epoch 125/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 9.0003 - aut_loss: 0.3337 - class_loss: 0.6874 - itpr_loss: 0.5946 - lit_loss: 0.4264 - prec_loss: 0.7190 - princ_loss: 0.4221 - psy_loss: 1.1428 - rule_loss: 0.5921 - syst_loss: 0.5060 - tele_loss: 0.3804 - aut_accuracy: 0.9689 - class_accuracy: 0.5186 - itpr_accuracy: 0.7671 - lit_accuracy: 0.9876 - prec_accuracy: 0.5342 - princ_accuracy: 0.8882 - psy_accuracy: 0.0714 - rule_accuracy: 0.7702 - syst_accuracy: 0.8789 - tele_accuracy: 0.9783 - val_loss: 10125.2725 - val_aut_loss: 2610.9170 - val_class_loss: 0.6827 - val_itpr_loss: 1728.0242 - val_lit_loss: 1069.0171 - val_prec_loss: 0.6937 - val_princ_loss: 785.8170 - val_psy_loss: 1844.6174 - val_rule_loss: 1285.0269 - val_syst_loss: 0.4952 - val_tele_loss: 796.7859 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.4444 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.5679 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 1.0000 - val_tele_accuracy: 0.9877\n",
            "Epoch 126/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 8.9488 - aut_loss: 0.3223 - class_loss: 0.6778 - itpr_loss: 0.5826 - lit_loss: 0.4296 - prec_loss: 0.7188 - princ_loss: 0.4178 - psy_loss: 1.1278 - rule_loss: 0.5701 - syst_loss: 0.5179 - tele_loss: 0.3883 - aut_accuracy: 0.9720 - class_accuracy: 0.5280 - itpr_accuracy: 0.7609 - lit_accuracy: 0.9876 - prec_accuracy: 0.5155 - princ_accuracy: 0.8882 - psy_accuracy: 0.0683 - rule_accuracy: 0.8012 - syst_accuracy: 0.8696 - tele_accuracy: 0.9689 - val_loss: 9706.0234 - val_aut_loss: 2335.1248 - val_class_loss: 0.6800 - val_itpr_loss: 1664.6958 - val_lit_loss: 820.2040 - val_prec_loss: 0.6964 - val_princ_loss: 981.6641 - val_psy_loss: 1828.6067 - val_rule_loss: 1194.6223 - val_syst_loss: 0.4909 - val_tele_loss: 876.0426 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.4568 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.5556 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 1.0000 - val_tele_accuracy: 0.9877\n",
            "Epoch 127/200\n",
            "6/6 [==============================] - 21s 4s/step - loss: 8.9093 - aut_loss: 0.3176 - class_loss: 0.6826 - itpr_loss: 0.5842 - lit_loss: 0.4117 - prec_loss: 0.7223 - princ_loss: 0.4056 - psy_loss: 1.1281 - rule_loss: 0.5594 - syst_loss: 0.5274 - tele_loss: 0.3744 - aut_accuracy: 0.9720 - class_accuracy: 0.5714 - itpr_accuracy: 0.7609 - lit_accuracy: 0.9845 - prec_accuracy: 0.4814 - princ_accuracy: 0.8944 - psy_accuracy: 0.0590 - rule_accuracy: 0.8012 - syst_accuracy: 0.8758 - tele_accuracy: 0.9907 - val_loss: 13015.9365 - val_aut_loss: 3431.1960 - val_class_loss: 0.6773 - val_itpr_loss: 2232.1792 - val_lit_loss: 1478.7607 - val_prec_loss: 0.6989 - val_princ_loss: 883.7469 - val_psy_loss: 2382.9700 - val_rule_loss: 1668.8488 - val_syst_loss: 0.4867 - val_tele_loss: 933.1759 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.4691 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.5432 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 1.0000 - val_tele_accuracy: 0.9877\n",
            "Epoch 128/200\n",
            "6/6 [==============================] - 21s 4s/step - loss: 8.8902 - aut_loss: 0.3036 - class_loss: 0.6721 - itpr_loss: 0.5936 - lit_loss: 0.4074 - prec_loss: 0.7023 - princ_loss: 0.4128 - psy_loss: 1.1181 - rule_loss: 0.5660 - syst_loss: 0.5260 - tele_loss: 0.3925 - aut_accuracy: 0.9720 - class_accuracy: 0.5621 - itpr_accuracy: 0.7609 - lit_accuracy: 0.9845 - prec_accuracy: 0.5062 - princ_accuracy: 0.8882 - psy_accuracy: 0.0807 - rule_accuracy: 0.7981 - syst_accuracy: 0.8696 - tele_accuracy: 0.9596 - val_loss: 10416.8096 - val_aut_loss: 2827.6868 - val_class_loss: 0.6747 - val_itpr_loss: 1816.6798 - val_lit_loss: 1333.9396 - val_prec_loss: 0.7015 - val_princ_loss: 516.5428 - val_psy_loss: 1996.3079 - val_rule_loss: 1353.2705 - val_syst_loss: 0.4827 - val_tele_loss: 567.3280 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.4691 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.5432 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 1.0000 - val_tele_accuracy: 0.9877\n",
            "Epoch 129/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 8.8764 - aut_loss: 0.3131 - class_loss: 0.6634 - itpr_loss: 0.5999 - lit_loss: 0.4107 - prec_loss: 0.7111 - princ_loss: 0.4168 - psy_loss: 1.1086 - rule_loss: 0.5609 - syst_loss: 0.5110 - tele_loss: 0.3852 - aut_accuracy: 0.9720 - class_accuracy: 0.5807 - itpr_accuracy: 0.7516 - lit_accuracy: 0.9845 - prec_accuracy: 0.5373 - princ_accuracy: 0.8913 - psy_accuracy: 0.0807 - rule_accuracy: 0.7919 - syst_accuracy: 0.8665 - tele_accuracy: 0.9814 - val_loss: 13186.6162 - val_aut_loss: 3173.4507 - val_class_loss: 0.6718 - val_itpr_loss: 2263.0359 - val_lit_loss: 1114.4691 - val_prec_loss: 0.7046 - val_princ_loss: 1333.1219 - val_psy_loss: 2485.7559 - val_rule_loss: 1623.2729 - val_syst_loss: 0.4783 - val_tele_loss: 1188.4589 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.4815 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.5309 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 1.0000 - val_tele_accuracy: 0.9877\n",
            "Epoch 130/200\n",
            "6/6 [==============================] - 21s 4s/step - loss: 1097.5320 - aut_loss: 285.3939 - class_loss: 0.6784 - itpr_loss: 207.8050 - lit_loss: 49.9532 - prec_loss: 0.7380 - princ_loss: 155.9711 - psy_loss: 227.0998 - rule_loss: 127.9302 - syst_loss: 0.5005 - tele_loss: 38.2660 - aut_accuracy: 0.9720 - class_accuracy: 0.5435 - itpr_accuracy: 0.7578 - lit_accuracy: 0.9938 - prec_accuracy: 0.4938 - princ_accuracy: 0.8851 - psy_accuracy: 0.0870 - rule_accuracy: 0.7857 - syst_accuracy: 0.8634 - tele_accuracy: 0.9720 - val_loss: 2028.2604 - val_aut_loss: 611.0192 - val_class_loss: 265.8902 - val_itpr_loss: 0.5193 - val_lit_loss: 0.3661 - val_prec_loss: 0.7079 - val_princ_loss: 0.3802 - val_psy_loss: 312.6364 - val_rule_loss: 832.7184 - val_syst_loss: 0.4741 - val_tele_loss: 0.3527 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.4938 - val_itpr_accuracy: 0.8889 - val_lit_accuracy: 1.0000 - val_prec_accuracy: 0.5185 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 1.0000 - val_tele_accuracy: 1.0000\n",
            "Epoch 131/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 8.8340 - aut_loss: 0.3046 - class_loss: 0.6664 - itpr_loss: 0.5872 - lit_loss: 0.3949 - prec_loss: 0.7164 - princ_loss: 0.4148 - psy_loss: 1.1687 - rule_loss: 0.5354 - syst_loss: 0.4864 - tele_loss: 0.3633 - aut_accuracy: 0.9720 - class_accuracy: 0.5621 - itpr_accuracy: 0.7484 - lit_accuracy: 0.9814 - prec_accuracy: 0.5497 - princ_accuracy: 0.8851 - psy_accuracy: 0.0559 - rule_accuracy: 0.8137 - syst_accuracy: 0.8913 - tele_accuracy: 0.9845 - val_loss: 7956.6938 - val_aut_loss: 2146.7622 - val_class_loss: 0.6670 - val_itpr_loss: 1380.6113 - val_lit_loss: 986.6307 - val_prec_loss: 0.7104 - val_princ_loss: 434.5074 - val_psy_loss: 1495.9833 - val_rule_loss: 1031.7749 - val_syst_loss: 0.4709 - val_tele_loss: 475.3797 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.5309 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.5185 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 1.0000 - val_tele_accuracy: 0.9877\n",
            "Epoch 132/200\n",
            "6/6 [==============================] - 21s 4s/step - loss: 8.8752 - aut_loss: 0.3131 - class_loss: 0.6687 - itpr_loss: 0.5841 - lit_loss: 0.3981 - prec_loss: 0.7288 - princ_loss: 0.4112 - psy_loss: 1.1472 - rule_loss: 0.5570 - syst_loss: 0.4963 - tele_loss: 0.3749 - aut_accuracy: 0.9720 - class_accuracy: 0.5280 - itpr_accuracy: 0.7733 - lit_accuracy: 0.9938 - prec_accuracy: 0.5031 - princ_accuracy: 0.8882 - psy_accuracy: 0.0714 - rule_accuracy: 0.8012 - syst_accuracy: 0.8727 - tele_accuracy: 0.9907 - val_loss: 9287.9395 - val_aut_loss: 2489.1108 - val_class_loss: 0.6650 - val_itpr_loss: 1605.3801 - val_lit_loss: 1121.0671 - val_prec_loss: 0.7126 - val_princ_loss: 546.4269 - val_psy_loss: 1728.8821 - val_rule_loss: 1200.4130 - val_syst_loss: 0.4676 - val_tele_loss: 591.6178 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.5432 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.4938 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 1.0000 - val_tele_accuracy: 0.9877\n",
            "Epoch 133/200\n",
            "6/6 [==============================] - 21s 4s/step - loss: 8.8862 - aut_loss: 0.3088 - class_loss: 0.6680 - itpr_loss: 0.5880 - lit_loss: 0.3966 - prec_loss: 0.7434 - princ_loss: 0.4178 - psy_loss: 1.1336 - rule_loss: 0.5647 - syst_loss: 0.4971 - tele_loss: 0.3725 - aut_accuracy: 0.9720 - class_accuracy: 0.5652 - itpr_accuracy: 0.7671 - lit_accuracy: 0.9938 - prec_accuracy: 0.5186 - princ_accuracy: 0.8882 - psy_accuracy: 0.0932 - rule_accuracy: 0.7764 - syst_accuracy: 0.9099 - tele_accuracy: 0.9658 - val_loss: 7455.4883 - val_aut_loss: 2102.6670 - val_class_loss: 0.6627 - val_itpr_loss: 1342.7365 - val_lit_loss: 1118.0924 - val_prec_loss: 0.7159 - val_princ_loss: 154.2503 - val_psy_loss: 1569.6183 - val_rule_loss: 984.1371 - val_syst_loss: 0.4639 - val_tele_loss: 178.9481 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.5432 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.4938 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 1.0000 - val_tele_accuracy: 0.9877\n",
            "Epoch 134/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 8.8416 - aut_loss: 0.3024 - class_loss: 0.6605 - itpr_loss: 0.5951 - lit_loss: 0.3867 - prec_loss: 0.7294 - princ_loss: 0.4064 - psy_loss: 1.1384 - rule_loss: 0.5603 - syst_loss: 0.4771 - tele_loss: 0.3895 - aut_accuracy: 0.9720 - class_accuracy: 0.5839 - itpr_accuracy: 0.7609 - lit_accuracy: 0.9907 - prec_accuracy: 0.4969 - princ_accuracy: 0.8851 - psy_accuracy: 0.0870 - rule_accuracy: 0.7950 - syst_accuracy: 0.9037 - tele_accuracy: 0.9596 - val_loss: 21324.6602 - val_aut_loss: 5762.2974 - val_class_loss: 0.6602 - val_itpr_loss: 3869.6169 - val_lit_loss: 2152.0950 - val_prec_loss: 0.7198 - val_princ_loss: 1501.7805 - val_psy_loss: 3282.9756 - val_rule_loss: 2827.3535 - val_syst_loss: 0.4598 - val_tele_loss: 1923.5062 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.5432 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.4815 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 1.0000 - val_tele_accuracy: 0.9877\n",
            "Epoch 135/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 8.8532 - aut_loss: 0.3051 - class_loss: 0.6785 - itpr_loss: 0.5888 - lit_loss: 0.3993 - prec_loss: 0.6988 - princ_loss: 0.4356 - psy_loss: 1.1063 - rule_loss: 0.5661 - syst_loss: 0.4919 - tele_loss: 0.3869 - aut_accuracy: 0.9720 - class_accuracy: 0.5217 - itpr_accuracy: 0.7733 - lit_accuracy: 0.9814 - prec_accuracy: 0.5280 - princ_accuracy: 0.8851 - psy_accuracy: 0.0994 - rule_accuracy: 0.7578 - syst_accuracy: 0.8944 - tele_accuracy: 0.9720 - val_loss: 195.7185 - val_aut_loss: 20.3146 - val_class_loss: 81.7718 - val_itpr_loss: 0.5134 - val_lit_loss: 0.3487 - val_prec_loss: 0.7241 - val_princ_loss: 0.3816 - val_psy_loss: 1.0895 - val_rule_loss: 74.4716 - val_syst_loss: 12.5592 - val_tele_loss: 0.3481 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.5802 - val_itpr_accuracy: 0.8889 - val_lit_accuracy: 1.0000 - val_prec_accuracy: 0.4691 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.0123 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 0.9877 - val_tele_accuracy: 1.0000\n",
            "Epoch 136/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 8.8473 - aut_loss: 0.2969 - class_loss: 0.6556 - itpr_loss: 0.5921 - lit_loss: 0.3826 - prec_loss: 0.7246 - princ_loss: 0.4102 - psy_loss: 1.1709 - rule_loss: 0.5601 - syst_loss: 0.4842 - tele_loss: 0.3744 - aut_accuracy: 0.9720 - class_accuracy: 0.5932 - itpr_accuracy: 0.7640 - lit_accuracy: 0.9845 - prec_accuracy: 0.5559 - princ_accuracy: 0.8851 - psy_accuracy: 0.0652 - rule_accuracy: 0.7919 - syst_accuracy: 0.8975 - tele_accuracy: 0.9689 - val_loss: 7811.6079 - val_aut_loss: 2093.5991 - val_class_loss: 0.6540 - val_itpr_loss: 1350.5502 - val_lit_loss: 942.3654 - val_prec_loss: 0.7284 - val_princ_loss: 459.4521 - val_psy_loss: 1454.4935 - val_rule_loss: 1009.4575 - val_syst_loss: 0.4518 - val_tele_loss: 496.6602 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.6173 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.4568 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 1.0000 - val_tele_accuracy: 0.9877\n",
            "Epoch 137/200\n",
            "6/6 [==============================] - 21s 4s/step - loss: 11.5330 - aut_loss: 0.7047 - class_loss: 0.6602 - itpr_loss: 0.5781 - lit_loss: 0.6407 - prec_loss: 0.9871 - princ_loss: 0.4195 - psy_loss: 1.1111 - rule_loss: 0.5541 - syst_loss: 2.2960 - tele_loss: 0.3858 - aut_accuracy: 0.9720 - class_accuracy: 0.5994 - itpr_accuracy: 0.7764 - lit_accuracy: 0.9907 - prec_accuracy: 0.4938 - princ_accuracy: 0.8851 - psy_accuracy: 0.0807 - rule_accuracy: 0.7857 - syst_accuracy: 0.8820 - tele_accuracy: 0.9503 - val_loss: 10232.7080 - val_aut_loss: 2722.3716 - val_class_loss: 0.6508 - val_itpr_loss: 1762.6854 - val_lit_loss: 1199.7971 - val_prec_loss: 0.7323 - val_princ_loss: 646.0280 - val_psy_loss: 1889.3615 - val_rule_loss: 1317.5212 - val_syst_loss: 0.4482 - val_tele_loss: 689.9167 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.6296 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.4568 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 1.0000 - val_tele_accuracy: 0.9877\n",
            "Epoch 138/200\n",
            "6/6 [==============================] - 21s 4s/step - loss: 8.8055 - aut_loss: 0.3047 - class_loss: 0.6544 - itpr_loss: 0.5816 - lit_loss: 0.3836 - prec_loss: 0.7592 - princ_loss: 0.4080 - psy_loss: 1.1328 - rule_loss: 0.5488 - syst_loss: 0.4683 - tele_loss: 0.3685 - aut_accuracy: 0.9720 - class_accuracy: 0.5963 - itpr_accuracy: 0.7640 - lit_accuracy: 0.9845 - prec_accuracy: 0.5062 - princ_accuracy: 0.8820 - psy_accuracy: 0.0839 - rule_accuracy: 0.7950 - syst_accuracy: 0.9130 - tele_accuracy: 0.9689 - val_loss: 9264.7764 - val_aut_loss: 2411.5276 - val_class_loss: 0.6475 - val_itpr_loss: 1590.7502 - val_lit_loss: 1019.0786 - val_prec_loss: 0.7361 - val_princ_loss: 667.7264 - val_psy_loss: 1712.8893 - val_rule_loss: 1179.2587 - val_syst_loss: 0.4449 - val_tele_loss: 678.5220 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.6296 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.4568 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 1.0000 - val_tele_accuracy: 0.9877\n",
            "Epoch 139/200\n",
            "6/6 [==============================] - 21s 4s/step - loss: 8.7642 - aut_loss: 0.2985 - class_loss: 0.6468 - itpr_loss: 0.5846 - lit_loss: 0.3633 - prec_loss: 0.7415 - princ_loss: 0.4181 - psy_loss: 1.1332 - rule_loss: 0.5375 - syst_loss: 0.4691 - tele_loss: 0.3761 - aut_accuracy: 0.9720 - class_accuracy: 0.6335 - itpr_accuracy: 0.7764 - lit_accuracy: 0.9907 - prec_accuracy: 0.5093 - princ_accuracy: 0.8851 - psy_accuracy: 0.0839 - rule_accuracy: 0.8043 - syst_accuracy: 0.9130 - tele_accuracy: 0.9627 - val_loss: 8575.3213 - val_aut_loss: 2213.0579 - val_class_loss: 0.6446 - val_itpr_loss: 1465.3490 - val_lit_loss: 905.0460 - val_prec_loss: 0.7396 - val_princ_loss: 663.5785 - val_psy_loss: 1564.6592 - val_rule_loss: 1088.3732 - val_syst_loss: 0.4418 - val_tele_loss: 670.2363 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.6543 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.4444 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 1.0000 - val_tele_accuracy: 0.9877\n",
            "Epoch 140/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 8.7352 - aut_loss: 0.3020 - class_loss: 0.6391 - itpr_loss: 0.6039 - lit_loss: 0.3734 - prec_loss: 0.7128 - princ_loss: 0.4074 - psy_loss: 1.1159 - rule_loss: 0.5528 - syst_loss: 0.4700 - tele_loss: 0.3623 - aut_accuracy: 0.9720 - class_accuracy: 0.6398 - itpr_accuracy: 0.7609 - lit_accuracy: 0.9814 - prec_accuracy: 0.4876 - princ_accuracy: 0.8820 - psy_accuracy: 0.0901 - rule_accuracy: 0.7857 - syst_accuracy: 0.9224 - tele_accuracy: 0.9783 - val_loss: 25216.6465 - val_aut_loss: 6605.6382 - val_class_loss: 0.6413 - val_itpr_loss: 4736.6958 - val_lit_loss: 2018.0820 - val_prec_loss: 0.7446 - val_princ_loss: 2156.3550 - val_psy_loss: 3775.1548 - val_rule_loss: 3313.7021 - val_syst_loss: 0.4380 - val_tele_loss: 2605.9985 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.6790 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.4444 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 1.0000 - val_tele_accuracy: 0.9877\n",
            "Epoch 141/200\n",
            "6/6 [==============================] - 21s 4s/step - loss: 508.9805 - aut_loss: 0.2930 - class_loss: 0.6382 - itpr_loss: 0.5854 - lit_loss: 96.2758 - prec_loss: 0.7240 - princ_loss: 73.9824 - psy_loss: 133.1874 - rule_loss: 123.7065 - syst_loss: 0.4599 - tele_loss: 75.9324 - aut_accuracy: 0.9720 - class_accuracy: 0.6584 - itpr_accuracy: 0.7702 - lit_accuracy: 0.9876 - prec_accuracy: 0.5248 - princ_accuracy: 0.8789 - psy_accuracy: 0.1056 - rule_accuracy: 0.7981 - syst_accuracy: 0.9348 - tele_accuracy: 0.9658 - val_loss: 9727.5098 - val_aut_loss: 2627.4304 - val_class_loss: 0.6381 - val_itpr_loss: 1689.2716 - val_lit_loss: 1206.4507 - val_prec_loss: 0.7496 - val_princ_loss: 529.5969 - val_psy_loss: 1830.3268 - val_rule_loss: 1261.6798 - val_syst_loss: 0.4344 - val_tele_loss: 577.7367 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.7037 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.4444 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 1.0000 - val_tele_accuracy: 0.9877\n",
            "Epoch 142/200\n",
            "6/6 [==============================] - 21s 4s/step - loss: 8.8195 - aut_loss: 0.2899 - class_loss: 0.6365 - itpr_loss: 0.5985 - lit_loss: 0.3642 - prec_loss: 0.7688 - princ_loss: 0.4370 - psy_loss: 1.1481 - rule_loss: 0.5594 - syst_loss: 0.4643 - tele_loss: 0.3572 - aut_accuracy: 0.9720 - class_accuracy: 0.6398 - itpr_accuracy: 0.7640 - lit_accuracy: 0.9876 - prec_accuracy: 0.5000 - princ_accuracy: 0.8727 - psy_accuracy: 0.0901 - rule_accuracy: 0.7764 - syst_accuracy: 0.9193 - tele_accuracy: 0.9752 - val_loss: 7411321.0000 - val_aut_loss: 75569.1562 - val_class_loss: 2711457.2500 - val_itpr_loss: 0.5064 - val_lit_loss: 0.3242 - val_prec_loss: 0.7533 - val_princ_loss: 0.3867 - val_psy_loss: 1.0897 - val_rule_loss: 3200708.2500 - val_syst_loss: 1423579.8750 - val_tele_loss: 0.3396 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.7037 - val_itpr_accuracy: 0.8889 - val_lit_accuracy: 1.0000 - val_prec_accuracy: 0.4321 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.0123 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 0.9877 - val_tele_accuracy: 1.0000\n",
            "Epoch 143/200\n",
            "6/6 [==============================] - 21s 4s/step - loss: 8.7465 - aut_loss: 0.2940 - class_loss: 0.6302 - itpr_loss: 0.5800 - lit_loss: 0.3533 - prec_loss: 0.7417 - princ_loss: 0.4242 - psy_loss: 1.1594 - rule_loss: 0.5275 - syst_loss: 0.4759 - tele_loss: 0.3646 - aut_accuracy: 0.9720 - class_accuracy: 0.6739 - itpr_accuracy: 0.7764 - lit_accuracy: 0.9938 - prec_accuracy: 0.4752 - princ_accuracy: 0.8789 - psy_accuracy: 0.0870 - rule_accuracy: 0.8106 - syst_accuracy: 0.9099 - tele_accuracy: 0.9783 - val_loss: 65189.7773 - val_aut_loss: 11651.7363 - val_class_loss: 23830.4590 - val_itpr_loss: 0.5055 - val_lit_loss: 0.3206 - val_prec_loss: 0.7585 - val_princ_loss: 0.3876 - val_psy_loss: 1.0903 - val_rule_loss: 24081.5176 - val_syst_loss: 5619.4619 - val_tele_loss: 0.3383 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.7037 - val_itpr_accuracy: 0.8889 - val_lit_accuracy: 1.0000 - val_prec_accuracy: 0.4321 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.0123 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 0.9877 - val_tele_accuracy: 1.0000\n",
            "Epoch 144/200\n",
            "6/6 [==============================] - 21s 4s/step - loss: 8.7745 - aut_loss: 0.2939 - class_loss: 0.6482 - itpr_loss: 0.6074 - lit_loss: 0.3709 - prec_loss: 0.7411 - princ_loss: 0.4267 - psy_loss: 1.0924 - rule_loss: 0.5466 - syst_loss: 0.4729 - tele_loss: 0.3788 - aut_accuracy: 0.9720 - class_accuracy: 0.6087 - itpr_accuracy: 0.7516 - lit_accuracy: 0.9907 - prec_accuracy: 0.4969 - princ_accuracy: 0.8820 - psy_accuracy: 0.1149 - rule_accuracy: 0.7857 - syst_accuracy: 0.9006 - tele_accuracy: 0.9627 - val_loss: 3270.4814 - val_aut_loss: 888.3250 - val_class_loss: 0.6285 - val_itpr_loss: 570.2356 - val_lit_loss: 410.0779 - val_prec_loss: 0.7627 - val_princ_loss: 168.5900 - val_psy_loss: 626.4467 - val_rule_loss: 423.6989 - val_syst_loss: 0.4252 - val_tele_loss: 178.0954 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.7407 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.4321 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 1.0000 - val_tele_accuracy: 0.9877\n",
            "Epoch 145/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 706.2561 - aut_loss: 131.2487 - class_loss: 17.8311 - itpr_loss: 0.6080 - lit_loss: 0.3524 - prec_loss: 0.7342 - princ_loss: 70.9077 - psy_loss: 337.0819 - rule_loss: 0.5484 - syst_loss: 143.3850 - tele_loss: 0.3630 - aut_accuracy: 0.9720 - class_accuracy: 0.6460 - itpr_accuracy: 0.7609 - lit_accuracy: 0.9876 - prec_accuracy: 0.4783 - princ_accuracy: 0.8789 - psy_accuracy: 0.0901 - rule_accuracy: 0.8137 - syst_accuracy: 0.9099 - tele_accuracy: 0.9534 - val_loss: 9622.5039 - val_aut_loss: 2484.0105 - val_class_loss: 0.6253 - val_itpr_loss: 1646.5725 - val_lit_loss: 1016.3713 - val_prec_loss: 0.7661 - val_princ_loss: 742.2099 - val_psy_loss: 1759.4349 - val_rule_loss: 1221.6785 - val_syst_loss: 0.4229 - val_tele_loss: 747.2161 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.7407 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.4321 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 1.0000 - val_tele_accuracy: 0.9877\n",
            "Epoch 146/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 8.7454 - aut_loss: 0.2944 - class_loss: 0.6351 - itpr_loss: 0.5931 - lit_loss: 0.3547 - prec_loss: 0.7200 - princ_loss: 0.4298 - psy_loss: 1.1489 - rule_loss: 0.5446 - syst_loss: 0.4655 - tele_loss: 0.3636 - aut_accuracy: 0.9720 - class_accuracy: 0.6273 - itpr_accuracy: 0.7453 - lit_accuracy: 0.9814 - prec_accuracy: 0.5124 - princ_accuracy: 0.8665 - psy_accuracy: 0.1056 - rule_accuracy: 0.7888 - syst_accuracy: 0.9099 - tele_accuracy: 0.9658 - val_loss: 34076.4570 - val_aut_loss: 9372.7549 - val_class_loss: 0.6219 - val_itpr_loss: 6215.1411 - val_lit_loss: 3762.3086 - val_prec_loss: 0.7703 - val_princ_loss: 2037.2267 - val_psy_loss: 5455.1499 - val_rule_loss: 4509.1338 - val_syst_loss: 0.4201 - val_tele_loss: 2719.7383 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.7407 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.4321 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 1.0000 - val_tele_accuracy: 0.9877\n",
            "Epoch 147/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 8.6692 - aut_loss: 0.2801 - class_loss: 0.6165 - itpr_loss: 0.5848 - lit_loss: 0.3498 - prec_loss: 0.7399 - princ_loss: 0.4191 - psy_loss: 1.1372 - rule_loss: 0.5437 - syst_loss: 0.4418 - tele_loss: 0.3607 - aut_accuracy: 0.9720 - class_accuracy: 0.6988 - itpr_accuracy: 0.7764 - lit_accuracy: 0.9845 - prec_accuracy: 0.5000 - princ_accuracy: 0.8851 - psy_accuracy: 0.0870 - rule_accuracy: 0.8043 - syst_accuracy: 0.9255 - tele_accuracy: 0.9689 - val_loss: 10684911.0000 - val_aut_loss: 406252.4375 - val_class_loss: 5262085.5000 - val_itpr_loss: 671115.5000 - val_lit_loss: 0.3103 - val_prec_loss: 0.7724 - val_princ_loss: 0.3900 - val_psy_loss: 1.0888 - val_rule_loss: 4303385.5000 - val_syst_loss: 42065.1094 - val_tele_loss: 0.3338 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.7407 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 1.0000 - val_prec_accuracy: 0.4321 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.0123 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 0.9877 - val_tele_accuracy: 1.0000\n",
            "Epoch 148/200\n",
            "6/6 [==============================] - 21s 4s/step - loss: 8.7285 - aut_loss: 0.2736 - class_loss: 0.6333 - itpr_loss: 0.5999 - lit_loss: 0.3419 - prec_loss: 0.7324 - princ_loss: 0.4372 - psy_loss: 1.1045 - rule_loss: 0.5786 - syst_loss: 0.4711 - tele_loss: 0.3605 - aut_accuracy: 0.9720 - class_accuracy: 0.6584 - itpr_accuracy: 0.7640 - lit_accuracy: 0.9876 - prec_accuracy: 0.5311 - princ_accuracy: 0.8634 - psy_accuracy: 0.1025 - rule_accuracy: 0.7981 - syst_accuracy: 0.9006 - tele_accuracy: 0.9752 - val_loss: 8997224.0000 - val_aut_loss: 701427.1875 - val_class_loss: 4197398.0000 - val_itpr_loss: 0.5033 - val_lit_loss: 0.3087 - val_prec_loss: 0.7745 - val_princ_loss: 0.3904 - val_psy_loss: 1.0873 - val_rule_loss: 3777787.7500 - val_syst_loss: 320604.0312 - val_tele_loss: 0.3327 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.7407 - val_itpr_accuracy: 0.8889 - val_lit_accuracy: 1.0000 - val_prec_accuracy: 0.4321 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.0123 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 0.9877 - val_tele_accuracy: 1.0000\n",
            "Epoch 149/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 8.6699 - aut_loss: 0.2830 - class_loss: 0.6153 - itpr_loss: 0.5868 - lit_loss: 0.3432 - prec_loss: 0.7481 - princ_loss: 0.4284 - psy_loss: 1.1071 - rule_loss: 0.5438 - syst_loss: 0.4527 - tele_loss: 0.3658 - aut_accuracy: 0.9720 - class_accuracy: 0.7143 - itpr_accuracy: 0.7671 - lit_accuracy: 0.9752 - prec_accuracy: 0.5155 - princ_accuracy: 0.8727 - psy_accuracy: 0.0839 - rule_accuracy: 0.8075 - syst_accuracy: 0.9099 - tele_accuracy: 0.9658 - val_loss: 9660.8662 - val_aut_loss: 2421.3203 - val_class_loss: 0.6144 - val_itpr_loss: 1653.5342 - val_lit_loss: 929.5359 - val_prec_loss: 0.7767 - val_princ_loss: 849.5366 - val_psy_loss: 1785.2095 - val_rule_loss: 1211.0601 - val_syst_loss: 0.4155 - val_tele_loss: 805.6675 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.7531 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.4198 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 1.0000 - val_tele_accuracy: 0.9877\n",
            "Epoch 150/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 8.7032 - aut_loss: 0.2881 - class_loss: 0.6061 - itpr_loss: 0.6057 - lit_loss: 0.3377 - prec_loss: 0.7471 - princ_loss: 0.4224 - psy_loss: 1.1603 - rule_loss: 0.5404 - syst_loss: 0.4396 - tele_loss: 0.3603 - aut_accuracy: 0.9720 - class_accuracy: 0.7050 - itpr_accuracy: 0.7733 - lit_accuracy: 0.9876 - prec_accuracy: 0.4969 - princ_accuracy: 0.8851 - psy_accuracy: 0.1056 - rule_accuracy: 0.7919 - syst_accuracy: 0.9286 - tele_accuracy: 0.9689 - val_loss: 9520.8945 - val_aut_loss: 2294.8677 - val_class_loss: 0.6127 - val_itpr_loss: 1632.8806 - val_lit_loss: 805.4918 - val_prec_loss: 0.7773 - val_princ_loss: 961.1852 - val_psy_loss: 1801.0656 - val_rule_loss: 1172.9781 - val_syst_loss: 0.4145 - val_tele_loss: 847.4258 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.7654 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.4074 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 1.0000 - val_tele_accuracy: 0.9877\n",
            "Epoch 151/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 8.6656 - aut_loss: 0.2876 - class_loss: 0.6330 - itpr_loss: 0.5820 - lit_loss: 0.3465 - prec_loss: 0.7500 - princ_loss: 0.3960 - psy_loss: 1.1530 - rule_loss: 0.5199 - syst_loss: 0.4505 - tele_loss: 0.3516 - aut_accuracy: 0.9720 - class_accuracy: 0.6460 - itpr_accuracy: 0.7857 - lit_accuracy: 0.9876 - prec_accuracy: 0.4938 - princ_accuracy: 0.8727 - psy_accuracy: 0.1211 - rule_accuracy: 0.7857 - syst_accuracy: 0.9317 - tele_accuracy: 0.9845 - val_loss: 12009101.0000 - val_aut_loss: 201762.3438 - val_class_loss: 6030448.0000 - val_itpr_loss: 1052688.2500 - val_lit_loss: 0.3063 - val_prec_loss: 0.7777 - val_princ_loss: 0.3893 - val_psy_loss: 1.0823 - val_rule_loss: 3630274.0000 - val_syst_loss: 1093922.7500 - val_tele_loss: 0.3296 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.7531 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 1.0000 - val_prec_accuracy: 0.4074 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.0123 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 0.9877 - val_tele_accuracy: 1.0000\n",
            "Epoch 152/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 8.6696 - aut_loss: 0.2804 - class_loss: 0.6144 - itpr_loss: 0.5916 - lit_loss: 0.3497 - prec_loss: 0.7492 - princ_loss: 0.4309 - psy_loss: 1.0938 - rule_loss: 0.5656 - syst_loss: 0.4433 - tele_loss: 0.3554 - aut_accuracy: 0.9720 - class_accuracy: 0.6770 - itpr_accuracy: 0.7671 - lit_accuracy: 0.9845 - prec_accuracy: 0.5093 - princ_accuracy: 0.8789 - psy_accuracy: 0.1025 - rule_accuracy: 0.7888 - syst_accuracy: 0.9379 - tele_accuracy: 0.9720 - val_loss: 11004367.0000 - val_aut_loss: 498151.5938 - val_class_loss: 5394916.0000 - val_itpr_loss: 473928.3438 - val_lit_loss: 0.3064 - val_prec_loss: 0.7772 - val_princ_loss: 0.3881 - val_psy_loss: 1.0790 - val_rule_loss: 4516746.5000 - val_syst_loss: 120617.3984 - val_tele_loss: 0.3285 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.7531 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 1.0000 - val_prec_accuracy: 0.4074 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.0123 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 0.9877 - val_tele_accuracy: 1.0000\n",
            "Epoch 153/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 126536.0469 - aut_loss: 0.2766 - class_loss: 45717.9219 - itpr_loss: 0.5746 - lit_loss: 0.3346 - prec_loss: 0.7438 - princ_loss: 48322.4180 - psy_loss: 1.0603 - rule_loss: 21840.6758 - syst_loss: 10648.4775 - tele_loss: 0.3605 - aut_accuracy: 0.9720 - class_accuracy: 0.6491 - itpr_accuracy: 0.7516 - lit_accuracy: 0.9938 - prec_accuracy: 0.5000 - princ_accuracy: 0.8882 - psy_accuracy: 0.1211 - rule_accuracy: 0.8106 - syst_accuracy: 0.9224 - tele_accuracy: 0.9658 - val_loss: 2718.8267 - val_aut_loss: 831.2530 - val_class_loss: 292.1497 - val_itpr_loss: 582.8362 - val_lit_loss: 395.2767 - val_prec_loss: 0.7775 - val_princ_loss: 19.2529 - val_psy_loss: 1.0763 - val_rule_loss: 0.6252 - val_syst_loss: 592.0564 - val_tele_loss: 0.3273 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.7531 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.4074 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0123 - val_rule_accuracy: 0.7778 - val_syst_accuracy: 0.9877 - val_tele_accuracy: 1.0000\n",
            "Epoch 154/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 1284.9679 - aut_loss: 0.2761 - class_loss: 0.6155 - itpr_loss: 462.7798 - lit_loss: 283.8345 - prec_loss: 2.3865 - princ_loss: 0.4293 - psy_loss: 63.3330 - rule_loss: 271.8887 - syst_loss: 0.4293 - tele_loss: 195.7998 - aut_accuracy: 0.9720 - class_accuracy: 0.7143 - itpr_accuracy: 0.7640 - lit_accuracy: 0.9969 - prec_accuracy: 0.5031 - princ_accuracy: 0.8882 - psy_accuracy: 0.1149 - rule_accuracy: 0.7919 - syst_accuracy: 0.9565 - tele_accuracy: 0.9752 - val_loss: 7195.1553 - val_aut_loss: 2447.9084 - val_class_loss: 2068.4841 - val_itpr_loss: 0.5041 - val_lit_loss: 0.3044 - val_prec_loss: 0.7805 - val_princ_loss: 0.3875 - val_psy_loss: 1.0759 - val_rule_loss: 2208.0076 - val_syst_loss: 464.1803 - val_tele_loss: 0.3258 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.7531 - val_itpr_accuracy: 0.8889 - val_lit_accuracy: 1.0000 - val_prec_accuracy: 0.4074 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.0123 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 0.9877 - val_tele_accuracy: 1.0000\n",
            "Epoch 155/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 8.6113 - aut_loss: 0.2868 - class_loss: 0.6105 - itpr_loss: 0.5780 - lit_loss: 0.3383 - prec_loss: 0.7365 - princ_loss: 0.4208 - psy_loss: 1.1157 - rule_loss: 0.5402 - syst_loss: 0.4336 - tele_loss: 0.3555 - aut_accuracy: 0.9689 - class_accuracy: 0.6677 - itpr_accuracy: 0.7764 - lit_accuracy: 0.9907 - prec_accuracy: 0.5124 - princ_accuracy: 0.8882 - psy_accuracy: 0.1087 - rule_accuracy: 0.7981 - syst_accuracy: 0.9596 - tele_accuracy: 0.9907 - val_loss: 9500.1865 - val_aut_loss: 2529.9048 - val_class_loss: 0.6026 - val_itpr_loss: 1640.5837 - val_lit_loss: 1114.8307 - val_prec_loss: 0.7821 - val_princ_loss: 595.0168 - val_psy_loss: 1761.0233 - val_rule_loss: 1223.5720 - val_syst_loss: 0.4087 - val_tele_loss: 630.2664 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.7654 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.4074 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 1.0000 - val_tele_accuracy: 0.9877\n",
            "Epoch 156/200\n",
            "6/6 [==============================] - 21s 4s/step - loss: 8.6044 - aut_loss: 0.2732 - class_loss: 0.6191 - itpr_loss: 0.5654 - lit_loss: 0.3361 - prec_loss: 0.7729 - princ_loss: 0.4019 - psy_loss: 1.0711 - rule_loss: 0.5551 - syst_loss: 0.4459 - tele_loss: 0.3683 - aut_accuracy: 0.9720 - class_accuracy: 0.7205 - itpr_accuracy: 0.7640 - lit_accuracy: 0.9938 - prec_accuracy: 0.5031 - princ_accuracy: 0.8820 - psy_accuracy: 0.1242 - rule_accuracy: 0.8106 - syst_accuracy: 0.9193 - tele_accuracy: 0.9627 - val_loss: 16.2116 - val_aut_loss: 3.0659 - val_class_loss: 0.5991 - val_itpr_loss: 0.5040 - val_lit_loss: 1.2670 - val_prec_loss: 0.7858 - val_princ_loss: 0.6937 - val_psy_loss: 3.6992 - val_rule_loss: 0.6305 - val_syst_loss: 1.4481 - val_tele_loss: 0.3228 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.7654 - val_itpr_accuracy: 0.8889 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.4074 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7778 - val_syst_accuracy: 0.9877 - val_tele_accuracy: 1.0000\n",
            "Epoch 157/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 8.5754 - aut_loss: 0.2713 - class_loss: 0.5960 - itpr_loss: 0.6141 - lit_loss: 0.3422 - prec_loss: 0.7589 - princ_loss: 0.3936 - psy_loss: 1.0877 - rule_loss: 0.5379 - syst_loss: 0.4346 - tele_loss: 0.3436 - aut_accuracy: 0.9720 - class_accuracy: 0.7578 - itpr_accuracy: 0.7640 - lit_accuracy: 0.9969 - prec_accuracy: 0.4783 - princ_accuracy: 0.8851 - psy_accuracy: 0.1180 - rule_accuracy: 0.8199 - syst_accuracy: 0.9379 - tele_accuracy: 0.9783 - val_loss: 51.9239 - val_aut_loss: 6.8601 - val_class_loss: 0.9212 - val_itpr_loss: 0.5037 - val_lit_loss: 0.2993 - val_prec_loss: 0.7888 - val_princ_loss: 0.3867 - val_psy_loss: 7.7568 - val_rule_loss: 23.4292 - val_syst_loss: 7.4616 - val_tele_loss: 0.3212 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.7531 - val_itpr_accuracy: 0.8889 - val_lit_accuracy: 1.0000 - val_prec_accuracy: 0.4074 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 0.9877 - val_tele_accuracy: 1.0000\n",
            "Epoch 158/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 8.5508 - aut_loss: 0.2693 - class_loss: 0.5960 - itpr_loss: 0.5913 - lit_loss: 0.3272 - prec_loss: 0.7576 - princ_loss: 0.4167 - psy_loss: 1.1109 - rule_loss: 0.5205 - syst_loss: 0.4253 - tele_loss: 0.3406 - aut_accuracy: 0.9720 - class_accuracy: 0.7267 - itpr_accuracy: 0.7733 - lit_accuracy: 0.9938 - prec_accuracy: 0.4752 - princ_accuracy: 0.8820 - psy_accuracy: 0.1242 - rule_accuracy: 0.8137 - syst_accuracy: 0.9379 - tele_accuracy: 0.9783 - val_loss: 393.5157 - val_aut_loss: 88.1739 - val_class_loss: 0.5918 - val_itpr_loss: 37.1576 - val_lit_loss: 64.3590 - val_prec_loss: 83.4527 - val_princ_loss: 0.3865 - val_psy_loss: 114.8420 - val_rule_loss: 0.6348 - val_syst_loss: 0.4025 - val_tele_loss: 0.3195 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.7778 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.3951 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7778 - val_syst_accuracy: 1.0000 - val_tele_accuracy: 1.0000\n",
            "Epoch 159/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 8.5544 - aut_loss: 0.2728 - class_loss: 0.5958 - itpr_loss: 0.5949 - lit_loss: 0.3274 - prec_loss: 0.7476 - princ_loss: 0.4310 - psy_loss: 1.0869 - rule_loss: 0.5390 - syst_loss: 0.4187 - tele_loss: 0.3449 - aut_accuracy: 0.9720 - class_accuracy: 0.7174 - itpr_accuracy: 0.7733 - lit_accuracy: 0.9876 - prec_accuracy: 0.5186 - princ_accuracy: 0.8851 - psy_accuracy: 0.1335 - rule_accuracy: 0.8137 - syst_accuracy: 0.9379 - tele_accuracy: 0.9814 - val_loss: 6171.9570 - val_aut_loss: 1721.7401 - val_class_loss: 0.5882 - val_itpr_loss: 1096.1577 - val_lit_loss: 863.2696 - val_prec_loss: 0.7973 - val_princ_loss: 205.4085 - val_psy_loss: 1254.6836 - val_rule_loss: 809.4560 - val_syst_loss: 0.4000 - val_tele_loss: 216.2604 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.7901 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.4074 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 1.0000 - val_tele_accuracy: 0.9877\n",
            "Epoch 160/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 8.5848 - aut_loss: 0.2698 - class_loss: 0.5962 - itpr_loss: 0.5903 - lit_loss: 0.3334 - prec_loss: 0.7397 - princ_loss: 0.4324 - psy_loss: 1.0895 - rule_loss: 0.5557 - syst_loss: 0.4347 - tele_loss: 0.3477 - aut_accuracy: 0.9720 - class_accuracy: 0.7205 - itpr_accuracy: 0.7422 - lit_accuracy: 0.9938 - prec_accuracy: 0.5217 - princ_accuracy: 0.8727 - psy_accuracy: 0.1429 - rule_accuracy: 0.8043 - syst_accuracy: 0.9006 - tele_accuracy: 0.9689 - val_loss: 126669.8906 - val_aut_loss: 41984.1562 - val_class_loss: 0.5847 - val_itpr_loss: 37597.7891 - val_lit_loss: 0.2924 - val_prec_loss: 23965.6816 - val_princ_loss: 0.3861 - val_psy_loss: 18769.7109 - val_rule_loss: 0.6404 - val_syst_loss: 4347.1333 - val_tele_loss: 0.3156 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.8025 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 1.0000 - val_prec_accuracy: 0.3951 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7778 - val_syst_accuracy: 0.9877 - val_tele_accuracy: 1.0000\n",
            "Epoch 161/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 626.0422 - aut_loss: 0.2719 - class_loss: 181.8330 - itpr_loss: 0.5737 - lit_loss: 0.3193 - prec_loss: 0.7595 - princ_loss: 260.4637 - psy_loss: 1.1076 - rule_loss: 160.1858 - syst_loss: 16.9711 - tele_loss: 0.3614 - aut_accuracy: 0.9689 - class_accuracy: 0.7453 - itpr_accuracy: 0.7795 - lit_accuracy: 0.9876 - prec_accuracy: 0.5155 - princ_accuracy: 0.8789 - psy_accuracy: 0.1149 - rule_accuracy: 0.8137 - syst_accuracy: 0.9193 - tele_accuracy: 0.9720 - val_loss: 1249.7010 - val_aut_loss: 43.6416 - val_class_loss: 616.8863 - val_itpr_loss: 48.9444 - val_lit_loss: 0.2917 - val_prec_loss: 0.8049 - val_princ_loss: 0.3852 - val_psy_loss: 1.0716 - val_rule_loss: 511.4536 - val_syst_loss: 22.7123 - val_tele_loss: 0.3141 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.8025 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 1.0000 - val_prec_accuracy: 0.4074 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.0123 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 0.9877 - val_tele_accuracy: 1.0000\n",
            "Epoch 162/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 8.5823 - aut_loss: 0.2585 - class_loss: 0.5914 - itpr_loss: 0.5823 - lit_loss: 0.3217 - prec_loss: 0.7935 - princ_loss: 0.4356 - psy_loss: 1.0817 - rule_loss: 0.5418 - syst_loss: 0.4308 - tele_loss: 0.3497 - aut_accuracy: 0.9720 - class_accuracy: 0.7205 - itpr_accuracy: 0.7391 - lit_accuracy: 0.9876 - prec_accuracy: 0.4596 - princ_accuracy: 0.8727 - psy_accuracy: 0.1460 - rule_accuracy: 0.8137 - syst_accuracy: 0.9255 - tele_accuracy: 0.9752 - val_loss: 55584.5938 - val_aut_loss: 13788.4766 - val_class_loss: 0.5807 - val_itpr_loss: 16859.8438 - val_lit_loss: 4252.7153 - val_prec_loss: 0.8054 - val_princ_loss: 0.3842 - val_psy_loss: 4677.4014 - val_rule_loss: 9254.3477 - val_syst_loss: 0.3950 - val_tele_loss: 6746.4482 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.8272 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.4074 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 1.0000 - val_tele_accuracy: 0.9877\n",
            "Epoch 163/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 130783.8750 - aut_loss: 4985.6626 - class_loss: 61493.1133 - itpr_loss: 0.5887 - lit_loss: 0.3412 - prec_loss: 0.7416 - princ_loss: 0.4218 - psy_loss: 1.0564 - rule_loss: 46100.2969 - syst_loss: 18198.1270 - tele_loss: 0.3483 - aut_accuracy: 0.9720 - class_accuracy: 0.7329 - itpr_accuracy: 0.7733 - lit_accuracy: 0.9845 - prec_accuracy: 0.4938 - princ_accuracy: 0.8820 - psy_accuracy: 0.1460 - rule_accuracy: 0.8012 - syst_accuracy: 0.9099 - tele_accuracy: 0.9720 - val_loss: 8.5041 - val_aut_loss: 0.2996 - val_class_loss: 0.5821 - val_itpr_loss: 0.5061 - val_lit_loss: 0.2918 - val_prec_loss: 0.8069 - val_princ_loss: 0.3888 - val_psy_loss: 1.0800 - val_rule_loss: 0.6437 - val_syst_loss: 0.3963 - val_tele_loss: 0.3136 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.8395 - val_itpr_accuracy: 0.8889 - val_lit_accuracy: 1.0000 - val_prec_accuracy: 0.4074 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7778 - val_syst_accuracy: 1.0000 - val_tele_accuracy: 1.0000\n",
            "Epoch 164/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 8.5235 - aut_loss: 0.2604 - class_loss: 0.5868 - itpr_loss: 0.6043 - lit_loss: 0.3214 - prec_loss: 0.7407 - princ_loss: 0.4014 - psy_loss: 1.1061 - rule_loss: 0.5362 - syst_loss: 0.4334 - tele_loss: 0.3374 - aut_accuracy: 0.9720 - class_accuracy: 0.7360 - itpr_accuracy: 0.7640 - lit_accuracy: 0.9938 - prec_accuracy: 0.5124 - princ_accuracy: 0.8882 - psy_accuracy: 0.1273 - rule_accuracy: 0.8230 - syst_accuracy: 0.9441 - tele_accuracy: 0.9907 - val_loss: 5887010.0000 - val_aut_loss: 0.2661 - val_class_loss: 1690367.6250 - val_itpr_loss: 448162.9062 - val_lit_loss: 0.2898 - val_prec_loss: 0.8099 - val_princ_loss: 0.3833 - val_psy_loss: 1.0644 - val_rule_loss: 2945050.7500 - val_syst_loss: 803422.6875 - val_tele_loss: 0.3102 - val_aut_accuracy: 0.9630 - val_class_accuracy: 0.8272 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 1.0000 - val_prec_accuracy: 0.4074 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.0123 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 0.9877 - val_tele_accuracy: 1.0000\n",
            "Epoch 165/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 8.5965 - aut_loss: 0.2663 - class_loss: 0.5889 - itpr_loss: 0.6113 - lit_loss: 0.3364 - prec_loss: 0.7731 - princ_loss: 0.4278 - psy_loss: 1.0476 - rule_loss: 0.5509 - syst_loss: 0.4526 - tele_loss: 0.3463 - aut_accuracy: 0.9720 - class_accuracy: 0.7236 - itpr_accuracy: 0.7391 - lit_accuracy: 0.9876 - prec_accuracy: 0.4627 - princ_accuracy: 0.8758 - psy_accuracy: 0.1366 - rule_accuracy: 0.7919 - syst_accuracy: 0.9193 - tele_accuracy: 0.9689 - val_loss: 15719.4248 - val_aut_loss: 4184.4697 - val_class_loss: 0.5753 - val_itpr_loss: 4768.1450 - val_lit_loss: 648.8715 - val_prec_loss: 0.8078 - val_princ_loss: 0.3818 - val_psy_loss: 1990.6349 - val_rule_loss: 2499.0737 - val_syst_loss: 0.3923 - val_tele_loss: 1622.8784 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.8519 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.4074 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 1.0000 - val_tele_accuracy: 0.9877\n",
            "Epoch 166/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 8.4964 - aut_loss: 0.2690 - class_loss: 0.5865 - itpr_loss: 0.5903 - lit_loss: 0.3337 - prec_loss: 0.7413 - princ_loss: 0.4176 - psy_loss: 1.0397 - rule_loss: 0.5242 - syst_loss: 0.4400 - tele_loss: 0.3589 - aut_accuracy: 0.9720 - class_accuracy: 0.7422 - itpr_accuracy: 0.7484 - lit_accuracy: 0.9876 - prec_accuracy: 0.5093 - princ_accuracy: 0.8789 - psy_accuracy: 0.1491 - rule_accuracy: 0.7826 - syst_accuracy: 0.9379 - tele_accuracy: 0.9814 - val_loss: 52.4311 - val_aut_loss: 16.1600 - val_class_loss: 13.2719 - val_itpr_loss: 0.5061 - val_lit_loss: 0.2923 - val_prec_loss: 0.8051 - val_princ_loss: 0.3803 - val_psy_loss: 1.0531 - val_rule_loss: 6.0717 - val_syst_loss: 0.3926 - val_tele_loss: 10.3027 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.8395 - val_itpr_accuracy: 0.8889 - val_lit_accuracy: 1.0000 - val_prec_accuracy: 0.4074 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.0123 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 1.0000 - val_tele_accuracy: 0.9877\n",
            "Epoch 167/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 8.6325 - aut_loss: 0.2729 - class_loss: 0.5903 - itpr_loss: 0.5809 - lit_loss: 0.3377 - prec_loss: 0.7657 - princ_loss: 0.4382 - psy_loss: 1.0465 - rule_loss: 0.5638 - syst_loss: 0.5017 - tele_loss: 0.3396 - aut_accuracy: 0.9720 - class_accuracy: 0.7267 - itpr_accuracy: 0.7391 - lit_accuracy: 0.9938 - prec_accuracy: 0.5031 - princ_accuracy: 0.8665 - psy_accuracy: 0.1335 - rule_accuracy: 0.8012 - syst_accuracy: 0.9130 - tele_accuracy: 0.9783 - val_loss: 101.5994 - val_aut_loss: 11.2872 - val_class_loss: 41.4595 - val_itpr_loss: 0.5066 - val_lit_loss: 0.2928 - val_prec_loss: 0.8031 - val_princ_loss: 0.3789 - val_psy_loss: 1.0495 - val_rule_loss: 41.5272 - val_syst_loss: 0.7912 - val_tele_loss: 0.3080 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.8519 - val_itpr_accuracy: 0.8889 - val_lit_accuracy: 1.0000 - val_prec_accuracy: 0.4074 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.0123 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 0.9877 - val_tele_accuracy: 1.0000\n",
            "Epoch 168/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 8.4377 - aut_loss: 0.2553 - class_loss: 0.5855 - itpr_loss: 0.5775 - lit_loss: 0.3262 - prec_loss: 0.7566 - princ_loss: 0.3868 - psy_loss: 1.0503 - rule_loss: 0.5209 - syst_loss: 0.4459 - tele_loss: 0.3375 - aut_accuracy: 0.9720 - class_accuracy: 0.7671 - itpr_accuracy: 0.7702 - lit_accuracy: 0.9876 - prec_accuracy: 0.5186 - princ_accuracy: 0.8820 - psy_accuracy: 0.1087 - rule_accuracy: 0.8385 - syst_accuracy: 0.8975 - tele_accuracy: 0.9814 - val_loss: 129.7001 - val_aut_loss: 24.5574 - val_class_loss: 51.0323 - val_itpr_loss: 0.5066 - val_lit_loss: 0.2919 - val_prec_loss: 0.8046 - val_princ_loss: 0.3780 - val_psy_loss: 1.0494 - val_rule_loss: 41.1205 - val_syst_loss: 6.4579 - val_tele_loss: 0.3063 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.8642 - val_itpr_accuracy: 0.8889 - val_lit_accuracy: 1.0000 - val_prec_accuracy: 0.4074 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.0123 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 0.9877 - val_tele_accuracy: 1.0000\n",
            "Epoch 169/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 8.5091 - aut_loss: 0.2518 - class_loss: 0.5827 - itpr_loss: 0.5979 - lit_loss: 0.3250 - prec_loss: 0.7377 - princ_loss: 0.4419 - psy_loss: 1.0599 - rule_loss: 0.5627 - syst_loss: 0.4170 - tele_loss: 0.3372 - aut_accuracy: 0.9720 - class_accuracy: 0.7578 - itpr_accuracy: 0.7391 - lit_accuracy: 0.9907 - prec_accuracy: 0.5373 - princ_accuracy: 0.8696 - psy_accuracy: 0.1398 - rule_accuracy: 0.8137 - syst_accuracy: 0.9224 - tele_accuracy: 0.9845 - val_loss: 1835.6018 - val_aut_loss: 454.6250 - val_class_loss: 0.5673 - val_itpr_loss: 618.6610 - val_lit_loss: 22.9688 - val_prec_loss: 161.6620 - val_princ_loss: 0.3771 - val_psy_loss: 506.2697 - val_rule_loss: 66.5805 - val_syst_loss: 0.3904 - val_tele_loss: 0.3048 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.8765 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.3951 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 1.0000 - val_tele_accuracy: 1.0000\n",
            "Epoch 170/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 8.5239 - aut_loss: 0.2597 - class_loss: 0.5729 - itpr_loss: 0.5992 - lit_loss: 0.3262 - prec_loss: 0.8096 - princ_loss: 0.4211 - psy_loss: 1.0524 - rule_loss: 0.5299 - syst_loss: 0.4147 - tele_loss: 0.3430 - aut_accuracy: 0.9720 - class_accuracy: 0.7702 - itpr_accuracy: 0.7516 - lit_accuracy: 0.9876 - prec_accuracy: 0.4845 - princ_accuracy: 0.8789 - psy_accuracy: 0.1335 - rule_accuracy: 0.7826 - syst_accuracy: 0.9534 - tele_accuracy: 0.9752 - val_loss: 1110.1602 - val_aut_loss: 189.7465 - val_class_loss: 0.5645 - val_itpr_loss: 239.3583 - val_lit_loss: 37.9614 - val_prec_loss: 27.8546 - val_princ_loss: 134.7295 - val_psy_loss: 325.2262 - val_rule_loss: 126.9578 - val_syst_loss: 0.3893 - val_tele_loss: 24.1768 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.9012 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.3951 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 1.0000 - val_tele_accuracy: 0.9877\n",
            "Epoch 171/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 8.4284 - aut_loss: 0.2526 - class_loss: 0.5732 - itpr_loss: 0.5905 - lit_loss: 0.3270 - prec_loss: 0.7665 - princ_loss: 0.4101 - psy_loss: 1.0441 - rule_loss: 0.5140 - syst_loss: 0.4209 - tele_loss: 0.3343 - aut_accuracy: 0.9720 - class_accuracy: 0.7702 - itpr_accuracy: 0.7795 - lit_accuracy: 0.9938 - prec_accuracy: 0.5062 - princ_accuracy: 0.8789 - psy_accuracy: 0.1087 - rule_accuracy: 0.8106 - syst_accuracy: 0.9441 - tele_accuracy: 0.9720 - val_loss: 112.4001 - val_aut_loss: 82.5038 - val_class_loss: 0.5599 - val_itpr_loss: 0.5068 - val_lit_loss: 0.2877 - val_prec_loss: 0.8126 - val_princ_loss: 0.3763 - val_psy_loss: 22.8242 - val_rule_loss: 0.6461 - val_syst_loss: 0.3867 - val_tele_loss: 0.3009 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.9259 - val_itpr_accuracy: 0.8889 - val_lit_accuracy: 1.0000 - val_prec_accuracy: 0.4074 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7778 - val_syst_accuracy: 1.0000 - val_tele_accuracy: 1.0000\n",
            "Epoch 172/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 8.4850 - aut_loss: 0.2605 - class_loss: 0.5623 - itpr_loss: 0.6009 - lit_loss: 0.3288 - prec_loss: 0.7573 - princ_loss: 0.4072 - psy_loss: 1.0688 - rule_loss: 0.5303 - syst_loss: 0.4335 - tele_loss: 0.3402 - aut_accuracy: 0.9720 - class_accuracy: 0.8043 - itpr_accuracy: 0.7391 - lit_accuracy: 0.9907 - prec_accuracy: 0.4938 - princ_accuracy: 0.8727 - psy_accuracy: 0.1491 - rule_accuracy: 0.8137 - syst_accuracy: 0.9379 - tele_accuracy: 0.9720 - val_loss: 64.5621 - val_aut_loss: 27.2466 - val_class_loss: 0.5565 - val_itpr_loss: 0.5070 - val_lit_loss: 0.2865 - val_prec_loss: 0.8155 - val_princ_loss: 0.3754 - val_psy_loss: 16.5591 - val_rule_loss: 11.2408 - val_syst_loss: 0.3849 - val_tele_loss: 3.3947 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.9259 - val_itpr_accuracy: 0.8889 - val_lit_accuracy: 1.0000 - val_prec_accuracy: 0.4074 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 1.0000 - val_tele_accuracy: 0.9877\n",
            "Epoch 173/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 8.4756 - aut_loss: 0.2608 - class_loss: 0.5786 - itpr_loss: 0.5733 - lit_loss: 0.3286 - prec_loss: 0.7980 - princ_loss: 0.4415 - psy_loss: 1.0308 - rule_loss: 0.5165 - syst_loss: 0.4243 - tele_loss: 0.3279 - aut_accuracy: 0.9720 - class_accuracy: 0.7795 - itpr_accuracy: 0.7453 - lit_accuracy: 0.9938 - prec_accuracy: 0.4814 - princ_accuracy: 0.8758 - psy_accuracy: 0.1832 - rule_accuracy: 0.8075 - syst_accuracy: 0.9472 - tele_accuracy: 0.9814 - val_loss: 43.4162 - val_aut_loss: 8.4581 - val_class_loss: 7.3164 - val_itpr_loss: 0.5074 - val_lit_loss: 0.2856 - val_prec_loss: 0.8176 - val_princ_loss: 0.3744 - val_psy_loss: 1.0447 - val_rule_loss: 0.6489 - val_syst_loss: 19.0181 - val_tele_loss: 1.7498 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.9136 - val_itpr_accuracy: 0.8889 - val_lit_accuracy: 1.0000 - val_prec_accuracy: 0.4074 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.0123 - val_rule_accuracy: 0.7778 - val_syst_accuracy: 0.9877 - val_tele_accuracy: 0.9877\n",
            "Epoch 174/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 8.4264 - aut_loss: 0.2584 - class_loss: 0.5610 - itpr_loss: 0.5828 - lit_loss: 0.3148 - prec_loss: 0.7798 - princ_loss: 0.4405 - psy_loss: 1.0308 - rule_loss: 0.5235 - syst_loss: 0.4162 - tele_loss: 0.3236 - aut_accuracy: 0.9720 - class_accuracy: 0.7702 - itpr_accuracy: 0.7702 - lit_accuracy: 1.0000 - prec_accuracy: 0.5062 - princ_accuracy: 0.8758 - psy_accuracy: 0.1429 - rule_accuracy: 0.8075 - syst_accuracy: 0.9317 - tele_accuracy: 0.9845 - val_loss: 878.7259 - val_aut_loss: 176.2609 - val_class_loss: 0.5517 - val_itpr_loss: 195.1699 - val_lit_loss: 40.2022 - val_prec_loss: 0.8186 - val_princ_loss: 112.1556 - val_psy_loss: 221.3070 - val_rule_loss: 34.2711 - val_syst_loss: 0.3823 - val_tele_loss: 94.4113 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.9259 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.4074 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 1.0000 - val_tele_accuracy: 0.9877\n",
            "Epoch 175/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 8.4482 - aut_loss: 0.2634 - class_loss: 0.5553 - itpr_loss: 0.6128 - lit_loss: 0.3085 - prec_loss: 0.7645 - princ_loss: 0.4162 - psy_loss: 1.0868 - rule_loss: 0.5298 - syst_loss: 0.4123 - tele_loss: 0.3033 - aut_accuracy: 0.9720 - class_accuracy: 0.8168 - itpr_accuracy: 0.7547 - lit_accuracy: 0.9907 - prec_accuracy: 0.4658 - princ_accuracy: 0.8851 - psy_accuracy: 0.1180 - rule_accuracy: 0.8261 - syst_accuracy: 0.9596 - tele_accuracy: 0.9876 - val_loss: 137.3018 - val_aut_loss: 20.1495 - val_class_loss: 60.9524 - val_itpr_loss: 2.9435 - val_lit_loss: 0.2850 - val_prec_loss: 0.8195 - val_princ_loss: 0.3725 - val_psy_loss: 1.0371 - val_rule_loss: 46.8717 - val_syst_loss: 0.3812 - val_tele_loss: 0.2942 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.9259 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 1.0000 - val_prec_accuracy: 0.4074 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.0123 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 1.0000 - val_tele_accuracy: 1.0000\n",
            "Epoch 176/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 8.4213 - aut_loss: 0.2546 - class_loss: 0.5600 - itpr_loss: 0.5928 - lit_loss: 0.3213 - prec_loss: 0.7967 - princ_loss: 0.4312 - psy_loss: 1.0185 - rule_loss: 0.5139 - syst_loss: 0.4123 - tele_loss: 0.3249 - aut_accuracy: 0.9720 - class_accuracy: 0.8199 - itpr_accuracy: 0.7702 - lit_accuracy: 0.9876 - prec_accuracy: 0.4938 - princ_accuracy: 0.8913 - psy_accuracy: 0.1398 - rule_accuracy: 0.8323 - syst_accuracy: 0.9441 - tele_accuracy: 0.9814 - val_loss: 220.1431 - val_aut_loss: 36.4621 - val_class_loss: 107.6949 - val_itpr_loss: 0.5095 - val_lit_loss: 0.2852 - val_prec_loss: 0.8182 - val_princ_loss: 0.3714 - val_psy_loss: 1.0324 - val_rule_loss: 68.7907 - val_syst_loss: 0.6904 - val_tele_loss: 0.2932 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.9259 - val_itpr_accuracy: 0.8889 - val_lit_accuracy: 1.0000 - val_prec_accuracy: 0.4074 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.0123 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 0.9877 - val_tele_accuracy: 1.0000\n",
            "Epoch 177/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 8.3857 - aut_loss: 0.2455 - class_loss: 0.5524 - itpr_loss: 0.5972 - lit_loss: 0.3001 - prec_loss: 0.7899 - princ_loss: 0.4101 - psy_loss: 1.0337 - rule_loss: 0.5222 - syst_loss: 0.4148 - tele_loss: 0.3245 - aut_accuracy: 0.9720 - class_accuracy: 0.7764 - itpr_accuracy: 0.7391 - lit_accuracy: 1.0000 - prec_accuracy: 0.4876 - princ_accuracy: 0.8758 - psy_accuracy: 0.1801 - rule_accuracy: 0.8137 - syst_accuracy: 0.9441 - tele_accuracy: 0.9783 - val_loss: 3440.5593 - val_aut_loss: 976.1634 - val_class_loss: 0.5464 - val_itpr_loss: 1078.9152 - val_lit_loss: 255.0440 - val_prec_loss: 555.8428 - val_princ_loss: 0.3702 - val_psy_loss: 569.1613 - val_rule_loss: 0.6482 - val_syst_loss: 0.3807 - val_tele_loss: 0.2922 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.9383 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.3951 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7778 - val_syst_accuracy: 1.0000 - val_tele_accuracy: 1.0000\n",
            "Epoch 178/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 8.3913 - aut_loss: 0.2496 - class_loss: 0.5388 - itpr_loss: 0.6147 - lit_loss: 0.3166 - prec_loss: 0.7449 - princ_loss: 0.4355 - psy_loss: 1.0242 - rule_loss: 0.5201 - syst_loss: 0.4250 - tele_loss: 0.3268 - aut_accuracy: 0.9720 - class_accuracy: 0.8261 - itpr_accuracy: 0.7329 - lit_accuracy: 0.9907 - prec_accuracy: 0.4907 - princ_accuracy: 0.8696 - psy_accuracy: 0.1615 - rule_accuracy: 0.8168 - syst_accuracy: 0.9441 - tele_accuracy: 0.9783 - val_loss: 272.3516 - val_aut_loss: 35.3464 - val_class_loss: 116.7148 - val_itpr_loss: 0.5109 - val_lit_loss: 0.2852 - val_prec_loss: 0.8170 - val_princ_loss: 0.3689 - val_psy_loss: 1.0237 - val_rule_loss: 110.1237 - val_syst_loss: 3.6751 - val_tele_loss: 0.2907 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.9259 - val_itpr_accuracy: 0.8889 - val_lit_accuracy: 1.0000 - val_prec_accuracy: 0.4074 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.0123 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 0.9877 - val_tele_accuracy: 1.0000\n",
            "Epoch 179/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 185.8273 - aut_loss: 0.2552 - class_loss: 23.1926 - itpr_loss: 25.1205 - lit_loss: 0.3133 - prec_loss: 65.4155 - princ_loss: 10.4747 - psy_loss: 47.8110 - rule_loss: 0.5263 - syst_loss: 9.1986 - tele_loss: 0.3246 - aut_accuracy: 0.9720 - class_accuracy: 0.8075 - itpr_accuracy: 0.7640 - lit_accuracy: 0.9783 - prec_accuracy: 0.4845 - princ_accuracy: 0.8789 - psy_accuracy: 0.1925 - rule_accuracy: 0.7981 - syst_accuracy: 0.9255 - tele_accuracy: 0.9814 - val_loss: 892.2614 - val_aut_loss: 193.3217 - val_class_loss: 0.5414 - val_itpr_loss: 181.3563 - val_lit_loss: 47.3646 - val_prec_loss: 0.8180 - val_princ_loss: 110.8963 - val_psy_loss: 202.3826 - val_rule_loss: 51.4437 - val_syst_loss: 0.3791 - val_tele_loss: 100.5626 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.9383 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.4074 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 1.0000 - val_tele_accuracy: 0.9877\n",
            "Epoch 180/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 8.4248 - aut_loss: 0.2396 - class_loss: 0.5575 - itpr_loss: 0.6184 - lit_loss: 0.3261 - prec_loss: 0.7782 - princ_loss: 0.4018 - psy_loss: 1.0229 - rule_loss: 0.5294 - syst_loss: 0.4283 - tele_loss: 0.3276 - aut_accuracy: 0.9720 - class_accuracy: 0.7888 - itpr_accuracy: 0.7267 - lit_accuracy: 0.9907 - prec_accuracy: 0.5000 - princ_accuracy: 0.8727 - psy_accuracy: 0.1677 - rule_accuracy: 0.8292 - syst_accuracy: 0.9410 - tele_accuracy: 0.9814 - val_loss: 760.6357 - val_aut_loss: 116.5466 - val_class_loss: 301.2579 - val_itpr_loss: 0.5120 - val_lit_loss: 0.2848 - val_prec_loss: 0.8169 - val_princ_loss: 0.3671 - val_psy_loss: 1.0174 - val_rule_loss: 278.8257 - val_syst_loss: 57.5242 - val_tele_loss: 0.2882 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.9259 - val_itpr_accuracy: 0.8889 - val_lit_accuracy: 1.0000 - val_prec_accuracy: 0.4074 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.0123 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 0.9877 - val_tele_accuracy: 1.0000\n",
            "Epoch 181/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 8.4148 - aut_loss: 0.2571 - class_loss: 0.5477 - itpr_loss: 0.5886 - lit_loss: 0.3341 - prec_loss: 0.7577 - princ_loss: 0.4302 - psy_loss: 1.0213 - rule_loss: 0.5360 - syst_loss: 0.4190 - tele_loss: 0.3279 - aut_accuracy: 0.9720 - class_accuracy: 0.7919 - itpr_accuracy: 0.7516 - lit_accuracy: 0.9720 - prec_accuracy: 0.5280 - princ_accuracy: 0.8789 - psy_accuracy: 0.1553 - rule_accuracy: 0.8230 - syst_accuracy: 0.9224 - tele_accuracy: 0.9689 - val_loss: 94.8416 - val_aut_loss: 33.0533 - val_class_loss: 0.5401 - val_itpr_loss: 0.5134 - val_lit_loss: 2.0828 - val_prec_loss: 0.8130 - val_princ_loss: 0.3658 - val_psy_loss: 34.9673 - val_rule_loss: 18.6432 - val_syst_loss: 0.3797 - val_tele_loss: 0.2880 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.9383 - val_itpr_accuracy: 0.8889 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.4074 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 1.0000 - val_tele_accuracy: 1.0000\n",
            "Epoch 182/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 8.3382 - aut_loss: 0.2391 - class_loss: 0.5379 - itpr_loss: 0.5929 - lit_loss: 0.3164 - prec_loss: 0.7840 - princ_loss: 0.4030 - psy_loss: 1.0268 - rule_loss: 0.5014 - syst_loss: 0.4259 - tele_loss: 0.3157 - aut_accuracy: 0.9689 - class_accuracy: 0.8385 - itpr_accuracy: 0.7391 - lit_accuracy: 0.9845 - prec_accuracy: 0.4907 - princ_accuracy: 0.8882 - psy_accuracy: 0.1584 - rule_accuracy: 0.8261 - syst_accuracy: 0.9161 - tele_accuracy: 0.9720 - val_loss: 412.9535 - val_aut_loss: 18.2611 - val_class_loss: 211.5536 - val_itpr_loss: 32.4519 - val_lit_loss: 0.2856 - val_prec_loss: 0.8143 - val_princ_loss: 0.3649 - val_psy_loss: 1.0080 - val_rule_loss: 112.2480 - val_syst_loss: 32.4845 - val_tele_loss: 0.2865 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.9259 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 1.0000 - val_prec_accuracy: 0.4074 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.0123 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 0.9877 - val_tele_accuracy: 1.0000\n",
            "Epoch 183/200\n",
            "6/6 [==============================] - 40s 7s/step - loss: 8.3574 - aut_loss: 0.2439 - class_loss: 0.5511 - itpr_loss: 0.5858 - lit_loss: 0.3254 - prec_loss: 0.7957 - princ_loss: 0.4057 - psy_loss: 0.9842 - rule_loss: 0.5352 - syst_loss: 0.4020 - tele_loss: 0.3333 - aut_accuracy: 0.9720 - class_accuracy: 0.8075 - itpr_accuracy: 0.7609 - lit_accuracy: 0.9783 - prec_accuracy: 0.4969 - princ_accuracy: 0.8851 - psy_accuracy: 0.1770 - rule_accuracy: 0.7888 - syst_accuracy: 0.9565 - tele_accuracy: 0.9720 - val_loss: 56.6539 - val_aut_loss: 0.2442 - val_class_loss: 11.0804 - val_itpr_loss: 0.5134 - val_lit_loss: 0.2829 - val_prec_loss: 0.8197 - val_princ_loss: 2.5325 - val_psy_loss: 21.4154 - val_rule_loss: 15.9102 - val_syst_loss: 0.3762 - val_tele_loss: 0.2840 - val_aut_accuracy: 0.9630 - val_class_accuracy: 0.9259 - val_itpr_accuracy: 0.8889 - val_lit_accuracy: 1.0000 - val_prec_accuracy: 0.4074 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 1.0000 - val_tele_accuracy: 1.0000\n",
            "Epoch 184/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 139.0629 - aut_loss: 0.2476 - class_loss: 49.5141 - itpr_loss: 0.6125 - lit_loss: 0.3171 - prec_loss: 0.7739 - princ_loss: 23.7364 - psy_loss: 4.7306 - rule_loss: 0.5307 - syst_loss: 55.0904 - tele_loss: 0.3146 - aut_accuracy: 0.9720 - class_accuracy: 0.8385 - itpr_accuracy: 0.7205 - lit_accuracy: 0.9876 - prec_accuracy: 0.5217 - princ_accuracy: 0.8820 - psy_accuracy: 0.1739 - rule_accuracy: 0.8075 - syst_accuracy: 0.9348 - tele_accuracy: 0.9845 - val_loss: 5978.1982 - val_aut_loss: 1191.0577 - val_class_loss: 0.5292 - val_itpr_loss: 2574.5083 - val_lit_loss: 0.2800 - val_prec_loss: 0.8260 - val_princ_loss: 0.3641 - val_psy_loss: 1724.7369 - val_rule_loss: 482.0454 - val_syst_loss: 0.3737 - val_tele_loss: 0.2813 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.9630 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 1.0000 - val_prec_accuracy: 0.4074 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 1.0000 - val_tele_accuracy: 1.0000\n",
            "Epoch 185/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 8.3947 - aut_loss: 0.2624 - class_loss: 0.5386 - itpr_loss: 0.6011 - lit_loss: 0.3181 - prec_loss: 0.7843 - princ_loss: 0.4190 - psy_loss: 1.0105 - rule_loss: 0.5448 - syst_loss: 0.4034 - tele_loss: 0.3174 - aut_accuracy: 0.9720 - class_accuracy: 0.8230 - itpr_accuracy: 0.7422 - lit_accuracy: 0.9845 - prec_accuracy: 0.4814 - princ_accuracy: 0.8851 - psy_accuracy: 0.1398 - rule_accuracy: 0.7826 - syst_accuracy: 0.9503 - tele_accuracy: 0.9876 - val_loss: 85.1455 - val_aut_loss: 27.4870 - val_class_loss: 0.5265 - val_itpr_loss: 0.5136 - val_lit_loss: 0.2792 - val_prec_loss: 0.8280 - val_princ_loss: 0.3632 - val_psy_loss: 18.4961 - val_rule_loss: 32.8051 - val_syst_loss: 0.3724 - val_tele_loss: 0.2795 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.9630 - val_itpr_accuracy: 0.8889 - val_lit_accuracy: 1.0000 - val_prec_accuracy: 0.4074 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 1.0000 - val_tele_accuracy: 1.0000\n",
            "Epoch 186/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 8.3469 - aut_loss: 0.2522 - class_loss: 0.5384 - itpr_loss: 0.5898 - lit_loss: 0.3302 - prec_loss: 0.7649 - princ_loss: 0.4193 - psy_loss: 0.9576 - rule_loss: 0.5591 - syst_loss: 0.4114 - tele_loss: 0.3290 - aut_accuracy: 0.9720 - class_accuracy: 0.8012 - itpr_accuracy: 0.7484 - lit_accuracy: 0.9783 - prec_accuracy: 0.4938 - princ_accuracy: 0.8758 - psy_accuracy: 0.2081 - rule_accuracy: 0.7950 - syst_accuracy: 0.9255 - tele_accuracy: 0.9720 - val_loss: 857.0532 - val_aut_loss: 123.0822 - val_class_loss: 370.3633 - val_itpr_loss: 0.5143 - val_lit_loss: 0.2788 - val_prec_loss: 0.8284 - val_princ_loss: 0.3622 - val_psy_loss: 1.0067 - val_rule_loss: 325.4423 - val_syst_loss: 31.7022 - val_tele_loss: 0.2778 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.9506 - val_itpr_accuracy: 0.8889 - val_lit_accuracy: 1.0000 - val_prec_accuracy: 0.4074 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.0123 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 0.9877 - val_tele_accuracy: 1.0000\n",
            "Epoch 187/200\n",
            "6/6 [==============================] - 23s 4s/step - loss: 8.4026 - aut_loss: 0.2583 - class_loss: 0.5344 - itpr_loss: 0.6381 - lit_loss: 0.3286 - prec_loss: 0.7808 - princ_loss: 0.4018 - psy_loss: 0.9834 - rule_loss: 0.5350 - syst_loss: 0.4215 - tele_loss: 0.3256 - aut_accuracy: 0.9720 - class_accuracy: 0.8634 - itpr_accuracy: 0.6894 - lit_accuracy: 0.9845 - prec_accuracy: 0.4907 - princ_accuracy: 0.8851 - psy_accuracy: 0.2236 - rule_accuracy: 0.7888 - syst_accuracy: 0.9037 - tele_accuracy: 0.9814 - val_loss: 1604.8519 - val_aut_loss: 439.1859 - val_class_loss: 0.5222 - val_itpr_loss: 423.4534 - val_lit_loss: 0.2788 - val_prec_loss: 124.3143 - val_princ_loss: 0.3611 - val_psy_loss: 612.2438 - val_rule_loss: 0.6498 - val_syst_loss: 0.3711 - val_tele_loss: 0.2765 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.9630 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 1.0000 - val_prec_accuracy: 0.3827 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7778 - val_syst_accuracy: 1.0000 - val_tele_accuracy: 1.0000\n",
            "Epoch 188/200\n",
            "6/6 [==============================] - 23s 4s/step - loss: 8.3460 - aut_loss: 0.2468 - class_loss: 0.5208 - itpr_loss: 0.6169 - lit_loss: 0.3133 - prec_loss: 0.7829 - princ_loss: 0.4459 - psy_loss: 1.0107 - rule_loss: 0.5115 - syst_loss: 0.3941 - tele_loss: 0.3082 - aut_accuracy: 0.9720 - class_accuracy: 0.8727 - itpr_accuracy: 0.6988 - lit_accuracy: 0.9876 - prec_accuracy: 0.5062 - princ_accuracy: 0.8727 - psy_accuracy: 0.1677 - rule_accuracy: 0.8137 - syst_accuracy: 0.9565 - tele_accuracy: 0.9876 - val_loss: 118.9359 - val_aut_loss: 45.8655 - val_class_loss: 0.5190 - val_itpr_loss: 0.5153 - val_lit_loss: 0.2776 - val_prec_loss: 0.8297 - val_princ_loss: 0.3605 - val_psy_loss: 18.9426 - val_rule_loss: 47.7862 - val_syst_loss: 0.3698 - val_tele_loss: 0.2746 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.9630 - val_itpr_accuracy: 0.8889 - val_lit_accuracy: 1.0000 - val_prec_accuracy: 0.3951 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 1.0000 - val_tele_accuracy: 1.0000\n",
            "Epoch 189/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 8.3740 - aut_loss: 0.2425 - class_loss: 0.5188 - itpr_loss: 0.6302 - lit_loss: 0.3076 - prec_loss: 0.7980 - princ_loss: 0.4359 - psy_loss: 0.9957 - rule_loss: 0.5275 - syst_loss: 0.4134 - tele_loss: 0.3095 - aut_accuracy: 0.9720 - class_accuracy: 0.8323 - itpr_accuracy: 0.7019 - lit_accuracy: 0.9907 - prec_accuracy: 0.4938 - princ_accuracy: 0.8727 - psy_accuracy: 0.2019 - rule_accuracy: 0.8106 - syst_accuracy: 0.9255 - tele_accuracy: 0.9752 - val_loss: 934.2798 - val_aut_loss: 228.0885 - val_class_loss: 0.5176 - val_itpr_loss: 172.0693 - val_lit_loss: 52.8422 - val_prec_loss: 0.8282 - val_princ_loss: 103.8671 - val_psy_loss: 191.9448 - val_rule_loss: 72.2355 - val_syst_loss: 0.3698 - val_tele_loss: 108.3218 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.9753 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.3951 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 1.0000 - val_tele_accuracy: 0.9877\n",
            "Epoch 190/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 8.3463 - aut_loss: 0.2300 - class_loss: 0.5399 - itpr_loss: 0.6091 - lit_loss: 0.3169 - prec_loss: 0.8285 - princ_loss: 0.3840 - psy_loss: 0.9436 - rule_loss: 0.5462 - syst_loss: 0.4332 - tele_loss: 0.3200 - aut_accuracy: 0.9720 - class_accuracy: 0.8075 - itpr_accuracy: 0.7236 - lit_accuracy: 0.9876 - prec_accuracy: 0.4596 - princ_accuracy: 0.8758 - psy_accuracy: 0.2019 - rule_accuracy: 0.7950 - syst_accuracy: 0.9099 - tele_accuracy: 0.9689 - val_loss: 587.3586 - val_aut_loss: 117.4607 - val_class_loss: 0.5155 - val_itpr_loss: 161.0562 - val_lit_loss: 0.2779 - val_prec_loss: 29.4831 - val_princ_loss: 34.0906 - val_psy_loss: 205.0958 - val_rule_loss: 35.5430 - val_syst_loss: 0.3690 - val_tele_loss: 0.2720 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.9753 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 1.0000 - val_prec_accuracy: 0.3827 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 1.0000 - val_tele_accuracy: 1.0000\n",
            "Epoch 191/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 8.2857 - aut_loss: 0.2347 - class_loss: 0.5255 - itpr_loss: 0.6111 - lit_loss: 0.3238 - prec_loss: 0.7637 - princ_loss: 0.4117 - psy_loss: 0.9894 - rule_loss: 0.5195 - syst_loss: 0.4114 - tele_loss: 0.3001 - aut_accuracy: 0.9720 - class_accuracy: 0.8137 - itpr_accuracy: 0.7329 - lit_accuracy: 0.9938 - prec_accuracy: 0.4907 - princ_accuracy: 0.8820 - psy_accuracy: 0.2081 - rule_accuracy: 0.8012 - syst_accuracy: 0.9348 - tele_accuracy: 0.9814 - val_loss: 375.7113 - val_aut_loss: 22.2345 - val_class_loss: 182.8397 - val_itpr_loss: 13.3753 - val_lit_loss: 0.2775 - val_prec_loss: 0.8297 - val_princ_loss: 0.3574 - val_psy_loss: 0.9918 - val_rule_loss: 109.7699 - val_syst_loss: 41.5703 - val_tele_loss: 0.2703 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.9630 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 1.0000 - val_prec_accuracy: 0.3827 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.0123 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 0.9877 - val_tele_accuracy: 1.0000\n",
            "Epoch 192/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 216866992.0000 - aut_loss: 0.2424 - class_loss: 0.5275 - itpr_loss: 78978824.0000 - lit_loss: 3717118.5000 - prec_loss: 0.8041 - princ_loss: 44919832.0000 - psy_loss: 0.9985 - rule_loss: 0.5542 - syst_loss: 89251216.0000 - tele_loss: 0.2993 - aut_accuracy: 0.9720 - class_accuracy: 0.7981 - itpr_accuracy: 0.7019 - lit_accuracy: 0.9907 - prec_accuracy: 0.4907 - princ_accuracy: 0.8696 - psy_accuracy: 0.2267 - rule_accuracy: 0.8075 - syst_accuracy: 0.9286 - tele_accuracy: 0.9845 - val_loss: 6284.9434 - val_aut_loss: 1234.6765 - val_class_loss: 0.5112 - val_itpr_loss: 2780.6851 - val_lit_loss: 0.2776 - val_prec_loss: 0.8287 - val_princ_loss: 0.3565 - val_psy_loss: 1800.5363 - val_rule_loss: 463.2397 - val_syst_loss: 0.3676 - val_tele_loss: 0.2693 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.9753 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 1.0000 - val_prec_accuracy: 0.3827 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 1.0000 - val_tele_accuracy: 1.0000\n",
            "Epoch 193/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 6084599.5000 - aut_loss: 0.2455 - class_loss: 0.5195 - itpr_loss: 1705799.0000 - lit_loss: 164825.7344 - prec_loss: 0.7867 - princ_loss: 0.4266 - psy_loss: 1832044.1250 - rule_loss: 1408082.2500 - syst_loss: 0.4105 - tele_loss: 973842.5625 - aut_accuracy: 0.9720 - class_accuracy: 0.8385 - itpr_accuracy: 0.7360 - lit_accuracy: 0.9907 - prec_accuracy: 0.5093 - princ_accuracy: 0.8634 - psy_accuracy: 0.2329 - rule_accuracy: 0.7981 - syst_accuracy: 0.9130 - tele_accuracy: 0.9658 - val_loss: 5997.1494 - val_aut_loss: 1197.1681 - val_class_loss: 0.5091 - val_itpr_loss: 2579.9468 - val_lit_loss: 0.2776 - val_prec_loss: 0.8281 - val_princ_loss: 0.3556 - val_psy_loss: 1734.9023 - val_rule_loss: 479.3319 - val_syst_loss: 0.3673 - val_tele_loss: 0.2680 - val_aut_accuracy: 0.9506 - val_class_accuracy: 1.0000 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 1.0000 - val_prec_accuracy: 0.3827 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 1.0000 - val_tele_accuracy: 1.0000\n",
            "Epoch 194/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 8.3352 - aut_loss: 0.2411 - class_loss: 0.5133 - itpr_loss: 0.6359 - lit_loss: 0.3106 - prec_loss: 0.7843 - princ_loss: 0.4332 - psy_loss: 1.0060 - rule_loss: 0.5199 - syst_loss: 0.4057 - tele_loss: 0.2904 - aut_accuracy: 0.9720 - class_accuracy: 0.8509 - itpr_accuracy: 0.7019 - lit_accuracy: 0.9876 - prec_accuracy: 0.5186 - princ_accuracy: 0.8758 - psy_accuracy: 0.1925 - rule_accuracy: 0.8012 - syst_accuracy: 0.9348 - tele_accuracy: 0.9907 - val_loss: 472.8971 - val_aut_loss: 41.3267 - val_class_loss: 235.6313 - val_itpr_loss: 0.5202 - val_lit_loss: 0.2790 - val_prec_loss: 0.8245 - val_princ_loss: 0.3545 - val_psy_loss: 0.9788 - val_rule_loss: 121.3305 - val_syst_loss: 68.1891 - val_tele_loss: 0.2676 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.9877 - val_itpr_accuracy: 0.8889 - val_lit_accuracy: 1.0000 - val_prec_accuracy: 0.3827 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.0123 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 0.9877 - val_tele_accuracy: 1.0000\n",
            "Epoch 195/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 8.3211 - aut_loss: 0.2518 - class_loss: 0.5110 - itpr_loss: 0.6232 - lit_loss: 0.3198 - prec_loss: 0.8032 - princ_loss: 0.4182 - psy_loss: 0.9909 - rule_loss: 0.5162 - syst_loss: 0.3990 - tele_loss: 0.2930 - aut_accuracy: 0.9720 - class_accuracy: 0.8602 - itpr_accuracy: 0.7174 - lit_accuracy: 0.9876 - prec_accuracy: 0.4814 - princ_accuracy: 0.8789 - psy_accuracy: 0.1925 - rule_accuracy: 0.8199 - syst_accuracy: 0.9472 - tele_accuracy: 0.9876 - val_loss: 1371.5829 - val_aut_loss: 260.9233 - val_class_loss: 0.5072 - val_itpr_loss: 428.3492 - val_lit_loss: 0.2793 - val_prec_loss: 111.3955 - val_princ_loss: 0.3536 - val_psy_loss: 547.0404 - val_rule_loss: 18.9051 - val_syst_loss: 0.3677 - val_tele_loss: 0.2666 - val_aut_accuracy: 0.9506 - val_class_accuracy: 1.0000 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 1.0000 - val_prec_accuracy: 0.3704 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 1.0000 - val_tele_accuracy: 1.0000\n",
            "Epoch 196/200\n",
            "6/6 [==============================] - 21s 4s/step - loss: 8.2452 - aut_loss: 0.2444 - class_loss: 0.5225 - itpr_loss: 0.6093 - lit_loss: 0.3139 - prec_loss: 0.7453 - princ_loss: 0.3912 - psy_loss: 0.9744 - rule_loss: 0.5337 - syst_loss: 0.4115 - tele_loss: 0.3042 - aut_accuracy: 0.9720 - class_accuracy: 0.8137 - itpr_accuracy: 0.7267 - lit_accuracy: 0.9907 - prec_accuracy: 0.4938 - princ_accuracy: 0.8913 - psy_accuracy: 0.2081 - rule_accuracy: 0.8261 - syst_accuracy: 0.9317 - tele_accuracy: 0.9907 - val_loss: 1518.4670 - val_aut_loss: 310.4289 - val_class_loss: 0.5045 - val_itpr_loss: 436.8655 - val_lit_loss: 0.2784 - val_prec_loss: 142.5757 - val_princ_loss: 0.3529 - val_psy_loss: 585.5532 - val_rule_loss: 38.0812 - val_syst_loss: 0.3666 - val_tele_loss: 0.2650 - val_aut_accuracy: 0.9506 - val_class_accuracy: 1.0000 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 1.0000 - val_prec_accuracy: 0.3704 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 1.0000 - val_tele_accuracy: 1.0000\n",
            "Epoch 197/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 2931.8965 - aut_loss: 813.4602 - class_loss: 0.5111 - itpr_loss: 527.9049 - lit_loss: 339.3228 - prec_loss: 0.8009 - princ_loss: 160.9699 - psy_loss: 482.9809 - rule_loss: 389.0903 - syst_loss: 0.4041 - tele_loss: 213.2566 - aut_accuracy: 0.9720 - class_accuracy: 0.8696 - itpr_accuracy: 0.7112 - lit_accuracy: 0.9876 - prec_accuracy: 0.4969 - princ_accuracy: 0.8727 - psy_accuracy: 0.2081 - rule_accuracy: 0.8075 - syst_accuracy: 0.9379 - tele_accuracy: 0.9783 - val_loss: 902.3707 - val_aut_loss: 166.2705 - val_class_loss: 335.9437 - val_itpr_loss: 0.5225 - val_lit_loss: 0.2778 - val_prec_loss: 0.8255 - val_princ_loss: 0.3522 - val_psy_loss: 0.9681 - val_rule_loss: 322.5986 - val_syst_loss: 71.1534 - val_tele_loss: 0.2637 - val_aut_accuracy: 0.9506 - val_class_accuracy: 0.9877 - val_itpr_accuracy: 0.8889 - val_lit_accuracy: 1.0000 - val_prec_accuracy: 0.3827 - val_princ_accuracy: 0.9136 - val_psy_accuracy: 0.0123 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 0.9877 - val_tele_accuracy: 1.0000\n",
            "Epoch 198/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 35.7603 - aut_loss: 0.2498 - class_loss: 0.5239 - itpr_loss: 8.1703 - lit_loss: 5.8324 - prec_loss: 0.7744 - princ_loss: 0.7664 - psy_loss: 3.2953 - rule_loss: 5.6314 - syst_loss: 0.4195 - tele_loss: 6.9020 - aut_accuracy: 0.9720 - class_accuracy: 0.8634 - itpr_accuracy: 0.7205 - lit_accuracy: 0.9814 - prec_accuracy: 0.5155 - princ_accuracy: 0.8882 - psy_accuracy: 0.2081 - rule_accuracy: 0.7795 - syst_accuracy: 0.9379 - tele_accuracy: 0.9845 - val_loss: 1055.7207 - val_aut_loss: 269.1760 - val_class_loss: 0.5019 - val_itpr_loss: 183.2589 - val_lit_loss: 91.5677 - val_prec_loss: 0.8239 - val_princ_loss: 101.8811 - val_psy_loss: 186.6631 - val_rule_loss: 104.3059 - val_syst_loss: 0.3659 - val_tele_loss: 113.9815 - val_aut_accuracy: 0.9506 - val_class_accuracy: 1.0000 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.3827 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 1.0000 - val_tele_accuracy: 0.9877\n",
            "Epoch 199/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 24625.2422 - aut_loss: 6854.3286 - class_loss: 0.5008 - itpr_loss: 0.6005 - lit_loss: 0.3026 - prec_loss: 0.8335 - princ_loss: 0.3934 - psy_loss: 12359.9600 - rule_loss: 0.5330 - syst_loss: 5404.2993 - tele_loss: 0.2940 - aut_accuracy: 0.9689 - class_accuracy: 0.8634 - itpr_accuracy: 0.7298 - lit_accuracy: 0.9938 - prec_accuracy: 0.4689 - princ_accuracy: 0.8975 - psy_accuracy: 0.2484 - rule_accuracy: 0.8261 - syst_accuracy: 0.9224 - tele_accuracy: 0.9876 - val_loss: 9384.7715 - val_aut_loss: 2382.5850 - val_class_loss: 0.5008 - val_itpr_loss: 1904.4073 - val_lit_loss: 487.5261 - val_prec_loss: 0.8236 - val_princ_loss: 924.7390 - val_psy_loss: 1422.2698 - val_rule_loss: 1192.5963 - val_syst_loss: 0.3655 - val_tele_loss: 1065.7627 - val_aut_accuracy: 0.9506 - val_class_accuracy: 1.0000 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.3827 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 1.0000 - val_tele_accuracy: 0.9877\n",
            "Epoch 200/200\n",
            "6/6 [==============================] - 22s 4s/step - loss: 8.2839 - aut_loss: 0.2192 - class_loss: 0.5060 - itpr_loss: 0.6215 - lit_loss: 0.3076 - prec_loss: 0.8099 - princ_loss: 0.4449 - psy_loss: 1.0009 - rule_loss: 0.5094 - syst_loss: 0.3865 - tele_loss: 0.2830 - aut_accuracy: 0.9720 - class_accuracy: 0.8789 - itpr_accuracy: 0.7236 - lit_accuracy: 0.9907 - prec_accuracy: 0.4596 - princ_accuracy: 0.8696 - psy_accuracy: 0.2050 - rule_accuracy: 0.8199 - syst_accuracy: 0.9379 - tele_accuracy: 0.9814 - val_loss: 1392.2219 - val_aut_loss: 370.6624 - val_class_loss: 0.5006 - val_itpr_loss: 239.1167 - val_lit_loss: 142.7715 - val_prec_loss: 0.8212 - val_princ_loss: 110.4090 - val_psy_loss: 238.5920 - val_rule_loss: 154.6478 - val_syst_loss: 0.3658 - val_tele_loss: 131.1402 - val_aut_accuracy: 0.9506 - val_class_accuracy: 1.0000 - val_itpr_accuracy: 0.8765 - val_lit_accuracy: 0.9877 - val_prec_accuracy: 0.3827 - val_princ_accuracy: 0.9012 - val_psy_accuracy: 0.0000e+00 - val_rule_accuracy: 0.7654 - val_syst_accuracy: 1.0000 - val_tele_accuracy: 0.9877\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "1dG-CjfGkRYn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  from sklearn.metrics import accuracy_score\n",
        "  \n",
        "  y_pred = model.predict(X_test)\n",
        "  # round probabilities to class labels\n",
        "  aut_pred = y_pred[0].round()\n",
        "  class_pred = y_pred[1].round()\n",
        "  itpr_pred = y_pred[2].round()\n",
        "  lit_pred = y_pred[3].round()\n",
        "  prec_pred = y_pred[4].round()\n",
        "  princ_pred = y_pred[5].round()\n",
        "  psy_pred = y_pred[6].round()\n",
        "  rule_pred = y_pred[7].round()\n",
        "  syst_pred = y_pred[8].round()\n",
        "  tele_pred = y_pred[9].round()\n",
        "\n",
        "  aut_test = np.round(y_test['Aut']).to_numpy()\n",
        "  class_test = np.round(y_test['Class']).to_numpy()\n",
        "  itpr_test = np.round(y_test['Itpr']).to_numpy()\n",
        "  lit_test = np.round(y_test['Lit']).to_numpy()\n",
        "  prec_test = np.round(y_test['Prec']).to_numpy()\n",
        "  princ_test = np.round(y_test['Princ']).to_numpy()\n",
        "  psy_test = np.round(y_test['Psy']).to_numpy()\n",
        "  rule_test = np.round(y_test['Rule']).to_numpy()\n",
        "  syst_test = np.round(y_test['Syst']).to_numpy()\n",
        "  tele_test = np.round(y_test['Tele']).to_numpy()\n",
        "  # calculate accuracy\n",
        "  aut_acc = accuracy_score(aut_test, aut_pred)\n",
        "  class_acc = accuracy_score(class_test, class_pred)\n",
        "  itpr_acc = accuracy_score(itpr_test, itpr_pred)\n",
        "  lit_acc = accuracy_score(lit_test, lit_pred)\n",
        "  prec_acc = accuracy_score(prec_test, prec_pred)\n",
        "  princ_acc = accuracy_score(princ_test, princ_pred)\n",
        "  psy_acc = accuracy_score(psy_test, psy_pred)\n",
        "  rule_acc = accuracy_score(rule_test, rule_pred)\n",
        "  syst_acc = accuracy_score(syst_test, syst_pred)\n",
        "  tele_acc = accuracy_score(tele_test, tele_pred)\n",
        "  # store result\n",
        "  print('Aut Accuracy:%.3f' % aut_acc)\n",
        "  print('Class Accuracy:%.3f' % class_acc)\n",
        "  print('Itpr Accuracy:%.3f' % itpr_acc)\n",
        "  print('Lit Accuracy:%.3f' % lit_acc)\n",
        "  print('Prec Accuracy:%.3f' % prec_acc)\n",
        "  print('Princ Accuracy:%.3f' % princ_acc)\n",
        "  print('Psy Accuracy:%.3f' % psy_acc)\n",
        "  print('Rule Accuracy:%.3f' % rule_acc)\n",
        "  print('Syst Accuracy:%.3f' % syst_acc)\n",
        "  print('Tele Accuracy:%.3f' % tele_acc)\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "y5_jOxicBLRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import multilabel_confusion_matrix\n",
        "from sklearn.metrics import coverage_error\n",
        "from sklearn.metrics import label_ranking_average_precision_score\n",
        "if MODEL == \"RNN\":\n",
        "  y_test=y_test.values\n",
        "  y_pred = np.array(model.predict(X_test))\n",
        "  y_pred = y_pred.reshape(y_pred.shape[1], y_pred.shape[0])\n",
        "  # y_pred = np.round(y_pred)\n",
        "  from sklearn.metrics import classification_report\n",
        "  # print(classification_report(y_test, y_pred, zero_division=0, target_names=df_new.columns[1:]))\n",
        "  print(\"Coverage error: \", coverage_error(y_test, y_pred))\n",
        "  print(\"Label ranking average precision: \", label_ranking_average_precision_score(y_test, y_pred))"
      ],
      "metadata": {
        "id": "lPzIov6SZVzY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3067530-5caa-4327-9789-fe829979a431"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coverage error:  5.833333333333333\n",
            "Label ranking average precision:  0.2992690658638938\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving of test parameters and results on file:"
      ],
      "metadata": {
        "id": "HWX_2zAOltC9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"multilabel_argumentation_type_classification_results.txt\", \"a\") as f:\n",
        "    f.write(\"MODEL: \" + MODEL + \"\\n\")\n",
        "    f.write(\"EMBEDDING: \" + EMBEDDING  + \"\\n\")\n",
        "    f.write(\"\\n\")\n",
        "\n",
        "    f.write(\"DATASET:\\n\")\n",
        "    f.write(str(DATASET[\"Argumentation_scheme\"].value_counts()))\n",
        "    f.write(\"\\n\\n\")\n",
        "    f.write(\"NUMBER OF SAMPLES: \" + str(len(X)) + \"\\n\")\n",
        "    f.write(\"TEST_SIZE: \" + str(TEST_SIZE*100) + \"%\\n\")\n",
        "    f.write(\"\\n\")\n",
        "\n",
        "    f.write(\"PARAMETERS:\\n\")\n",
        "\n",
        "    if MODEL == \"RNN\":\n",
        "      f.write(\"EPOCHS: \" + str(EPOCHS) + \"\\n\")\n",
        "      f.write(\"BATCH_SIZE: \" + str(BATCH_SIZE) + \"\\n\")\n",
        "      f.write(\"LAYERS: \" + str(LAYERS) + \"\\n\")\n",
        "      f.write(\"LR: \" + str(LR) + \"\\n\")\n",
        "      f.write(\"L2_FACTOR: \" + str(L2_FACTOR) + \"\\n\")\n",
        "      f.write(\"DROP: \" + str(DROP) + \"\\n\")\n",
        "      f.write(\"\\n\")\n",
        "\n",
        "    f.write(\"RESULTS:\\n\")\n",
        "    f.write(classification_report(y_test, y_pred, zero_division=0, target_names=df_new.columns[1:])+\"\\n\")\n",
        "\n",
        "    f.write(\"\\n\")\n",
        "    f.write(\"#######################################################\")\n",
        "    f.write(\"\\n\\n\")\n",
        "    \n",
        "\n"
      ],
      "metadata": {
        "id": "N5DRUGoUluIc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "outputId": "b61054fa-689c-4061-bf43-0356397ca2bb"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-22366624ad4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RESULTS:\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzero_division\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[1;32m   2108\u001b[0m     \"\"\"\n\u001b[1;32m   2109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2110\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2112\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     93\u001b[0m         raise ValueError(\n\u001b[1;32m     94\u001b[0m             \"Classification metrics can't handle a mix of {0} and {1} targets\".format(\n\u001b[0;32m---> 95\u001b[0;31m                 \u001b[0mtype_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m             )\n\u001b[1;32m     97\u001b[0m         )\n",
            "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of multilabel-indicator and continuous-multioutput targets"
          ]
        }
      ]
    }
  ]
}